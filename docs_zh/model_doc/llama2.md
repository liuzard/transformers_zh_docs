<!--ç‰ˆæƒ 2023 HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache è®¸å¯è¯ï¼Œç‰ˆæœ¬ 2.0 (the "License")ï¼Œé™¤éç¬¦åˆè®¸å¯è¯ä¸­çš„è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å¾—è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯å‘è¡Œçš„è½¯ä»¶ä»¥"æŒ‰åŸæ ·"çš„æ–¹å¼åˆ†å‘ï¼Œ
æ— è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯éšå«çš„ï¼Œä½†ä¸é™äºå¯¹é€‚é”€æ€§å’Œç‰¹å®šç”¨é€”çš„é€‚ç”¨æ€§çš„ä¿è¯ã€‚è¯¦ç»†äº†è§£è®¸å¯è¯çš„ç‰¹å®šè¯­è¨€ç‰ˆæœ¬ï¼Œ
è¯·å‚é˜…è®¸å¯è¯ä¸‹çš„é™åˆ¶å’Œæƒåˆ©ã€‚

âš ï¸ è¯·æ³¨æ„ï¼Œè¯¥æ–‡ä»¶æ˜¯ä»¥ Markdown æ ¼å¼ç¼–å†™çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„ doc-builder çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œ
è¿™å¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ¸²æŸ“ã€‚

-->

# Llama2

## æ¦‚è§ˆ

Llama2 æ¨¡å‹æ˜¯ç”± Hugo Touvronã€Louis Martinã€Kevin Stoneã€Peter Albertã€Amjad Almahairiã€Yasmine Babaeiã€Nikolay Bashlykovã€Soumya Batraã€Prajjwal Bhargavaã€Shruti Bhosaleã€Dan Bikelã€Lukas Blecherã€Cristian Canton Ferrerã€Moya Chenã€Guillem Cucurullã€David Esiobuã€Jude Fernandesã€Jeremy Fuã€Wenyin Fuã€Brian Fullerã€Cynthia Gaoã€Vedanuj Goswamiã€Naman Goyalã€Anthony Hartshornã€Saghar Hosseiniã€Rui Houã€Hakan Inanã€Marcin Kardasã€Viktor Kerkezã€Madian Khabsaã€Isabel Kloumannã€Artem Korenevã€Punit Singh Kouraã€Marie-Anne Lachauxã€Thibaut Lavrilã€Jenya Leeã€Diana Liskovichã€Yinghai Luã€Yuning Maoã€Xavier Martinetã€Todor Mihaylovã€Pushkar Mishraã€Igor Molybogã€Yixin Nieã€Andrew Poultonã€Jeremy Reizensteinã€Rashi Rungtaã€Kalyan Saladiã€Alan Scheltenã€Ruan Silvaã€Eric Michael Smithã€Ranjan Subramanianã€Xiaoqing EllenTanã€Binh Tangã€Ross Taylorã€Adina Williamsã€Jian Xiang Kuanã€Puxin Xuã€Zheng Yanã€Illian Zarovã€Yuchen Zhangã€Angela Fanã€Melanie Kambadurã€Sharan Narangã€Aurelien Rodriguezã€Robert Stojnicã€Sergey Edunov å’Œ Thomas Scialom æå‡ºçš„ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«ä» 70B åˆ° 7B å‚æ•°çš„åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆï¼Œé€šè¿‡å¾®è°ƒç”¨äºå¯¹è¯åº”ç”¨çš„æ£€æŸ¥ç‚¹ï¼

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº† Llama 2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æˆ‘ä»¬çš„å¾®è°ƒ LLMsï¼Œç§°ä¸º Llama 2-Chatï¼Œé’ˆå¯¹å¯¹è¯ä½¿ç”¨æ¡ˆä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ä¼˜äºå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶ä¸”æ ¹æ®æˆ‘ä»¬çš„äººå·¥è¯„ä¼°ï¼Œå¯¹äºæœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œå®ƒä»¬å¯èƒ½æ˜¯å¯¹å°é—­æºæ¨¡å‹çš„åˆé€‚æ›¿ä»£ã€‚ä¸ºäº†ä½¿ç¤¾åŒºèƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šè¿›è¡Œæ„å»ºå¹¶æ¨åŠ¨ LLMs çš„è´Ÿè´£ä»»å¼€å‘ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºæˆ‘ä»¬å¾®è°ƒ Llama 2-Chat å’Œå®‰å…¨æ€§æ”¹è¿›çš„è¯¦ç»†è¯´æ˜ã€‚*

åœ¨æ­¤å¤„æŸ¥çœ‹æ‰€æœ‰ Llama2 æ¨¡å‹ [here](https://huggingface.co/models?search=llama2)

<Tip warning={true}>

`Llama2` æ¨¡å‹æ˜¯ä½¿ç”¨ `bfloat16` è¿›è¡Œè®­ç»ƒçš„ï¼Œä½†åŸå§‹æ¨æ–­ä½¿ç”¨çš„æ˜¯ `float16`ã€‚åœ¨ hub ä¸Šä¸Šä¼ çš„æ£€æŸ¥ç‚¹ä½¿ç”¨ `torch_dtype = 'float16'`ï¼Œ `AutoModel` API å°†ä½¿ç”¨å®ƒå°†æ£€æŸ¥ç‚¹ä» `torch.float32` è½¬æ¢ä¸º `torch.float16`ã€‚

åœ¨çº¿æƒé‡çš„ `dtype` å¤§å¤šä¸ç›¸å…³ï¼Œé™¤éåœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨äº† `torch_dtype="auto"`ï¼Œä¾‹å¦‚ `model = AutoModelForCausalLM.from_pretrained("path", torch_dtype="auto")`ã€‚åŸå› æ˜¯æ¨¡å‹é¦–å…ˆä¼šè¢«ä¸‹è½½ï¼ˆä½¿ç”¨åœ¨çº¿æ£€æŸ¥ç‚¹çš„ `dtype`ï¼‰ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸º `torch` çš„é»˜è®¤ `dtype`ï¼ˆå³ `torch.float32`ï¼‰ï¼Œæœ€åå¦‚æœé…ç½®ä¸­æä¾›äº† `torch_dtype`ï¼Œåˆ™ä½¿ç”¨è¯¥ `dtype`ã€‚

ä¸å»ºè®®ä½¿ç”¨ `float16` å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå·²çŸ¥ä¼šäº§ç”Ÿ `nan`ï¼Œå› æ­¤å»ºè®®åœ¨ `bfloat16` ä¸Šè®­ç»ƒæ¨¡å‹ã€‚

</Tip>

æç¤ºï¼š

- å¯ä»¥é€šè¿‡å¡«å†™[æ­¤è¡¨æ ¼](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)ä»¥è·å– Llama2 æ¨¡å‹çš„æƒé‡ã€‚
- è¯¥æ¶æ„ä¸ç¬¬ä¸€ä¸ª Llama éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯å¢åŠ äº† Grouped Query Attention (GQA)ï¼Œæ¥è‡ªè¿™ç¯‡[è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)ã€‚
- å°† `config.pretraining_tp` è®¾ç½®ä¸ºä¸ 1 ä¸åŒçš„å€¼å°†æ¿€æ´»æ›´å‡†ç¡®ä½†è¾ƒæ…¢çš„çº¿æ€§å±‚è®¡ç®—ï¼Œè¿™åº”è¯¥æ›´å¥½åœ°åŒ¹é…åŸå§‹å¯¹æ•°å€¼ã€‚
- åŸå§‹æ¨¡å‹ä½¿ç”¨ `pad_id = -1` ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰å¡«å……æ ‡è®°ã€‚æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œç¡®ä¿ä½¿ç”¨ `tokenizer.add_special_tokens({"pad_token":"<pad>"})` æ·»åŠ ä¸€ä¸ªå¡«å……æ ‡è®°å¹¶ç›¸åº”è°ƒæ•´æ ‡è®°åµŒå…¥ã€‚æ‚¨è¿˜åº”è¯¥è®¾ç½® `model.config.pad_token_id`ã€‚æ¨¡å‹çš„ `embed_tokens` å±‚é€šè¿‡ `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)` åˆå§‹åŒ–ï¼Œä»¥ç¡®ä¿ç¼–ç å¡«å……æ ‡è®°è¾“å‡ºä¸ºé›¶ï¼Œå› æ­¤å»ºè®®åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’å®ƒã€‚
- å¡«å†™è¡¨å•å¹¶è·å¾—æ¨¡å‹æ£€æŸ¥ç‚¹çš„è®¿é—®æƒé™åï¼Œåº”èƒ½å¤Ÿä½¿ç”¨è½¬æ¢åçš„æ£€æŸ¥ç‚¹ã€‚å¦åˆ™ï¼Œå¦‚æœè¦è½¬æ¢è‡ªå·±çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨[è½¬æ¢è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ã€‚è¯¥è„šæœ¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ï¼ˆç¤ºä¾‹ï¼‰å‘½ä»¤è°ƒç”¨ï¼š

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

- è½¬æ¢åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹å’Œ tokenizerï¼š

```python
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

è¯·æ³¨æ„ï¼Œæ‰§è¡Œè„šæœ¬éœ€è¦è¶³å¤Ÿçš„ CPU RAM æ¥å®¹çº³ float16 ç²¾åº¦çš„æ•´ä¸ªæ¨¡å‹ï¼ˆå³ä½¿æœ€å¤§ç‰ˆæœ¬åˆ†ä¸ºå¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œæ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½åŒ…å«éƒ¨åˆ†æ¨¡å‹æƒé‡ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å…¨éƒ¨åŠ è½½åˆ° RAM ä¸­ï¼‰ã€‚å¯¹äº75Bæ¨¡å‹ï¼Œéœ€è¦ 145GB RAMã€‚

- LLaMA åˆ†è¯å™¨æ˜¯åŸºäº [sentencepiece](https://github.com/google/sentencepiece) çš„ BPE æ¨¡å‹ã€‚sentencepiece çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„åœ¨äºï¼Œå½“è§£ç åºåˆ—æ—¶ï¼Œå¦‚æœç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯å•è¯çš„å¼€å¤´ï¼ˆä¾‹å¦‚"Banana"ï¼‰ï¼Œåˆ†è¯å™¨ä¸ä¼šåœ¨å­—ç¬¦ä¸²å‰é¢åŠ ä¸Šå‰ç¼€ç©ºæ ¼ã€‚

æ­¤æ¨¡å‹ç”± [Arthur Zucker](https://huggingface.co/ArthurZ) è´¡çŒ®ï¼Œ[Lysandre Debut](https://huggingface.co/lysandre) ä¹Ÿåšå‡ºäº†è´¡çŒ®ã€‚Hugging Face ä¸­çš„å®ç°ä»£ç åŸºäº GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox)ã€‚ä½œè€…çš„åŸå§‹ä»£ç å¯ä»¥åœ¨ [here](https://github.com/facebookresearch/llama) æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯ Hugging Face å®˜æ–¹èµ„æºå’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¿«é€Ÿå¼€å§‹ä½¿ç”¨ LLaMA2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤è¦åŒ…å«åœ¨æ­¤å¤„çš„èµ„æºï¼Œè¯·éšæ—¶æå‡ºæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œè¯„å®¡ï¼è¯¥èµ„æºåº”è¯¥å±•ç¤ºå‡ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

- [Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)ï¼šä¸€ç¯‡å…³äº Llama 2 åŠå¦‚ä½•ä½¿ç”¨å®ƒä¸ ğŸ¤— Transformers å’Œ ğŸ¤— PEFT çš„åšå®¢æ–‡ç« ã€‚
- [LLaMA 2 - Every Resource you need](https://www.philschmid.de/llama-2)ï¼šç¼–è¯‘äº†ä¸€äº›ä¸ LLaMA 2 å’Œå¦‚ä½•å¿«é€Ÿå…¥é—¨æœ‰å…³çš„ç›¸å…³èµ„æºã€‚

<PipelineTag pipeline="text-generation"/>

- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•åœ¨ Google Colab ä¸­ä½¿ç”¨ QLoRA å’Œ 4 ä½ç²¾åº¦å¾®è°ƒ Llama 2ã€‚ğŸŒ
- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ 4 ä½ QLoRA å¯¹ "Llama-v2-7b-guanaco" æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶ä» PDF ç”Ÿæˆé—®ç­”æ•°æ®é›†ã€‚ğŸŒ

<PipelineTag pipeline="text-classification"/>

- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ QLoRaã€TRL å’ŒéŸ©æ–‡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ğŸŒğŸ‡°ğŸ‡·

âš—ï¸ ä¼˜åŒ–
- [ç”¨ DPO å¾®è°ƒ Llama 2](https://huggingface.co/blog/dpo-trl)ï¼šä½¿ç”¨ TRL åº“çš„ DPO æ–¹æ³•å¾®è°ƒ Llama 2 çš„æŒ‡å—ã€‚
- [æ‰©å±•æŒ‡å—ï¼šæŒ‡ä»¤å¾®è°ƒ Llama 2](https://www.philschmid.de/instruction-tune-llama-2)ï¼šå°† Llama 2 è®­ç»ƒä¸ºæ ¹æ®è¾“å…¥ç”ŸæˆæŒ‡ä»¤ï¼Œå°†æ¨¡å‹ä»æŒ‡ä»¤éµå¾ªè½¬å˜ä¸ºæŒ‡ä»¤ç»™äºˆã€‚
- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ QLoRa å’Œ TRL åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¯¹ Llama 2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ğŸŒ

âš¡ï¸ æ¨ç†
- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ AutoGPTQ åº“çš„ GPTQ å¯¹ Llama 2 æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚ğŸŒ
- ä¸€ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing) ï¼Œä»‹ç»äº†å¦‚ä½•åœ¨æœ¬åœ°è®¡ç®—æœºæˆ– Google Colab ä¸Šä½¿ç”¨ 4 ä½é‡åŒ–è¿è¡Œ Llama 2 Chat Modelã€‚ğŸŒ

ğŸš€ éƒ¨ç½²
- [åœ¨ Amazon SageMaker ä¸Šå¯¹ LLaMA 2 (7-70B) è¿›è¡Œå¾®è°ƒ](https://www.philschmid.de/sagemaker-llama2-qlora)ï¼šä»è®¾ç½®åˆ° QLoRA å¾®è°ƒå’Œéƒ¨ç½² Amazon SageMaker çš„å®Œæ•´æŒ‡å—ã€‚
- [åœ¨ Amazon SageMaker ä¸Šéƒ¨ç½² Llama 2 7B/13B/70B](https://www.philschmid.de/sagemaker-llama-llm)ï¼šä½¿ç”¨ Hugging Face çš„ LLM DLC å®¹å™¨å®‰å…¨ä¸”å¯æ‰©å±•åœ°éƒ¨ç½²ã€‚
