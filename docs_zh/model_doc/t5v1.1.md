<!--
版权所有 2021 年 HuggingFace 团队。保留所有权利。

根据 Apache 许可证第 2.0 版（“-License-”）获得许可；除非符合许可证的规定，否则你不得使用此文件。你可以获取许可证的副本，<此处输入证书链接>

除非适用法律要求或书面同意，否则根据许可证分发的软件是基于“按原样”基础分发的，不附带任何明示或暗示的保证或条件。请参阅许可证以了解有关特定语言的权限以及许可证下的限制。

⚠️ 请注意，此文件为 Markdown 格式，但包含我们的文档构建程序（类似于 MDX）的特定语法，可能在你的 Markdown 查看器中无法正确显示。

-->

# T5v1.1

## 概述

T5v1.1 是由 Colin Raffel 等人在 [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) 仓库中发布的。它是原始 T5 模型的改进版本。

你可以直接将 T5v1.1 的权重插入到 T5 模型中，例如：

```python
>>> from transformers import T5ForConditionalGeneration

>>> model = T5ForConditionalGeneration.from_pretrained("google/t5-v1_1-base")
```

与原始 T5 模型相比，T5 Version 1.1 包括以下改进：

- 在前向隐藏层中使用 GEGLU 激活函数而不是 ReLU，请参阅[这篇论文](https://arxiv.org/abs/2002.05202)。

- 预训期间关闭了 Dropout（质量提升）。在微调期间应重新启用 Dropout。

- 仅使用 C4 进行预训练，不混入下游任务。。

- 嵌入层和分类器层之间没有参数共享。

- "xl" 和 "xxl" 替代了 "3B" 和 "11B"。模型形状有点不同 - 更大的 `d_model` 和较小的 `num_heads` 和 `d_ff`。

注意：T5 Version 1.1 仅在预训期间使用 [C4](https://huggingface.co/datasets/c4)，不包括任何监督训练。因此，与原始 T5 模型不同，此模型在使用下游任务时需要进行微调。由于 t5v1.1 是无监督预训练的，使用单任务微调期间使用任务前缀没有真正的优势。如果你进行多任务微调，应使用前缀。

Google 已经发布了以下变种：

- [google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)

- [google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)

- [google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)

- [google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)

- [google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl)。

有关所有提示、代码示例和笔记本，请参阅[T5 的文档页面](t5)。

该模型由 [patrickvonplaten](https://huggingface.co/patrickvonplaten) 贡献。原始代码可以在[这里](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)找到。