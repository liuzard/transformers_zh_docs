<!--
ç‰ˆæƒæ‰€æœ‰2021å¹´The HuggingFace Teamã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆä»¥ä¸‹ç®€ç§°â€œè®¸å¯è¯â€ï¼‰ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–è®¸å¯åè®®çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æŒ‰"AS IS"åŸæ ·åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯Markdownæ ¼å¼ï¼Œä½†å«æœ‰ç‰¹å®šäºæˆ‘ä»¬æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„è¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚

-->

# CLIP

## æ¦‚è¿°

CLIPæ¨¡å‹æ˜¯ç”±Alec Radfordã€Jong Wook Kimã€Chris Hallacyã€Aditya Rameshã€Gabriel Gohã€Sandhini Agarwalã€Girish Sastryã€Amanda Askellã€Pamela Mishkinã€Jack Clarkã€Gretchen Kruegerå’ŒIlya Sutskeveråœ¨ã€ŠLearning Transferable Visual Models From Natural Language Supervisionã€‹ä¸­æå‡ºçš„ã€‚CLIPï¼ˆContrastive Language-Image Pre-Trainingï¼‰æ˜¯ä¸€ä¸ªåœ¨å„ç§ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œã€‚ç±»ä¼¼äºGPT-2å’Œ3çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå®ƒå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å¯¼ï¼Œåœ¨ç»™å®šå›¾åƒçš„æƒ…å†µä¸‹é¢„æµ‹æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œè€Œæ— éœ€ç›´æ¥é’ˆå¯¹ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚

æ‘˜è¦å¦‚ä¸‹ï¼š

*å½“å‰æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿè¢«è®­ç»ƒç”¨äºé¢„æµ‹ä¸€ç»„å›ºå®šçš„é¢„å®šä¹‰å¯¹è±¡ç±»åˆ«ã€‚è¿™ç§å—é™çš„ç›‘ç£å½¢å¼é™åˆ¶äº†å®ƒä»¬çš„æ™®é€‚æ€§å’Œå¯ç”¨æ€§ï¼Œå› ä¸ºéœ€è¦é¢å¤–çš„æ ‡æ³¨æ•°æ®æ¥æŒ‡å®šä»»ä½•å…¶ä»–è§†è§‰æ¦‚å¿µã€‚ç›´æ¥ä»åŸå§‹æ–‡æœ¬å­¦ä¹ å›¾åƒæ˜¯ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†æ›´å¹¿æ³›çš„ç›‘ç£æ¥æºã€‚æˆ‘ä»¬è¯æ˜ï¼Œé¢„æµ‹å“ªä¸ªæ ‡é¢˜ä¸å“ªä¸ªå›¾åƒç›¸å¯¹åº”çš„ç®€å•é¢„è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ç§ä»å¤´å¼€å§‹é«˜æ•ˆå¯æ‰©å±•åœ°å­¦ä¹ åƒå·«å›¾åƒè¡¨ç¤ºçš„æ–¹æ³•ï¼Œè¿™äº›å›¾åƒè¡¨ç¤ºæ˜¯ä»äº’è”ç½‘ä¸Šæ”¶é›†çš„4äº¿ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹çš„æ•°æ®é›†ä¸­é¢„è®­ç»ƒå¾—åˆ°çš„ã€‚åœ¨é¢„è®­ç»ƒä¹‹åï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥å‚è€ƒå­¦ä¹ åˆ°çš„è§†è§‰æ¦‚å¿µï¼ˆæˆ–æè¿°æ–°çš„æ¦‚å¿µï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé›¶æ ·æœ¬åœ°è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡åœ¨30å¤šä¸ªä¸åŒçš„è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ¯”å¦‚OCRã€è§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«ã€åœ°ç†å®šä½å’Œè®¸å¤šç±»å‹çš„ç»†ç²’åº¦å¯¹è±¡åˆ†ç±»ç­‰ä»»åŠ¡ï¼Œç ”ç©¶äº†è¿™ç§æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šå¸¸å¯ä»¥éå¸¸æœ‰æ•ˆåœ°è½¬ç§»åˆ°å¤§å¤šæ•°ä»»åŠ¡ä¸­ï¼Œå¹¶ä¸”å¸¸å¸¸åœ¨æ— éœ€ä»»ä½•ç‰¹å®šæ•°æ®é›†è®­ç»ƒçš„æƒ…å†µä¸‹ä¸å®Œå…¨æœ‰ç›‘ç£çš„åŸºçº¿æ¨¡å‹ç«äº‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨ImageNeté›¶æ ·æœ¬ä¸Šä¸åŸå§‹ResNet-50çš„å‡†ç¡®ç‡ç›¸åŒ¹é…ï¼Œè€Œæ— éœ€ä½¿ç”¨åŸå§‹ResNet-50è®­ç»ƒçš„128ä¸‡ä¸ªç¤ºä¾‹ä¹‹ä¸€ã€‚æˆ‘ä»¬åœ¨æ­¤https URLä¸Šé‡Šæ”¾äº†æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚*

## ç”¨æ³•

CLIPæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚å®ƒå¯ä»¥ç”¨äºå›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦å’Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚CLIPä½¿ç”¨ç±»ä¼¼ViTçš„transformerè·å–è§†è§‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å› æœè¯­è¨€æ¨¡å‹è·å–æ–‡æœ¬ç‰¹å¾ã€‚ç„¶åå°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾æŠ•å½±åˆ°ä¸€ä¸ªç›¸åŒç»´åº¦çš„æ½œç©ºé—´ä¸­ï¼Œç„¶åä½¿ç”¨æŠ•å½±å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åˆ†æ•°ã€‚

ä¸ºäº†å°†å›¾åƒè¾“å…¥Transformerç¼–ç å™¨ï¼Œéœ€è¦å°†æ¯ä¸ªå›¾åƒåˆ†å‰²æˆä¸€ä¸ªå›ºå®šå¤§å°çš„éé‡å çš„è¡¥ä¸åºåˆ—ï¼Œç„¶åè¿›è¡Œçº¿æ€§åµŒå…¥ã€‚æ·»åŠ [CLS]ä»¤ç‰Œä½œä¸ºæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºã€‚ä½œè€…è¿˜æ·»åŠ äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå¹¶å°†ç”Ÿæˆçš„å‘é‡åºåˆ—é¦ˆé€åˆ°æ ‡å‡†Transformerç¼–ç å™¨ä¸­ã€‚[`CLIPImageProcessor`]å¯ç”¨äºè°ƒæ•´ï¼ˆæˆ–ç¼©æ”¾ï¼‰å’Œè§„èŒƒåŒ–æ¨¡å‹çš„å›¾åƒã€‚

[`CLIPTokenizer`]ç”¨äºç¼–ç æ–‡æœ¬ã€‚[`CLIPProcessor`]å°è£…äº†[`CLIPImageProcessor`]å’Œ[`CLIPTokenizer`]ï¼Œç”¨äºåŒæ—¶ç¼–ç æ–‡æœ¬å’Œå‡†å¤‡å›¾åƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨[`CLIPProcessor`]å’Œ[`CLIPModel`]è·å–å›¾åƒæ–‡æœ¬ç›¸ä¼¼æ€§å¾—åˆ†ã€‚

```python
>>> from PIL import Image
>>> import requests

>>> from transformers import CLIPProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # è¿™æ˜¯å›¾åƒæ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†
>>> probs = logits_per_image.softmax(dim=1)  # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨softmaxè·å–æ ‡ç­¾æ¦‚ç‡
```

æ­¤æ¨¡å‹ç”±[valhalla](https://huggingface.co/valhalla)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/openai/CLIP)æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰çš„èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©ä½ å¼€å§‹ä½¿ç”¨CLIPã€‚

- ä¸€ç¯‡å…³äº[å¦‚ä½•åœ¨10,000ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå¾®è°ƒCLIP](https://huggingface.co/blog/fine-tune-clip-rsicd)çš„åšå®¢æ–‡ç« ã€‚
- CLIPæ”¯æŒçš„[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)ã€‚

å¦‚æœä½ æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡ºPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ã€‚
èµ„æºç†æƒ³ä¸Šåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚

## CLIPConfig

[[autodoc]] CLIPConfig
    - from_text_vision_configs

## CLIPTextConfig

[[autodoc]] CLIPTextConfig

## CLIPVisionConfig

[[autodoc]] CLIPVisionConfig

## CLIPTokenizer

[[autodoc]] CLIPTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## CLIPTokenizerFast

[[autodoc]] CLIPTokenizerFast

## CLIPImageProcessor

[[autodoc]] CLIPImageProcessor
    - preprocess

## CLIPFeatureExtractor

[[autodoc]] CLIPFeatureExtractor

## CLIPProcessor

[[autodoc]] CLIPProcessor

## CLIPModel

[[autodoc]] CLIPModel
    - forward
    - get_text_features
    - get_image_features

## CLIPTextModel

[[autodoc]] CLIPTextModel
    - forward

## CLIPTextModelWithProjection

[[autodoc]] CLIPTextModelWithProjection
    - forward

## CLIPVisionModelWithProjection

[[autodoc]] CLIPVisionModelWithProjection
    - forward


## CLIPVisionModel

[[autodoc]] CLIPVisionModel
    - forward

## TFCLIPModel

[[autodoc]] TFCLIPModel
    - call
    - get_text_features
    - get_image_features

## TFCLIPTextModel

[[autodoc]] TFCLIPTextModel
    - call

## TFCLIPVisionModel

[[autodoc]] TFCLIPVisionModel
    - call

## FlaxCLIPModel

[[autodoc]] FlaxCLIPModel
    - __call__
    - get_text_features
    - get_image_features

## FlaxCLIPTextModel

[[autodoc]] FlaxCLIPTextModel
    - __call__

## FlaxCLIPTextModelWithProjection

[[autodoc]] FlaxCLIPTextModelWithProjection
    - __call__

## FlaxCLIPVisionModel

[[autodoc]] FlaxCLIPVisionModel
    - __call__