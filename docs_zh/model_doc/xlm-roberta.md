<!--ç‰ˆæƒæ‰€æœ‰2020å¹´HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯2.0ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰æˆæƒ;é™¤éç¬¦åˆè®¸å¯è¯è§„å®šå¦åˆ™ç¦æ­¢ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚

ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œä¾æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäº"æŒ‰åŸæ ·" BASISï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ˜ç¤ºæˆ–æš—ç¤ºã€‚
æœ‰å…³è®¸å¯è¯ä¸‹çš„ç‰¹å®šè¯­è¨€çš„æ˜ç¤ºæˆ–æš—ç¤ºçš„ä»»ä½•å½¢å¼æ‹…ä¿å’Œæ¡ä»¶ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

âš ï¸ æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨Markdownæ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬doc-builderï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨Markdowné˜…è¯»å™¨ä¸­æ­£å¸¸å‘ˆç°ã€‚

-->

# XLM-RoBERTa

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=xlm-roberta">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-xlm--roberta-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/xlm-roberta-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## æ¦‚è§ˆ

XLM-RoBERTaæ¨¡å‹æ˜¯ç”±Alexis Conneauï¼ŒKartikay Khandelwalï¼ŒNaman Goyalï¼ŒVishrav Chaudharyï¼ŒGuillaume
Wenzekï¼ŒFrancisco GuzmÃ¡nï¼ŒEdouard Graveï¼ŒMyle Ottï¼ŒLuke Zettlemoyerå’ŒVeselin Stoyanovåœ¨è®ºæ–‡[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)ä¸­æå‡ºçš„ã€‚å®ƒåŸºäºFacebookäº2019å¹´å‘å¸ƒçš„RoBERTaæ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒäº†2.5TBçš„ç»è¿‡ç­›é€‰çš„CommonCrawlæ•°æ®ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æœ¬æ–‡è¡¨æ˜ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒå¤šè¯­è¨€è¯­è¨€æ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜å¹¿æ³›çš„è·¨è¯­è¨€è½¬ç§»ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸€ç™¾ç§è¯­è¨€ä¸Šè®­ç»ƒäº†ä¸€ç§åŸºäºTransformerçš„å±è”½è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº†2 TBä»¥ä¸Šçš„ç»è¿‡ç­›é€‰çš„CommonCrawlæ•°æ®ã€‚æˆ‘ä»¬çš„æ¨¡å‹åä¸ºXLM-Rï¼Œåœ¨å„ç§è·¨è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬XNLIå¹³å‡å‡†ç¡®ç‡æé«˜äº†13.8ï¼…ï¼ŒMLQAå¹³å‡F1åˆ†æ•°æé«˜äº†12.3ï¼…ï¼ŒNERå¹³å‡F1åˆ†æ•°æé«˜äº†2.1ï¼…ã€‚ XLM-Råœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸å¯¹äºä¹‹å‰çš„XLMæ¨¡å‹ï¼ŒSwahiliçš„XNLIå‡†ç¡®ç‡æé«˜äº†11.8ï¼…ï¼Œä¹Œå°”éƒ½è¯­æé«˜äº†9.2ï¼…ã€‚æˆ‘ä»¬è¿˜è¯¦ç»†è¯„ä¼°äº†å®ç°è¿™äº›å¢ç›Šæ‰€éœ€çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ­£å‘è½¬ç§»å’Œå®¹é‡ç¨€é‡Šä¹‹é—´çš„æƒè¡¡å’Œï¼ˆ2ï¼‰å¤§è§„æ¨¡é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é¦–æ¬¡å±•ç¤ºäº†åœ¨ä¸ç‰ºç‰²æ¯ç§è¯­è¨€çš„æ€§èƒ½çš„æƒ…å†µä¸‹è¿›è¡Œå¤šè¯­è¨€å»ºæ¨¡çš„å¯èƒ½æ€§ï¼›åœ¨GLUEå’ŒXNLIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒXLM-Rä¸å¼ºå¤§çš„å•è¯­æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚æˆ‘ä»¬å°†å…¬å¼€æä¾›XLM-Rçš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚*

æç¤ºï¼š

- XLM-RoBERTaæ˜¯åœ¨100ç§ä¸åŒè¯­è¨€ä¸Šè®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹ã€‚ä¸ä¸€äº›XLMå¤šè¯­è¨€æ¨¡å‹ä¸åŒï¼Œå®ƒä¸éœ€è¦`lang`å¼ é‡æ¥åˆ¤æ–­ä½¿ç”¨çš„æ˜¯å“ªç§è¯­è¨€ï¼Œå¹¶ä¸”åº”è¯¥èƒ½å¤Ÿä»è¾“å…¥idç¡®å®šæ­£ç¡®çš„è¯­è¨€ã€‚
- ä½¿ç”¨äº†RoBERTaåœ¨XLMæ–¹æ³•ä¸Šçš„æŠ€å·§ï¼Œä½†ä¸ä½¿ç”¨ç¿»è¯‘è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚å®ƒä»…å¯¹æ¥è‡ªä¸€ç§è¯­è¨€çš„å¥å­è¿›è¡Œå±è”½è¯­è¨€å»ºæ¨¡ã€‚
- æ­¤å®ç°ä¸RoBERTaç›¸åŒã€‚æœ‰å…³ç”¨æ³•ç¤ºä¾‹ä»¥åŠè¾“å…¥å’Œè¾“å‡ºçš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚é˜…[RoBERTaçš„æ–‡æ¡£](roberta)ã€‚

æ­¤æ¨¡å‹ç”±[stefan-it](https://huggingface.co/stefan-it)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œä»¥å¸®åŠ©ä½ å…¥é—¨XLM-RoBERTaã€‚å¦‚æœä½ æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼è¯¥èµ„æºåº”è¯¥å±•ç¤ºå‡ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

<PipelineTag pipeline="text-classification"/>

- æœ‰å…³å¦‚ä½•åœ¨AWSä¸Šä½¿ç”¨Habana Gaudi [å¯¹XLM RoBERTaè¿›è¡Œå¤šç±»åˆ†ç±»å¾®è°ƒçš„åšæ–‡](https://www.philschmid.de/habana-distributed-training)
- [`XLMRobertaForSequenceClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)æ”¯æŒã€‚
- [`TFXLMRobertaForSequenceClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)æ”¯æŒã€‚
- [`FlaxXLMRobertaForSequenceClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)æ”¯æŒã€‚
- [æ–‡æœ¬åˆ†ç±»](https://huggingface.co/docs/transformers/tasks/sequence_classification)ç« èŠ‚çš„ğŸ¤— Hugging Faceä»»åŠ¡æŒ‡å—ã€‚
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- [`XLMRobertaForTokenClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)æ”¯æŒã€‚
- [`TFXLMRobertaForTokenClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)æ”¯æŒã€‚
- [`FlaxXLMRobertaForTokenClassification`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)æ”¯æŒã€‚
- [æ ‡è®°åˆ†ç±»](https://huggingface.co/course/chapter7/2?fw=pt)ç« èŠ‚çš„ğŸ¤— Hugging Faceè¯¾ç¨‹ã€‚
- [æ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)

<PipelineTag pipeline="text-generation"/>

- [`XLMRobertaForCausalLM`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒã€‚
- [å› æœè¯­è¨€æ¨¡å‹](https://huggingface.co/docs/transformers/tasks/language_modeling)ç« èŠ‚çš„ğŸ¤— Hugging Faceä»»åŠ¡æŒ‡å—ã€‚
- [å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)

<PipelineTag pipeline="fill-mask"/>

- [`XLMRobertaForMaskedLM`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒã€‚
- [`TFXLMRobertaForMaskedLM`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)æ”¯æŒã€‚
- [`FlaxXLMRobertaForMaskedLM`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)æ”¯æŒã€‚
- [å±è”½è¯­è¨€æ¨¡å‹](https://huggingface.co/course/chapter7/3?fw=pt)ç« èŠ‚çš„ğŸ¤— Hugging Faceè¯¾ç¨‹ã€‚
- [å±è”½è¯­è¨€æ¨¡å‹](../tasks/masked_language_modeling)

<PipelineTag pipeline="question-answering"/>

- [`XLMRobertaForQuestionAnswering`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)æ”¯æŒã€‚
- [`TFXLMRobertaForQuestionAnswering`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)æ”¯æŒã€‚
- [`FlaxXLMRobertaForQuestionAnswering`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)æ”¯æŒã€‚
- [é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt)ç« èŠ‚çš„ğŸ¤— Hugging Faceè¯¾ç¨‹ã€‚
- [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)

**å¤šé€‰**

- [`XLMRobertaForMultipleChoice`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)æ”¯æŒã€‚
- [`TFXLMRobertaForMultipleChoice`]ç”±æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)æ”¯æŒã€‚
- [å¤šé€‰é¢˜ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)

ğŸš€ éƒ¨ç½²

- æœ‰å…³å¦‚ä½•åœ¨AWS Lambdaä¸Š[éƒ¨ç½²æ— æœåŠ¡å™¨çš„XLM RoBERTa](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface)çš„åšæ–‡ã€‚

## XLMRobertaConfig

[[autodoc]] XLMRobertaConfig

## XLMRobertaTokenizer

[[autodoc]] XLMRobertaTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## XLMRobertaTokenizerFast

[[autodoc]] XLMRobertaTokenizerFast

## XLMRobertaModel

[[autodoc]] XLMRobertaModel
    - forward

## XLMRobertaForCausalLM

[[autodoc]] XLMRobertaForCausalLM
    - forward

## XLMRobertaForMaskedLM

[[autodoc]] XLMRobertaForMaskedLM
    - forward

## XLMRobertaForSequenceClassification

[[autodoc]] XLMRobertaForSequenceClassification
    - forward

## XLMRobertaForMultipleChoice

[[autodoc]] XLMRobertaForMultipleChoice
    - forward

## XLMRobertaForTokenClassification

[[autodoc]] XLMRobertaForTokenClassification
    - forward

## XLMRobertaForQuestionAnswering

[[autodoc]] XLMRobertaForQuestionAnswering
    - forward

## TFXLMRobertaModel

[[autodoc]] TFXLMRobertaModel
    - call

## TFXLMRobertaForCausalLM

[[autodoc]] TFXLMRobertaForCausalLM
    - call

## TFXLMRobertaForMaskedLM

[[autodoc]] TFXLMRobertaForMaskedLM
    - call

## TFXLMRobertaForSequenceClassification

[[autodoc]] TFXLMRobertaForSequenceClassification
    - call

## TFXLMRobertaForMultipleChoice

[[autodoc]] TFXLMRobertaForMultipleChoice
    - call

## TFXLMRobertaForTokenClassification

[[autodoc]] TFXLMRobertaForTokenClassification
    - call

## TFXLMRobertaForQuestionAnswering

[[autodoc]] TFXLMRobertaForQuestionAnswering
    - call

## FlaxXLMRobertaModel

[[autodoc]] FlaxXLMRobertaModel
    - __call__

## FlaxXLMRobertaForCausalLM

[[autodoc]] FlaxXLMRobertaForCausalLM
    - __call__

## FlaxXLMRobertaForMaskedLM

[[autodoc]] FlaxXLMRobertaForMaskedLM
    - __call__

## FlaxXLMRobertaForSequenceClassification

[[autodoc]] FlaxXLMRobertaForSequenceClassification
    - __call__

## FlaxXLMRobertaForMultipleChoice

[[autodoc]] FlaxXLMRobertaForMultipleChoice
    - __call__

## FlaxXLMRobertaForTokenClassification

[[autodoc]] FlaxXLMRobertaForTokenClassification
    - __call__

## FlaxXLMRobertaForQuestionAnswering

[[autodoc]] FlaxXLMRobertaForQuestionAnswering
    - __call__