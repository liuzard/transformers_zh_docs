<!--ç‰ˆæƒæ‰€æœ‰2021å¹´HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œ
å¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™åœ¨è®¸å¯è¯ä¸‹åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€basisæä¾›çš„ï¼Œ
æ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯¦è§è®¸å¯è¯ä¸­çš„ç‰¹å®šè¯­è¨€ï¼Œ
ä»¥åŠè®¸å¯è¯ä¸‹çš„é™åˆ¶ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä¸ºMarkdownæ ¼å¼ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬çš„doc-builderï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œ
åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤ºã€‚

-->

# GPT-J

## æ¦‚è§ˆ

GPT-Jæ¨¡å‹ç”±Ben Wangå’ŒAran Komatsuzakiåœ¨[kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)ä»£ç åº“ä¸­å‘å¸ƒã€‚å®ƒæ˜¯ä¸€ä¸ªåŸºäº[GPT-2](https://pile.eleuther.ai/)æ•°æ®é›†è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹ã€‚

æ­¤æ¨¡å‹ç”±[Stella Biderman](https://huggingface.co/stellaathena)è´¡çŒ®ã€‚

æç¤ºï¼š

- è¦ä»¥float32åŠ è½½[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)ï¼Œè‡³å°‘éœ€è¦2å€çš„æ¨¡å‹å¤§å°çš„RAMï¼š
   1å€ç”¨äºåˆå§‹åŒ–æƒé‡ï¼Œå¦å¤–1å€ç”¨äºåŠ è½½æ£€æŸ¥ç‚¹ã€‚å› æ­¤ï¼ŒåŠ è½½æ¨¡å‹è‡³å°‘éœ€è¦48GBçš„RAMã€‚ä¸ºäº†å‡å°‘RAMçš„ä½¿ç”¨ï¼Œ
   å¯ä»¥ä½¿ç”¨`torch_dtype`å‚æ•°ä»…åœ¨CUDAè®¾å¤‡ä¸Šä»¥åŠç²¾åº¦åˆå§‹åŒ–æ¨¡å‹ã€‚è¿˜æœ‰ä¸€ä¸ªfp16åˆ†æ”¯å¯å­˜å‚¨fp16æƒé‡ï¼Œ
   å¯ä»¥ç”¨äºè¿›ä¸€æ­¥å‡å°‘RAMçš„ä½¿ç”¨ï¼š

```python
>>> from transformers import GPTJForCausalLM
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained(
...     "EleutherAI/gpt-j-6B",
...     revision="float16",
...     torch_dtype=torch.float16,
... ).to(device)
```

- æ¨¡å‹åº”é€‚ç”¨äº16GB GPUç”¨äºæ¨æ–­ã€‚å¯¹äºè®­ç»ƒ/å¾®è°ƒï¼Œéœ€è¦æ›´å¤šGPU RAMã€‚ä¾‹å¦‚ï¼ŒAdamä¼˜åŒ–å™¨ä¼šç”Ÿæˆæ¨¡å‹çš„å››ä¸ªå‰¯æœ¬ï¼š
  æ¨¡å‹æœ¬èº«ã€æ¢¯åº¦ã€æ¢¯åº¦çš„å¹³å‡å€¼å’Œå¹³æ–¹å¹³å‡å€¼ã€‚å› æ­¤ï¼Œå³ä½¿ä½¿ç”¨æ··åˆç²¾åº¦ï¼Œæ¢¯åº¦æ›´æ–°ä»ä»¥fp32è¡¨ç¤ºï¼Œ
  ä»éœ€è¦è‡³å°‘4å€æ¨¡å‹å¤§å°çš„GPUå†…å­˜ã€‚è¿™è¿˜ä¸åŒ…æ‹¬æ¿€æ´»å’Œæ•°æ®æ‰¹å¤„ç†ï¼Œè¿™äº›ä¹Ÿéœ€è¦ä¸€äº›é¢å¤–çš„GPU RAMã€‚
  å› æ­¤ï¼Œå»ºè®®ä½¿ç”¨DeepSpeedç­‰è§£å†³æ–¹æ¡ˆæ¥è®­ç»ƒ/å¾®è°ƒæ¨¡å‹ã€‚å¦ä¸€ä¸ªé€‰é¡¹æ˜¯ä½¿ç”¨åŸå§‹ä»£ç åº“åœ¨TPUä¸Šè®­ç»ƒ/å¾®è°ƒæ¨¡å‹ï¼Œ
  ç„¶åå°†æ¨¡å‹è½¬æ¢ä¸ºTransformersæ ¼å¼ä»¥è¿›è¡Œæ¨æ–­ã€‚æœ‰å…³æ­¤è¿‡ç¨‹çš„è¯´æ˜å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)æ‰¾åˆ°ã€‚

- å°½ç®¡åµŒå…¥çŸ©é˜µçš„å¤§å°ä¸º50400ï¼Œä½†GPT-2åˆ†è¯å™¨ä»…ä½¿ç”¨50257ä¸ªè¯æ¡ã€‚è¿™äº›é¢å¤–çš„è¯æ¡æ˜¯ä¸ºäº†åœ¨TPUsä¸Šæé«˜æ•ˆç‡è€Œæ·»åŠ çš„ã€‚
  ä¸ºäº†é¿å…åµŒå…¥çŸ©é˜µå¤§å°ä¸è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…ï¼Œ[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)çš„åˆ†è¯å™¨åŒ…å«äº†é¢å¤–çš„143ä¸ªè¯æ¡
  `<|extratoken_1|>... <|extratoken_143|>`ï¼Œå› æ­¤åˆ†è¯å™¨çš„`vocab_size`ä¹Ÿå˜ä¸º50400ã€‚

### ç”Ÿæˆ

å¯ä»¥ä½¿ç”¨[`~generation.GenerationMixin.generate`]æ–¹æ³•ä½¿ç”¨GPT-Jæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

...æˆ–ä»¥float16ç²¾åº¦æ‰§è¡Œï¼š

```python
>>> from transformers import GPTJForCausalLM, AutoTokenizer
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B", torch_dtype=torch.float16).to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹Hugging Faceèµ„æºå’Œç¤¾åŒºèµ„æºï¼ˆç”±ğŸŒæ ‡è¯†ï¼‰ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨GPT-Jã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥æ˜¯å±•ç¤ºæ–°å†…å®¹è€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºçš„ç†æƒ³é€‰æ‹©ã€‚

<PipelineTag pipeline="text-generation"/>

- [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)çš„æè¿°ã€‚
- å¦‚ä½•ä½¿ç”¨Hugging Face Transformerså’ŒAmazon SageMakeréƒ¨ç½²GPT-J 6Bè¿›è¡Œæ¨æ–­çš„åšå®¢ã€‚
- å¦‚ä½•åœ¨GPUä¸Šä½¿ç”¨DeepSpeed-InferenceåŠ é€ŸGPT-Jæ¨æ–­çš„åšå®¢ã€‚
- ä»‹ç»[GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)çš„åšå®¢å¸–å­ã€‚ ğŸŒ
- [GPT-J-6Bæ¨æ–­æ¼”ç¤º](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb)çš„notebookã€‚ ğŸŒ
- å¦ä¸€ä¸ªæ¼”ç¤º[GPT-J-6Bæ¨æ–­](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb)çš„notebookã€‚  
-  ğŸ¤— Hugging Faceè¯¾ç¨‹ä¸­çš„[å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch)ç« èŠ‚ã€‚
- [`GPTJForCausalLM`]ç”±æ­¤[å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)ã€[æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒã€‚
- [`TFGPTJForCausalLM`]ç”±æ­¤[å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)æ”¯æŒã€‚
- [`FlaxGPTJForCausalLM`]ç”±æ­¤[å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb)æ”¯æŒã€‚

**æ–‡æ¡£èµ„æº**
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)
- [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)
- [å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)

## GPTJConfig

[[autodoc]] GPTJConfig
    - æ‰€æœ‰

## GPTJModel

[[autodoc]] GPTJModel
    - forward

## GPTJForCausalLM

[[autodoc]] GPTJForCausalLM
    - forward

## GPTJForSequenceClassification

[[autodoc]] GPTJForSequenceClassification
    - forward

## GPTJForQuestionAnswering

[[autodoc]] GPTJForQuestionAnswering
    - forward

## TFGPTJModel

[[autodoc]] TFGPTJModel
    - call

## TFGPTJForCausalLM

[[autodoc]] TFGPTJForCausalLM
    - call

## TFGPTJForSequenceClassification

[[autodoc]] TFGPTJForSequenceClassification
    - call

## TFGPTJForQuestionAnswering

[[autodoc]] TFGPTJForQuestionAnswering
    - call

## FlaxGPTJModel

[[autodoc]] FlaxGPTJModel
    - __call__

## FlaxGPTJForCausalLM

[[autodoc]] FlaxGPTJForCausalLM
    - __call__