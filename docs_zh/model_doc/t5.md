---
ç‰ˆæƒæ‰€æœ‰ 2020 å¹´ HuggingFace å›¢é˜Ÿ. 
å…¶éµå¾ª Apache è®¸å¯è¯ 2.0 ï¼ˆä»¥ä¸‹ç®€ç§°â€œè®¸å¯è¯â€ï¼‰ä½¿ç”¨æœ¬æ–‡ä»¶; é™¤éç¬¦åˆè®¸å¯è¯, å¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶. 
ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€å¤„è·å–è¯¥è®¸å¯è¯çš„å‰¯æœ¬ï¼š http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨çš„æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æŒ‰â€œåŸæ ·â€åŸºç¡€å‘å¸ƒçš„è½¯ä»¶æŒ‰â€œåŸæ ·â€
åˆ†å‘, åœ¨æ²¡æœ‰ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶çš„æƒ…å†µä¸‹, æ— è®ºæ˜¯æ˜ç¤ºè¿˜æ˜¯æš—ç¤º. æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶çš„è¯¦ç»†ä¿¡æ¯.

 âš ï¸æ³¨æ„ï¼šè¯¥æ–‡ä»¶æ ¼å¼é‡‡ç”¨ Markdown ä½†åŒ…å«äº†æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨çš„ç‰¹æ®Šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œå¯èƒ½åœ¨ä½ çš„ Markdown é˜…è¯»å™¨ä¸­æ¸²æŸ“ä¸æ­£ç¡®ã€‚

-->

# T5

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=t5">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-t5-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/t5-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
<a href="https://huggingface.co/papers/1910.10683">
<img alt="Paper page" src="https://img.shields.io/badge/Paper%20page-1910.10683-green">
</a>
</div>

## æ¦‚è¿°

T5æ¨¡å‹åœ¨ã€Šç”¨ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨æ¢ç´¢è¿ç§»å­¦ä¹ çš„æé™ã€‹ï¼ˆExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformerï¼‰ä¸€æ–‡ä¸­è¢«ä»‹ç»ã€‚è¯¥æ–‡ä½œè€…ä¸ºColin Raffel, Noam Shazeerï¼ŒAdam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Liä»¥åŠPeter J. Liuã€‚

è¯¥è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰å·²ç»åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è¯æ˜äº†å…¶å¼ºå¤§ä¹‹å¤„ï¼Œè¿™ç§æ–¹æ³•æ˜¯å°†æ¨¡å‹åœ¨æ•°æ®å……è¶³çš„ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§å‚¬ç”Ÿäº†å¤šç§æ–¹æ³•ã€æ–¹æ³•è®ºå’Œå®è·µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ¯ä¸ªè¯­è¨€é—®é¢˜éƒ½è½¬åŒ–ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆtext-to-textï¼‰æ ¼å¼ï¼Œæ¢ç´¢äº†NLPçš„è¿ç§»å­¦ä¹ æŠ€æœ¯çš„å„ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ€§ç ”ç©¶æ¯”è¾ƒäº†é¢„è®­ç»ƒç›®æ ‡ã€æ¶æ„ã€æ— æ ‡ç­¾æ•°æ®é›†ã€è¿ç§»æ–¹æ³•ä»¥åŠå…¶ä»–å› ç´ åœ¨å¤§é‡è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚å€ŸåŠ©æˆ‘ä»¬çš„æ¢ç´¢ç»“æœã€è§„æ¨¡ä»¥åŠæˆ‘ä»¬çš„æ–°æ•°æ®é›†â€œå·¨å‹æ¸…æ´çˆ¬å–è¯­æ–™â€ï¼Œæˆ‘ä»¬åœ¨è®¸å¤šä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ¶µç›–äº†æ‘˜è¦ã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ç­‰å¤šä¸ªé¢†åŸŸã€‚ä¸ºäº†ä¿ƒè¿›å°†æ¥åœ¨NLPçš„è¿ç§»å­¦ä¹ ä¸Šçš„å·¥ä½œï¼Œæˆ‘ä»¬é‡Šæ”¾äº†æˆ‘ä»¬çš„æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹å’Œä»£ç ã€‚*

æç¤ºï¼š

- T5æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œä½¿ç”¨å¤šä»»åŠ¡æ··åˆçš„æ— ç›‘ç£å’Œæœ‰ç›‘ç£ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä¸”æ¯ä¸ªä»»åŠ¡è¢«è½¬åŒ–ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬çš„æ ¼å¼ã€‚T5å¯ä»¥é€šè¿‡åœ¨è¾“å…¥å‰é¢æ·»åŠ ä¸åŒçš„å‰ç¼€æ¥é€‚åº”å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘ä»»åŠ¡çš„å‰ç¼€ä¸ºâ€œtranslate English to German: ...â€ï¼Œæ‘˜è¦ä»»åŠ¡çš„å‰ç¼€ä¸ºâ€œsummarize: ...â€ã€‚

- é¢„è®­ç»ƒåŒæ—¶åŒ…æ‹¬æœ‰ç›‘ç£å’Œè‡ªç›‘ç£è®­ç»ƒã€‚æœ‰ç›‘ç£è®­ç»ƒæ˜¯åœ¨GLUEå’ŒSuperGLUEåŸºå‡†ä»»åŠ¡ä¸Šè¿›è¡Œçš„ï¼ˆå°†å…¶è½¬æ¢ä¸ºä¸Šè¿°çš„æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ï¼‰ã€‚è‡ªç›‘ç£è®­ç»ƒä½¿ç”¨è¢«ç ´åçš„æ ‡è®°ï¼Œéšæœºåˆ é™¤15%çš„æ ‡è®°ï¼Œå¹¶ç”¨å•ç‹¬çš„æ ‡è®°æ›¿æ¢å®ƒä»¬ï¼ˆå¦‚æœè¿ç»­çš„å¤šä¸ªæ ‡è®°è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œåˆ™æ•´ä¸ªç»„å°†æ›¿æ¢ä¸ºä¸€ä¸ªå•ä¸€çš„æ ‡è®°ï¼‰ã€‚ç¼–ç å™¨çš„è¾“å…¥æ˜¯è¢«ç ´åçš„å¥å­ï¼Œè§£ç å™¨çš„è¾“å…¥æ˜¯åŸå§‹å¥å­ï¼Œç›®æ ‡åˆ™æ˜¯ç”±å…¶æ ‡è®°åˆ†éš”çš„åˆ é™¤æ ‡è®°ã€‚

- T5ä½¿ç”¨äº†ç›¸å¯¹æ ‡é‡åµŒå…¥ã€‚ç¼–ç å™¨çš„è¾“å…¥å¯ä»¥åœ¨å·¦ä¾§å’Œå³ä¾§è¿›è¡Œå¡«å……ã€‚

- æœ‰å…³ä½¿ç”¨ç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹ä¸‹é¢çš„[training](#training)ã€[inference](#inference)å’Œ[scripts](#scripts)éƒ¨åˆ†ã€‚

T5æœ‰ä¸åŒçš„å°ºå¯¸ï¼š

- [t5-small](https://huggingface.co/t5-small)

- [t5-base](https://huggingface.co/t5-base)

- [t5-large](https://huggingface.co/t5-large)

- [t5-3b](https://huggingface.co/t5-3b)

- [t5-11b](https://huggingface.co/t5-11b)ã€‚

åŸºäºåŸå§‹çš„T5æ¨¡å‹ï¼Œè°·æ­Œå‘å¸ƒäº†ä¸€äº›åç»­çš„å·¥ä½œï¼š

- **T5v1.1**ï¼šT5v1.1 æ˜¯T5çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹å…¶è¿›è¡Œäº†ä¸€äº›æ¶æ„è°ƒæ•´ï¼Œä»…åœ¨C4ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ²¡æœ‰æ··åˆä½¿ç”¨æœ‰ç›‘ç£ä»»åŠ¡ã€‚æœ‰å…³T5v1.1çš„æ–‡æ¡£ï¼Œè¯·å‚é˜…[è¿™é‡Œ](t5v1.1)ã€‚

- **mT5**ï¼šmT5 æ˜¯å¤šè¯­è¨€T5æ¨¡å‹ã€‚å®ƒåœ¨åŒ…æ‹¬101ç§è¯­è¨€çš„mC4è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚æœ‰å…³mT5çš„æ–‡æ¡£ï¼Œè¯·å‚é˜…[è¿™é‡Œ](mt5)ã€‚

- **byT5**ï¼šbyT5 æ˜¯åŸºäºå­—èŠ‚åºåˆ—è€Œä¸æ˜¯SentencePieceå­è¯æ ‡è®°åºåˆ—è¿›è¡Œé¢„è®­ç»ƒçš„T5æ¨¡å‹ã€‚æœ‰å…³byT5çš„æ–‡æ¡£ï¼Œè¯·å‚é˜…[è¿™é‡Œ](byt5)ã€‚

- **UL2**ï¼šUL2 æ˜¯ä¸€ä¸ªç±»ä¼¼T5çš„æ¨¡å‹ï¼Œä½¿ç”¨å„ç§å»å™ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒã€‚

- **Flan-T5**ï¼šFlanæ˜¯ä¸€ç§åŸºäºæç¤ºçš„é¢„è®­ç»ƒæ–¹æ³•ã€‚Flan-T5 æ˜¯åœ¨Flanæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„T5æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬ `taskmaster2`ã€`djaym7/wiki_dialog`ã€`deepmind/code_contests`ã€`lambada`ã€`gsm8k`ã€`aqua_rat`ã€`esnli`ã€`quasc`ã€`qed` ç­‰æ•°æ®é›†ã€‚

- **FLan-UL2**ï¼šæ˜¯ä½¿ç”¨â€œFlanâ€æç¤ºè°ƒä¼˜å’Œæ•°æ®é›†æ”¶é›†å¯¹UL2æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„T5æ¨¡å‹ã€‚

- **UMT5**ï¼šUmT5æ˜¯ä¸€ä¸ªåœ¨æ”¹è¿›å’Œæ›´æ–°çš„mC4å¤šè¯­è¨€è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒçš„å¤šè¯­è¨€T5æ¨¡å‹ï¼Œä½¿ç”¨äº†æ–°çš„é‡‡æ ·æ–¹æ³•UniMaxã€‚æœ‰å…³mT5çš„æ–‡æ¡£ï¼Œè¯·å‚é˜…[è¿™é‡Œ](umt5)ã€‚

æ‰€æœ‰çš„æ£€æŸ¥ç‚¹å¯ä»¥åœ¨[hub](https://huggingface.co/models?search=t5)ä¸Šæ‰¾åˆ°ã€‚

è¿™ä¸ªæ¨¡å‹ç”±[thomwolf](https://huggingface.co/thomwolf)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/google-research/text-to-text-transfer-transformer)æ‰¾åˆ°ã€‚

<a id='training'></a>

## è®­ç»ƒ

T5æ¨¡å‹æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå¯ä»¥å°†æ‰€æœ‰NLPé—®é¢˜è½¬åŒ–ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ã€‚å®ƒæ˜¯ä½¿ç”¨teacher forcingè¿›è¡Œè®­ç»ƒçš„ã€‚è¿™æ„å‘³ç€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯éœ€è¦ä¸€ä¸ªè¾“å…¥åºåˆ—å’Œä¸€ä¸ªç›¸åº”çš„ç›®æ ‡åºåˆ—ã€‚è¾“å…¥åºåˆ—é€šè¿‡`input_ids`è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚ç›®æ ‡åºåˆ—å‘å³ç§»åŠ¨ï¼Œå³åœ¨å‰é¢åŠ ä¸Šä¸€ä¸ªèµ·å§‹åºåˆ—æ ‡è®°ï¼Œå¹¶ä½¿ç”¨`decoder_input_ids`è¾“å…¥åˆ°è§£ç å™¨ä¸­ã€‚åœ¨teacher forcingæ¨¡å¼ä¸‹ï¼Œç›®æ ‡åºåˆ—ä¼šåœ¨æœ«å°¾æ·»åŠ EOSæ ‡è®°ï¼Œå¹¶å¯¹åº”äº`labels`ã€‚PADæ ‡è®°åœ¨è¿™é‡Œç”¨ä½œèµ·å§‹åºåˆ—æ ‡è®°ã€‚T5å¯ä»¥ä»¥æœ‰ç›‘ç£å’Œæ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒ/å¾®è°ƒã€‚

å¯ä»¥ä½¿ç”¨[`T5ForConditionalGeneration`]ï¼ˆæˆ–Tensorflow/Flaxå˜ä½“ï¼‰ï¼Œå®ƒåœ¨è§£ç å™¨çš„é¡¶éƒ¨åŒ…æ‹¬äº†è¯­è¨€å»ºæ¨¡å¤´ã€‚

- æ— ç›‘ç£å»å™ªè®­ç»ƒ

åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œè¾“å…¥åºåˆ—çš„ç‰‡æ®µé€šè¿‡æ‰€è°“çš„æ ‡è®°ç¬¦æ ‡è®°ä¸ºé®ç›–çŠ¶æ€ï¼ŒåŒæ—¶è¾“å‡ºåºåˆ—ç”±åŒæ ·çš„æ ‡è®°ç¬¦å’ŒçœŸæ­£çš„é®ç›–æ ‡è®°ç»„æˆã€‚æ¯ä¸ªæ ‡è®°ç¬¦ä»£è¡¨è¿™ä¸ªå¥å­çš„å”¯ä¸€é®ç›–æ ‡è®°ï¼Œå¹¶ä¸”åº”è¯¥ä»¥`<extra_id_0>`ã€`<extra_id_1>`ç­‰å¼€å¤´ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`T5Tokenizer`]ä¸­æœ‰100ä¸ªæ ‡è®°ç¬¦å¯ç”¨ã€‚

ä¾‹å¦‚ï¼Œå¥å­"The cute dog walks in the park"è¢«æ ‡è®°ä¸º"cute dog"å’Œ"the"ï¼Œå¤„ç†å¦‚ä¸‹ï¼š

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids

>>> # æ­£å‘å‡½æ•°ä¼šè‡ªåŠ¨ç”Ÿæˆæ­£ç¡®çš„decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
3.7837
```

å¦‚æœä½ æœ‰å…´è¶£åœ¨æ–°çš„è¯­æ–™åº“ä¸Šå¯¹T5è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯·æŸ¥çœ‹ç¤ºä¾‹ç›®å½•ä¸­çš„[run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling)è„šæœ¬ã€‚

- æœ‰ç›‘ç£è®­ç»ƒ

åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œè¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—æ˜¯æ ‡å‡†çš„åºåˆ—åˆ°åºåˆ—çš„è¾“å…¥è¾“å‡ºæ˜ å°„ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹ç¿»è¯‘è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ï¼šè¾“å…¥åºåˆ—ä¸º"The house is wonderful."ï¼Œè¾“å‡ºåºåˆ—ä¸º"Das Haus ist wunderbar."ï¼Œé‚£ä¹ˆå®ƒä»¬åº”è¯¥è¢«å‡†å¤‡æˆå¦‚ä¸‹å½¢å¼çš„æ¨¡å‹è¾“å…¥ï¼š

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

>>> # æ­£å‘å‡½æ•°ä¼šè‡ªåŠ¨ç”Ÿæˆæ­£ç¡®çš„decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
0.2542
```

å¦‚ä½ æ‰€è§ï¼Œæ¨¡å‹åªéœ€è¦2ä¸ªè¾“å…¥æ‰èƒ½è®¡ç®—æŸå¤±ï¼š`input_ids`ï¼ˆç¼–ç åçš„è¾“å…¥åºåˆ—çš„`input_ids`ï¼‰å’Œ`labels`ï¼ˆç¼–ç åçš„ç›®æ ‡åºåˆ—çš„`input_ids`ï¼‰ã€‚æ¨¡å‹å°†æ ¹æ®`labels`è‡ªåŠ¨åˆ›å»º`decoder_input_ids`ï¼Œé€šè¿‡å°†å®ƒä»¬å‘å³ç§»åŠ¨ä¸€ä¸ªä½ç½®å¹¶åœ¨å‰é¢æ·»åŠ `config.decoder_start_token_id`ï¼ˆå¯¹äºT5æ¥è¯´ï¼Œè¿™ä¸ªå€¼æ˜¯0ï¼Œå³padæ ‡è®°çš„idï¼‰ã€‚è¿˜è¦æ³¨æ„ä»»åŠ¡å‰ç¼€ï¼šæˆ‘ä»¬å°†è¾“å…¥åºåˆ—çš„å‰é¢åŠ ä¸Š"translate English to German: "ï¼Œç„¶åå†è¿›è¡Œç¼–ç ã€‚è¿™å°†æœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œå› ä¸ºåœ¨T5çš„é¢„è®­ç»ƒä¸­ä½¿ç”¨äº†è¿™ä¸ªä»»åŠ¡å‰ç¼€ã€‚

ç„¶è€Œï¼Œä¸Šé¢çš„ç¤ºä¾‹åªæ˜¾ç¤ºäº†ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä»¥æ‰¹æ¬¡æ–¹å¼è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»å°†ç¤ºä¾‹å¡«å……/æˆªæ–­ä¸ºç›¸åŒçš„é•¿åº¦ã€‚å¯¹äºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œé€šå¸¸ä¼šå®šä¹‰`max_source_length`å’Œ`max_target_length`ï¼Œåˆ†åˆ«ç¡®å®šè¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆå¦åˆ™å®ƒä»¬ä¼šè¢«æˆªæ–­ï¼‰ã€‚è¿™äº›å€¼åº”æ ¹æ®ä»»åŠ¡ä»”ç»†è®¾ç½®ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿`labels`ä¸­çš„å¡«å……æ ‡è®°idä¸è¢«æŸå¤±å‡½æ•°è®¡å…¥ã€‚åœ¨PyTorchå’ŒTensorFlowä¸­ï¼Œå¯ä»¥é€šè¿‡å°†å…¶æ›¿æ¢ä¸º-100æ¥å®ç°ï¼Œ-100æ˜¯`CrossEntropyLoss`çš„`ignore_index`ã€‚åœ¨Flaxä¸­ï¼Œå¯ä»¥ä½¿ç”¨`decoder_attention_mask`åœ¨æŸå¤±ä¸­å¿½ç•¥å¡«å……æ ‡è®°ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§Flaxæ‘˜è¦è„šæœ¬ï¼‰ã€‚æˆ‘ä»¬è¿˜å°†`attention_mask`ä½œä¸ºé™„åŠ è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œä»¥ç¡®ä¿å¿½ç•¥è¾“å…¥çš„å¡«å……æ ‡è®°ã€‚ä¸‹é¢çš„ä»£ç ç¤ºä¾‹è¯´æ˜äº†æ‰€æœ‰è¿™äº›ã€‚

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import torch

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> # ä»¥ä¸‹ä¸¤ä¸ªè¶…å‚æ•°å–å†³äºå…·ä½“çš„ä»»åŠ¡
>>> max_source_length = 512
>>> max_target_length = 128

>>> # å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹ä¸¤ä¸ªè®­ç»ƒç¤ºä¾‹ï¼š
>>> input_sequence_1 = "Welcome to NYC"
>>> output_sequence_1 = "Bienvenue Ã  NYC"

>>> input_sequence_2 = "HuggingFace is a company"
>>> output_sequence_2 = "HuggingFace est une entreprise"

>>> # å¯¹è¾“å…¥è¿›è¡Œç¼–ç 
>>> task_prefix = "translate English to French: "
>>> input_sequences = [input_sequence_1, input_sequence_2]

>>> encoding = tokenizer(
...     [task_prefix + sequence for sequence in input_sequences],
...     padding="longest",
...     max_length=max_source_length,
...     truncation=True,
...     return_tensors="pt",
... )

>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

>>> # å¯¹ç›®æ ‡è¿›è¡Œç¼–ç 
>>> target_encoding = tokenizer(
...     [output_sequence_1, output_sequence_2],
...     padding="longest",
...     max_length=max_target_length,
...     truncation=True,
...     return_tensors="pt",
... )
>>> labels = target_encoding.input_ids

>>> # å°†æ ‡ç­¾ä¸­çš„å¡«å……æ ‡è®°idæ›¿æ¢ä¸º-100ï¼Œä»¥ä¾¿å®ƒä»¬ä¸ä¼šè¢«æŸå¤±å‡½æ•°è®¡å…¥
>>> labels[labels == tokenizer.pad_token_id] = -100

>>> # æ­£å‘ä¼ æ’­
>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
>>> loss.item()
0.188
```

å…¶ä»–è®­ç»ƒæç¤ºï¼š

- T5æ¨¡å‹åœ¨ä½¿ç”¨AdamWä¼˜åŒ–å™¨æ—¶ï¼Œéœ€è¦ç•¥é«˜çš„å­¦ä¹ ç‡ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼ˆåˆ†ç±»ã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ã€é—®é¢˜ç”Ÿæˆï¼‰ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œ1e-4å’Œ3e-4éƒ½æ•ˆæœè‰¯å¥½ã€‚è¯·æ³¨æ„ï¼ŒT5æ˜¯ä½¿ç”¨AdaFactorä¼˜åŒ–å™¨è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚

æ ¹æ®[æ­¤è®ºå›å¸–å­](https://discuss.huggingface.co/t/t5-finetuning-tips/684)ï¼Œä»»åŠ¡å‰ç¼€åœ¨ä»¥ä¸‹æƒ…å†µä¸‹å¾ˆé‡è¦ï¼š(1) è¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼›(2) ä½ çš„ä»»åŠ¡ä¸T5çš„é¢„è®­ç»ƒæ··åˆä¸­ä½¿ç”¨çš„æœ‰ç›‘ç£ä»»åŠ¡ç±»ä¼¼æˆ–ç›¸å…³ï¼ˆè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/pdf/1910.10683.pdf)é™„å½•Dä¸­ä½¿ç”¨çš„ä»»åŠ¡å‰ç¼€ï¼‰ã€‚

å¦‚æœä½¿ç”¨TPUè¿›è¡Œè®­ç»ƒï¼Œå»ºè®®å¯¹æ•°æ®é›†çš„æ‰€æœ‰ç¤ºä¾‹è¿›è¡Œå¡«å……ï¼Œä½¿å…¶å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼Œæˆ–è€…åˆ©ç”¨*pad_to_multiple_of*ï¼Œä»¥ä¾¿ä½¿ç”¨ä¸€å°éƒ¨åˆ†é¢„å®šä¹‰çš„æ¡¶å°ºå¯¸æ¥é€‚åº”æ‰€æœ‰ç¤ºä¾‹ã€‚åœ¨TPUä¸ŠåŠ¨æ€åœ°å¯¹æ‰¹æ¬¡è¿›è¡Œå¡«å……åˆ°æœ€é•¿çš„ç¤ºä¾‹æ˜¯ä¸å»ºè®®çš„ï¼Œå› ä¸ºå¯¹äºè®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°çš„æ¯ä¸ªæ‰¹æ¬¡å½¢çŠ¶è§¦å‘é‡æ–°ç¼–è¯‘ï¼Œè¿™ä¼šä¸¥é‡é™ä½è®­ç»ƒé€Ÿåº¦ã€‚

<a id='inference'></a>

## æ¨ç†

åœ¨æ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨[`~generation.GenerationMixin.generate`]ã€‚è¿™ä¸ªæ–¹æ³•ä¼šå¤„ç†è¾“å…¥çš„ç¼–ç å’Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚ä¼ é€’ç¼–ç çš„éšè—çŠ¶æ€ç»™è§£ç å™¨ï¼Œå¹¶æŒ‰è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆè§£ç å™¨è¾“å‡ºã€‚[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)åŒ…å«å…³äºä½¿ç”¨Transformersç”Ÿæˆæ–‡æœ¬çš„è¯¦ç»†ä¿¡æ¯ã€‚è¿˜æœ‰[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/encoder-decoder#encoder-decoder)è¯¦ç»†è§£é‡Šäº†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä¸­ç”Ÿæˆçš„å·¥ä½œåŸç†ã€‚

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Das Haus ist wunderbar.
```

æ³¨æ„ï¼ŒT5ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_start_token_id`ï¼Œå› æ­¤ï¼Œå¦‚æœåœ¨ä¸ä½¿ç”¨[`~generation.GenerationMixin.generate`]è¿›è¡Œç”Ÿæˆæ—¶ï¼Œè¯·ç¡®ä¿ä»`pad_token_id`å¼€å§‹ã€‚

ä¸Šé¢çš„ç¤ºä¾‹åªæ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ã€‚ä½ ä¹Ÿå¯ä»¥è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> task_prefix = "å°†è‹±è¯­ç¿»è¯‘ä¸ºå¾·è¯­ï¼š"
>>> # use different length sentences to test batching
>>> sentences = ["The house is wonderful.", "I like to work in NYC."]

>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors="pt", padding=True)

>>> output_sequences = model.generate(
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     do_sample=False,  # disable sampling to test if batching affects output
... )

>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))
['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']
```

å› ä¸ºT5æ¨¡å‹é€šè¿‡span-mask denoisingç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢„æµ‹sentinelï¼ˆè¢«maskçš„ï¼‰æ ‡è®°ã€‚é¢„æµ‹çš„æ ‡è®°å°†è¢«æ”¾ç½®åœ¨sentinelæ ‡è®°ä¹‹é—´ã€‚

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids

>>> sequence_ids = model.generate(input_ids)
>>> sequences = tokenizer.batch_decode(sequence_ids)
>>> sequences
['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']
```

## æ€§èƒ½

å¦‚æœä½ æƒ³è¦æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ï¼Œè¯·å®‰è£…[apex](https://github.com/NVIDIA/apex#quick-start)ï¼Œç„¶åæ¨¡å‹å°†è‡ªåŠ¨ä½¿ç”¨`apex.normalization.FusedRMSNorm`ä»£æ›¿`T5LayerNorm`ã€‚å‰è€…ä½¿ç”¨äº†ä¼˜åŒ–çš„èåˆå†…æ ¸ï¼Œæ¯”åè€…å¿«å‡ å€ã€‚

## èµ„æº

ä¸‹é¢æ˜¯ä¸€äº›å®˜æ–¹çš„Hugging Faceèµ„æºå’Œç¤¾åŒºèµ„æºï¼ˆé€šè¿‡ğŸŒæ ‡è¯†ï¼‰ï¼Œå¯å¸®åŠ©ä½ å¼€å§‹ä½¿ç”¨T5ã€‚å¦‚æœä½ æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€Pull Requestï¼Œæˆ‘ä»¬ä¼šå®¡æŸ¥å¹¶æ·»åŠ è¿›å»ï¼è¿™äº›èµ„æºåº”è¯¥å±•ç¤ºå‡ºä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚

<PipelineTag pipeline="text-classification"/>

- ä¸€ä¸ª[ä¸ºåˆ†ç±»å’Œå¤šé€‰é¢˜å¾®è°ƒT5çš„notebook](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)ã€‚
- ä¸€ä¸ª[ä¸ºæƒ…æ„Ÿè·¨åº¦æå–å¾®è°ƒT5çš„notebook](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)ã€‚ğŸŒ

<PipelineTag pipeline="token-classification"/>

- ä¸€ä¸ª[ä¸ºå‘½åå®ä½“è¯†åˆ«å¾®è°ƒT5çš„notebook](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)ã€‚ğŸŒ

<PipelineTag pipeline="text-generation"/>

- ä¸€ä¸ª[å¾®è°ƒCodeT5ä»¥ä»Rubyä»£ç ç”Ÿæˆæ–‡æ¡£å­—ç¬¦ä¸²çš„notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb)ã€‚

<PipelineTag pipeline="summarization"/>

- ä¸€ä¸ª[ä¸ºè·å…°è¯­æ‘˜è¦ç”Ÿæˆå¾®è°ƒT5-base-dutchçš„notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb)ã€‚
- ä¸€ä¸ª[ä½¿ç”¨PyTorchå¾®è°ƒT5è¿›è¡Œæ‘˜è¦æ ‡é¢˜ç”Ÿæˆå¹¶ä½¿ç”¨WandBè¿›è¡Œå®éªŒè¿½è¸ªçš„notebook](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)ã€‚ğŸŒ
- ä¸€ç¯‡å…³äº[ä½¿ç”¨ğŸ¤— Transformerså’ŒAmazon SageMakeråˆ†å¸ƒå¼è®­ç»ƒBART/T5è¿›è¡Œæ‘˜è¦ç”Ÿæˆçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq)ã€‚
- [`T5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)æä¾›æ”¯æŒã€‚
- [`TFT5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)æä¾›æ”¯æŒã€‚
- [`FlaxT5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization)æä¾›æ”¯æŒã€‚
- [æ‘˜è¦](https://huggingface.co/course/chapter7/5?fw=pt#summarization)éƒ¨åˆ†ä½œä¸ºğŸ¤— Hugging Faceè¯¾ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
- [æ‘˜è¦ä»»åŠ¡æŒ‡å—](../tasks/summarization)

<PipelineTag pipeline="fill-mask"/>

- [`FlaxT5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling)æä¾›æ”¯æŒï¼Œç”¨äºè®­ç»ƒå…·æœ‰span-maskedè¯­è¨€æ¨¡å‹ç›®æ ‡çš„T5ã€‚è¯¥è„šæœ¬è¿˜å±•ç¤ºäº†å¦‚ä½•è®­ç»ƒT5 tokenizerã€‚è¿™ä¸ª[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ä¹Ÿæ”¯æŒ[`FlaxT5ForConditionalGeneration`]ã€‚

<PipelineTag pipeline="translation"/>

- [`T5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)æä¾›æ”¯æŒã€‚
- [`TFT5ForConditionalGeneration`]ç”±è¯¥[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation)å’Œ[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)æä¾›æ”¯æŒã€‚
- [ç¿»è¯‘ä»»åŠ¡æŒ‡å—](../tasks/translation)

<PipelineTag pipeline="question-answering"/>

- ä¸€ä¸ªå…³äºå¦‚ä½•[ä½¿ç”¨TensorFlow 2ä¸ºé—®é¢˜å›ç­”å¾®è°ƒT5çš„notebook](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)ã€‚ğŸŒ
- ä¸€ä¸ªå…³äºå¦‚ä½•[åœ¨TPUä¸Šä¸ºé—®é¢˜å›ç­”å¾®è°ƒT5çš„notebook](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)ã€‚


ğŸš€ **éƒ¨ç½²**
- ä¸€ç¯‡å…³äºå¦‚ä½•ä»¥ä¸åˆ°500ç¾å…ƒçš„ä»·æ ¼éƒ¨ç½²[T5 11Bè¿›è¡Œæ¨æ–­çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/deploy-t5-11b)ã€‚

## T5Config

[[autodoc]] T5Config

## T5Tokenizer

[[autodoc]] T5Tokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## T5TokenizerFast

[[autodoc]] T5TokenizerFast

## T5Model

[[autodoc]] T5Model
    - forward

## T5ForConditionalGeneration

[[autodoc]] T5ForConditionalGeneration
    - forward

## T5EncoderModel

[[autodoc]] T5EncoderModel
    - forward

## T5ForSequenceClassification

[[autodoc]] T5ForSequenceClassification
    - forward

## T5ForQuestionAnswering

[[autodoc]] T5ForQuestionAnswering
    - forward

## TFT5Model

[[autodoc]] TFT5Model
    - call

## TFT5ForConditionalGeneration

[[autodoc]] TFT5ForConditionalGeneration
    - call

## TFT5EncoderModel

[[autodoc]] TFT5EncoderModel
    - call

## FlaxT5Model

[[autodoc]] FlaxT5Model
    - __call__
    - encode
    - decode

## FlaxT5ForConditionalGeneration

[[autodoc]] FlaxT5ForConditionalGeneration
    - __call__
    - encode
    - decode

## FlaxT5EncoderModel

[[autodoc]] FlaxT5EncoderModel
    - __call__
```