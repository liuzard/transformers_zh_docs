---
版权所有 2020 年 HuggingFace 团队. 
其遵循 Apache 许可证 2.0 （以下简称“许可证”）使用本文件; 除非符合许可证, 否则你不得使用此文件. 
你可以在以下网址处获取该许可证的副本： http://www.apache.org/licenses/LICENSE-2.0

除非适用的法律要求或书面同意，否则按“原样”基础发布的软件按“原样”
分发, 在没有任何形式的保证或条件的情况下, 无论是明示还是暗示. 查看许可证以获取特定语言的权限和限制的详细信息.

 ⚠️注意：该文件格式采用 Markdown 但包含了我们的文档生成器的特殊语法（类似于 MDX），可能在你的 Markdown 阅读器中渲染不正确。

-->

# T5

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=t5">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-t5-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/t5-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
<a href="https://huggingface.co/papers/1910.10683">
<img alt="Paper page" src="https://img.shields.io/badge/Paper%20page-1910.10683-green">
</a>
</div>

## 概述

T5模型在《用统一的文本到文本转换器探索迁移学习的极限》（Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer）一文中被介绍。该文作者为Colin Raffel, Noam Shazeer，Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li以及Peter J. Liu。

该论文摘要如下：

*迁移学习（Transfer Learning）已经在自然语言处理（NLP）中证明了其强大之处，这种方法是将模型在数据充足的任务上进行预训练，然后在下游任务上进行微调。迁移学习的有效性催生了多种方法、方法论和实践。在本文中，我们通过引入一个统一的框架，将每个语言问题都转化为文本到文本（text-to-text）格式，探索了NLP的迁移学习技术的各个方面。我们的系统性研究比较了预训练目标、架构、无标签数据集、迁移方法以及其他因素在大量语言理解任务上的效果。借助我们的探索结果、规模以及我们的新数据集“巨型清洁爬取语料”，我们在许多代表性基准测试上取得了最先进的结果，涵盖了摘要、问答、文本分类等多个领域。为了促进将来在NLP的迁移学习上的工作，我们释放了我们的数据集、预训练模型和代码。*

提示：

- T5是一个编码器-解码器模型，使用多任务混合的无监督和有监督任务进行预训练，并且每个任务被转化为文本到文本的格式。T5可以通过在输入前面添加不同的前缀来适应各种任务，例如翻译任务的前缀为“translate English to German: ...”，摘要任务的前缀为“summarize: ...”。

- 预训练同时包括有监督和自监督训练。有监督训练是在GLUE和SuperGLUE基准任务上进行的（将其转换为上述的文本到文本任务）。自监督训练使用被破坏的标记，随机删除15%的标记，并用单独的标记替换它们（如果连续的多个标记被标记为删除，则整个组将替换为一个单一的标记）。编码器的输入是被破坏的句子，解码器的输入是原始句子，目标则是由其标记分隔的删除标记。

- T5使用了相对标量嵌入。编码器的输入可以在左侧和右侧进行填充。

- 有关使用细节，请查看下面的[training](#training)、[inference](#inference)和[scripts](#scripts)部分。

T5有不同的尺寸：

- [t5-small](https://huggingface.co/t5-small)

- [t5-base](https://huggingface.co/t5-base)

- [t5-large](https://huggingface.co/t5-large)

- [t5-3b](https://huggingface.co/t5-3b)

- [t5-11b](https://huggingface.co/t5-11b)。

基于原始的T5模型，谷歌发布了一些后续的工作：

- **T5v1.1**：T5v1.1 是T5的改进版本，对其进行了一些架构调整，仅在C4上进行预训练，没有混合使用有监督任务。有关T5v1.1的文档，请参阅[这里](t5v1.1)。

- **mT5**：mT5 是多语言T5模型。它在包括101种语言的mC4语料库上进行了预训练。有关mT5的文档，请参阅[这里](mt5)。

- **byT5**：byT5 是基于字节序列而不是SentencePiece子词标记序列进行预训练的T5模型。有关byT5的文档，请参阅[这里](byt5)。

- **UL2**：UL2 是一个类似T5的模型，使用各种去噪目标进行预训练。

- **Flan-T5**：Flan是一种基于提示的预训练方法。Flan-T5 是在Flan数据集上进行训练的T5模型，其中包括 `taskmaster2`、`djaym7/wiki_dialog`、`deepmind/code_contests`、`lambada`、`gsm8k`、`aqua_rat`、`esnli`、`quasc`、`qed` 等数据集。

- **FLan-UL2**：是使用“Flan”提示调优和数据集收集对UL2模型进行微调的T5模型。

- **UMT5**：UmT5是一个在改进和更新的mC4多语言语料库上进行训练的多语言T5模型，使用了新的采样方法UniMax。有关mT5的文档，请参阅[这里](umt5)。

所有的检查点可以在[hub](https://huggingface.co/models?search=t5)上找到。

这个模型由[thomwolf](https://huggingface.co/thomwolf)贡献。原始代码可以在[这里](https://github.com/google-research/text-to-text-transfer-transformer)找到。

<a id='training'></a>

## 训练

T5模型是一个编码器-解码器模型，可以将所有NLP问题转化为文本到文本格式。它是使用teacher forcing进行训练的。这意味着在训练过程中，我们总是需要一个输入序列和一个相应的目标序列。输入序列通过`input_ids`输入到模型中。目标序列向右移动，即在前面加上一个起始序列标记，并使用`decoder_input_ids`输入到解码器中。在teacher forcing模式下，目标序列会在末尾添加EOS标记，并对应于`labels`。PAD标记在这里用作起始序列标记。T5可以以有监督和无监督的方式进行训练/微调。

可以使用[`T5ForConditionalGeneration`]（或Tensorflow/Flax变体），它在解码器的顶部包括了语言建模头。

- 无监督去噪训练

在这个设置中，输入序列的片段通过所谓的标记符标记为遮盖状态，同时输出序列由同样的标记符和真正的遮盖标记组成。每个标记符代表这个句子的唯一遮盖标记，并且应该以`<extra_id_0>`、`<extra_id_1>`等开头。默认情况下，[`T5Tokenizer`]中有100个标记符可用。

例如，句子"The cute dog walks in the park"被标记为"cute dog"和"the"，处理如下：

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids

>>> # 正向函数会自动生成正确的decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
3.7837
```

如果你有兴趣在新的语料库上对T5进行预训练，请查看示例目录中的[run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling)脚本。

- 有监督训练

在这个设置中，输入序列和输出序列是标准的序列到序列的输入输出映射。例如，假设我们想要对翻译进行微调，我们有一个训练示例：输入序列为"The house is wonderful."，输出序列为"Das Haus ist wunderbar."，那么它们应该被准备成如下形式的模型输入：

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

>>> # 正向函数会自动生成正确的decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
0.2542
```

如你所见，模型只需要2个输入才能计算损失：`input_ids`（编码后的输入序列的`input_ids`）和`labels`（编码后的目标序列的`input_ids`）。模型将根据`labels`自动创建`decoder_input_ids`，通过将它们向右移动一个位置并在前面添加`config.decoder_start_token_id`（对于T5来说，这个值是0，即pad标记的id）。还要注意任务前缀：我们将输入序列的前面加上"translate English to German: "，然后再进行编码。这将有助于提高性能，因为在T5的预训练中使用了这个任务前缀。

然而，上面的示例只显示了一个训练示例。在实践中，我们通常以批次方式训练深度学习模型。这意味着我们必须将示例填充/截断为相同的长度。对于编码器-解码器模型，通常会定义`max_source_length`和`max_target_length`，分别确定输入序列和输出序列的最大长度（否则它们会被截断）。这些值应根据任务仔细设置。

此外，我们必须确保`labels`中的填充标记id不被损失函数计入。在PyTorch和TensorFlow中，可以通过将其替换为-100来实现，-100是`CrossEntropyLoss`的`ignore_index`。在Flax中，可以使用`decoder_attention_mask`在损失中忽略填充标记（有关详细信息，请参见Flax摘要脚本）。我们还将`attention_mask`作为附加输入传递给模型，以确保忽略输入的填充标记。下面的代码示例说明了所有这些。

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import torch

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> # 以下两个超参数取决于具体的任务
>>> max_source_length = 512
>>> max_target_length = 128

>>> # 假设我们有以下两个训练示例：
>>> input_sequence_1 = "Welcome to NYC"
>>> output_sequence_1 = "Bienvenue à NYC"

>>> input_sequence_2 = "HuggingFace is a company"
>>> output_sequence_2 = "HuggingFace est une entreprise"

>>> # 对输入进行编码
>>> task_prefix = "translate English to French: "
>>> input_sequences = [input_sequence_1, input_sequence_2]

>>> encoding = tokenizer(
...     [task_prefix + sequence for sequence in input_sequences],
...     padding="longest",
...     max_length=max_source_length,
...     truncation=True,
...     return_tensors="pt",
... )

>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

>>> # 对目标进行编码
>>> target_encoding = tokenizer(
...     [output_sequence_1, output_sequence_2],
...     padding="longest",
...     max_length=max_target_length,
...     truncation=True,
...     return_tensors="pt",
... )
>>> labels = target_encoding.input_ids

>>> # 将标签中的填充标记id替换为-100，以便它们不会被损失函数计入
>>> labels[labels == tokenizer.pad_token_id] = -100

>>> # 正向传播
>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
>>> loss.item()
0.188
```

其他训练提示：

- T5模型在使用AdamW优化器时，需要略高的学习率。对于大多数问题（分类、摘要、翻译、问答、问题生成），通常情况下，1e-4和3e-4都效果良好。请注意，T5是使用AdaFactor优化器进行预训练的。

根据[此论坛帖子](https://discuss.huggingface.co/t/t5-finetuning-tips/684)，任务前缀在以下情况下很重要：(1) 进行多任务训练；(2) 你的任务与T5的预训练混合中使用的有监督任务类似或相关（请参阅[论文](https://arxiv.org/pdf/1910.10683.pdf)附录D中使用的任务前缀）。

如果使用TPU进行训练，建议对数据集的所有示例进行填充，使其具有相同的长度，或者利用*pad_to_multiple_of*，以便使用一小部分预定义的桶尺寸来适应所有示例。在TPU上动态地对批次进行填充到最长的示例是不建议的，因为对于训练过程中遇到的每个批次形状触发重新编译，这会严重降低训练速度。

<a id='inference'></a>

## 推理

在推理时，建议使用[`~generation.GenerationMixin.generate`]。这个方法会处理输入的编码和通过交叉注意力层传递编码的隐藏状态给解码器，并按自回归的方式生成解码器输出。[这篇博客文章](https://huggingface.co/blog/how-to-generate)包含关于使用Transformers生成文本的详细信息。还有[这篇博客文章](https://huggingface.co/blog/encoder-decoder#encoder-decoder)详细解释了编码器-解码器模型中生成的工作原理。

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Das Haus ist wunderbar.
```

注意，T5使用`pad_token_id`作为`decoder_start_token_id`，因此，如果在不使用[`~generation.GenerationMixin.generate`]进行生成时，请确保从`pad_token_id`开始。

上面的示例只显示了一个示例。你也可以进行批量推理，代码如下：

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> task_prefix = "将英语翻译为德语："
>>> # use different length sentences to test batching
>>> sentences = ["The house is wonderful.", "I like to work in NYC."]

>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors="pt", padding=True)

>>> output_sequences = model.generate(
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     do_sample=False,  # disable sampling to test if batching affects output
... )

>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))
['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']
```

因为T5模型通过span-mask denoising目标进行训练，所以可以在推理过程中预测sentinel（被mask的）标记。预测的标记将被放置在sentinel标记之间。

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids

>>> sequence_ids = model.generate(input_ids)
>>> sequences = tokenizer.batch_decode(sequence_ids)
>>> sequences
['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']
```

## 性能

如果你想要更快的训练和推理性能，请安装[apex](https://github.com/NVIDIA/apex#quick-start)，然后模型将自动使用`apex.normalization.FusedRMSNorm`代替`T5LayerNorm`。前者使用了优化的融合内核，比后者快几倍。

## 资源

下面是一些官方的Hugging Face资源和社区资源（通过🌎标识），可帮助你开始使用T5。如果你有兴趣提交资源以包含在此处，请随时打开Pull Request，我们会审查并添加进去！这些资源应该展示出一些新的东西，而不是重复现有的资源。

<PipelineTag pipeline="text-classification"/>

- 一个[为分类和多选题微调T5的notebook](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)。
- 一个[为情感跨度提取微调T5的notebook](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)。🌎

<PipelineTag pipeline="token-classification"/>

- 一个[为命名实体识别微调T5的notebook](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)。🌎

<PipelineTag pipeline="text-generation"/>

- 一个[微调CodeT5以从Ruby代码生成文档字符串的notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb)。

<PipelineTag pipeline="summarization"/>

- 一个[为荷兰语摘要生成微调T5-base-dutch的notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb)。
- 一个[使用PyTorch微调T5进行摘要标题生成并使用WandB进行实验追踪的notebook](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)。🌎
- 一篇关于[使用🤗 Transformers和Amazon SageMaker分布式训练BART/T5进行摘要生成的博客文章](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq)。
- [`T5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)和[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)提供支持。
- [`TFT5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)和[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)提供支持。
- [`FlaxT5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization)提供支持。
- [摘要](https://huggingface.co/course/chapter7/5?fw=pt#summarization)部分作为🤗 Hugging Face课程的一部分。
- [摘要任务指南](../tasks/summarization)

<PipelineTag pipeline="fill-mask"/>

- [`FlaxT5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling)提供支持，用于训练具有span-masked语言模型目标的T5。该脚本还展示了如何训练T5 tokenizer。这个[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)也支持[`FlaxT5ForConditionalGeneration`]。

<PipelineTag pipeline="translation"/>

- [`T5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation)和[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)提供支持。
- [`TFT5ForConditionalGeneration`]由该[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation)和[notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)提供支持。
- [翻译任务指南](../tasks/translation)

<PipelineTag pipeline="question-answering"/>

- 一个关于如何[使用TensorFlow 2为问题回答微调T5的notebook](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)。🌎
- 一个关于如何[在TPU上为问题回答微调T5的notebook](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)。


🚀 **部署**
- 一篇关于如何以不到500美元的价格部署[T5 11B进行推断的博客文章](https://www.philschmid.de/deploy-t5-11b)。

## T5Config

[[autodoc]] T5Config

## T5Tokenizer

[[autodoc]] T5Tokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## T5TokenizerFast

[[autodoc]] T5TokenizerFast

## T5Model

[[autodoc]] T5Model
    - forward

## T5ForConditionalGeneration

[[autodoc]] T5ForConditionalGeneration
    - forward

## T5EncoderModel

[[autodoc]] T5EncoderModel
    - forward

## T5ForSequenceClassification

[[autodoc]] T5ForSequenceClassification
    - forward

## T5ForQuestionAnswering

[[autodoc]] T5ForQuestionAnswering
    - forward

## TFT5Model

[[autodoc]] TFT5Model
    - call

## TFT5ForConditionalGeneration

[[autodoc]] TFT5ForConditionalGeneration
    - call

## TFT5EncoderModel

[[autodoc]] TFT5EncoderModel
    - call

## FlaxT5Model

[[autodoc]] FlaxT5Model
    - __call__
    - encode
    - decode

## FlaxT5ForConditionalGeneration

[[autodoc]] FlaxT5ForConditionalGeneration
    - __call__
    - encode
    - decode

## FlaxT5EncoderModel

[[autodoc]] FlaxT5EncoderModel
    - __call__
```