<!--
ç‰ˆæƒæ‰€æœ‰2021å¹´The HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯2.0ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚ä½ å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥å¤„è·å¾—è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è¯¥è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶ä»¥"åŸæ ·"åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è¯¥è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸‹çš„è®¸å¯è¯é™åˆ¶ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨Markdownæ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬doc-builderï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚

-->

# Wav2Vec2

## æ€»è§ˆ

Wav2Vec2æ¨¡å‹æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedå’ŒMichael Auliåœ¨[â€œwav2vec 2.0:è‡ªæˆ‘ç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶â€](https://arxiv.org/abs/2006.11477)è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºçš„ã€‚

è®ºæ–‡ä¸­çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬é¦–æ¬¡å±•ç¤ºäº†ä»…ä»è¯­éŸ³éŸ³é¢‘ä¸­å­¦ä¹ å¼ºå¤§çš„è¡¨ç¤ºï¼Œç„¶ååœ¨è½¬å½•çš„è¯­éŸ³ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥è¶…è¶Šæœ€ä½³çš„åŠç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶æ¦‚å¿µä¸Šæ›´ç®€å•ã€‚wav2vec 2.0åœ¨æ½œåœ¨ç©ºé—´ä¸­æ©ç è¯­éŸ³è¾“å…¥ï¼Œå¹¶åœ¨è”åˆå­¦ä¹ çš„æ½œåœ¨è¡¨ç¤ºçš„é‡åŒ–ä¸Šè§£å†³äº†å¯¹æ¯”ä»»åŠ¡ã€‚åœ¨Librispeechçš„æ‰€æœ‰æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œçš„å®éªŒåœ¨å¹²å‡€/å…¶ä»–æµ‹è¯•é›†ä¸Šå®ç°äº†1.8/3.3çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚å½“å°†æ ‡è®°æ•°æ®é‡å‡å°‘åˆ°ä¸€å°æ—¶æ—¶ï¼Œwav2vec 2.0åœ¨ä½¿ç”¨100å€å°‘çš„æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹è¶…è¶Šäº†ä¹‹å‰æœ€å…ˆè¿›çš„åœ¨100å°æ—¶å­é›†ä¸Šçš„æ¨¡å‹ã€‚ä»…ä½¿ç”¨10åˆ†é’Ÿçš„æ ‡è®°æ•°æ®ï¼Œå¹¶åœ¨53000å°æ—¶çš„æ— æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»ç„¶å®ç°äº†4.8/8.2çš„WERã€‚è¿™è¯æ˜äº†å°‘é‡æ ‡è®°æ•°æ®ä¸‹è¯­éŸ³è¯†åˆ«çš„å¯è¡Œæ€§ã€‚*

æç¤ºï¼š

- Wav2Vec2æ˜¯ä¸€ä¸ªæ¥å—æµ®ç‚¹æ•°ç»„ä½œä¸ºåŸå§‹è¯­éŸ³ä¿¡å·æ³¢å½¢çš„è¯­éŸ³æ¨¡å‹ã€‚
- Wav2Vec2æ¨¡å‹ä½¿ç”¨äº†è¿ç»­æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å¿…é¡»ä½¿ç”¨[`Wav2Vec2CTCTokenizer`]å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè§£ç ã€‚

è¯¥æ¨¡å‹ç”±[patrickvonplaten](https://huggingface.co/patrickvonplaten)è´¡çŒ®ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©ä½ å¿«é€Ÿå…¥é—¨Wav2Vec2ã€‚å¦‚æœä½ æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡ºPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼è¯¥èµ„æºç†æƒ³æƒ…å†µä¸‹åº”å±•ç¤ºå‡ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

<PipelineTag pipeline="audio-classification"/>

- æœ‰å…³å¦‚ä½•[åˆ©ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- [`Wav2Vec2ForCTC`]åœ¨æ­¤[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)ä¸­å¾—åˆ°æ”¯æŒã€‚
- [éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/audio_classification)

<PipelineTag pipeline="automatic-speech-recognition"/>

- ä¸€ç¯‡å…³äº[å¢å¼ºWav2Vec2ä¸ğŸ¤—Transformersä¸­çš„n-gram](https://huggingface.co/blog/wav2vec2-with-ngram)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨ğŸ¤—Transformersæ¥å¾®è°ƒè‹±è¯­ASRä¸­çš„Wav2Vec2](https://huggingface.co/blog/fine-tune-wav2vec2-english)çš„åšå®¢æ–‡ç« ã€‚
- å…³äº[ç”¨ğŸ¤—Transformerså¾®è°ƒXLS-Rè¿›è¡Œå¤šè¯­è¨€ASR](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ä»½æœ‰å…³å¦‚ä½•[ä½¿ç”¨Wav2Vec2ä»ä»»ä½•è§†é¢‘åˆ›å»ºYouTubeå­—å¹•](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- [`Wav2Vec2ForCTC`]ç”±ä¸€ä»½å…³äº[å¦‚ä½•åœ¨è‹±è¯­ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)å’Œä¸€ä»½å…³äº[å¦‚ä½•åœ¨ä»»ä½•è¯­è¨€ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)çš„ç¬”è®°æœ¬å¾—åˆ°æ”¯æŒã€‚
- [è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æŒ‡å—](../tasks/asr)

ğŸš€éƒ¨ç½²

- å…³äºå¦‚ä½•ä½¿ç”¨Hugging Faceçš„Transformerså’ŒAmazon SageMakeréƒ¨ç½²Wav2Vec2è¿›è¡Œ[è‡ªåŠ¨è¯­éŸ³è¯†åˆ«](https://www.philschmid.de/automatic-speech-recognition-sagemaker)çš„åšå®¢æ–‡ç« ã€‚

## Wav2Vec2Config

[[autodoc]] Wav2Vec2Config

## Wav2Vec2CTCTokenizer

[[autodoc]] Wav2Vec2CTCTokenizer
    - __call__
    - save_vocabulary
    - decode
    - batch_decode
    - set_target_lang

## Wav2Vec2FeatureExtractor

[[autodoc]] Wav2Vec2FeatureExtractor
    - __call__

## Wav2Vec2Processor

[[autodoc]] Wav2Vec2Processor
    - __call__
    - pad
    - from_pretrained
    - save_pretrained
    - batch_decode
    - decode

## Wav2Vec2ProcessorWithLM

[[autodoc]] Wav2Vec2ProcessorWithLM
    - __call__
    - pad
    - from_pretrained
    - save_pretrained
    - batch_decode
    - decode

### è§£ç å¤šä¸ªéŸ³é¢‘

å¦‚æœä½ è®¡åˆ’å¯¹å¤šä¸ªéŸ³é¢‘è¿›è¡Œè§£ç ï¼Œåº”è€ƒè™‘ä½¿ç”¨[`~Wav2Vec2ProcessorWithLM.batch_decode`]ï¼Œå¹¶ä¼ é€’ä¸€ä¸ªå·²å®ä¾‹åŒ–çš„`multiprocessing.Pool`ã€‚
å¦åˆ™ï¼Œ[`~Wav2Vec2ProcessorWithLM.batch_decode`]çš„æ€§èƒ½å°†æ¯”é€ä¸ªè°ƒç”¨[`~Wav2Vec2ProcessorWithLM.decode`]è¦æ…¢ï¼Œå› ä¸ºå®ƒåœ¨æ¯æ¬¡è°ƒç”¨æ—¶å†…éƒ¨å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`Pool`ã€‚ è¯·å‚è€ƒä»¥ä¸‹ç¤ºä¾‹ï¼š

```python
>>> # è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ç”¨æˆ·ç®¡ç†çš„æ± æ¥æ‰¹é‡è§£ç å¤šä¸ªéŸ³é¢‘
>>> from multiprocessing import get_context
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # å¯¼å…¥æ¨¡å‹ã€ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm").to("cuda")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # åŠ è½½ç¤ºä¾‹æ•°æ®é›†
>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))


>>> def map_to_array(batch):
...     batch["speech"] = batch["audio"]["array"]
...     return batch


>>> # ä¸ºæ‰¹é‡æ¨ç†å‡†å¤‡è¯­éŸ³æ•°æ®
>>> dataset = dataset.map(map_to_array, remove_columns=["audio"])


>>> def map_to_pred(batch, pool):
...     inputs = processor(batch["speech"], sampling_rate=16_000, padding=True, return_tensors="pt")
...     inputs = {k: v.to("cuda") for k, v in inputs.items()}

...     with torch.no_grad():
...         logits = model(**inputs).logits

...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text
...     batch["transcription"] = transcription
...     return batch


>>> # æ³¨æ„ï¼špoolåº”åœ¨`Wav2Vec2ProcessorWithLM`ä¹‹åå®ä¾‹åŒ–ã€‚å¦åˆ™ï¼ŒLMå°†å¯¹æ± çš„å­è¿›ç¨‹ä¸å¯ç”¨
>>> è·å–ä¸Šä¸‹æ–‡("fork")å·¥å…·æ è¢«æ± ä½œä¸ºè¿›ç¨‹è¡¨":
...     è¾“å…¥æ¨¡å‹ï¼Œç‰¹å¾æå–å™¨ï¼Œtokenizer
...     batch["speech"] = batch["audio"]["array"]
...     è¿”å›æ‰¹

...     æ‰¹å¤„ç†=æ‰¹å¤„ç†ä¸­çš„é¢„æµ‹,æ‰¹å¤„ç†=True,æ‰¹å¤„ç†å¤§å°=2,fn_kwargs={"pool": pool},remove_columns=["speech"]
... )

>>> batch["transcription"][:2]
['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', "NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER"]
```

## Wav2Vec2ç‰¹å®šè¾“å‡º

[[autodoc]] models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput

[[autodoc]] models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput

[[autodoc]] models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput

[[autodoc]] models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput

[[autodoc]] models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput

## Wav2Vec2Model

[[autodoc]] Wav2Vec2Model
    - forward

## Wav2Vec2ForCTC

[[autodoc]] Wav2Vec2ForCTC
    - forward
    - load_adapter

## Wav2Vec2ForSequenceClassification

[[autodoc]] Wav2Vec2ForSequenceClassification
    - forward

## Wav2Vec2ForAudioFrameClassification

[[autodoc]] Wav2Vec2ForAudioFrameClassification
    - forward

## Wav2Vec2ForXVector

[[autodoc]] Wav2Vec2ForXVector
    - forward

## Wav2Vec2ForPreTraining

[[autodoc]] Wav2Vec2ForPreTraining
    - forward

## TFWav2Vec2Model

[[autodoc]] TFWav2Vec2Model
    - call

## TFWav2Vec2ForSequenceClassification

[[autodoc]] TFWav2Vec2ForSequenceClassification
    - call

## TFWav2Vec2ForCTC

[[autodoc]] TFWav2Vec2ForCTC
    - call

## FlaxWav2Vec2Model

[[autodoc]] FlaxWav2Vec2Model
    - __call__

## FlaxWav2Vec2ForCTC

[[autodoc]] FlaxWav2Vec2ForCTC
    - __call__

## FlaxWav2Vec2ForPreTraining

[[autodoc]] FlaxWav2Vec2ForPreTraining
    - __call__
