<!--ç‰ˆæƒ2020 HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apache Licenseï¼ŒVersion 2.0 (è®¸å¯è¯)è®¸å¯; é™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œ
å¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä¸‹é¢çš„é“¾æ¥å¤„è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨çš„æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯åè®®åˆ†å‘çš„è½¯ä»¶ä»¥"AS IS"çš„æ–¹å¼åˆ†å‘ï¼Œ
ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹ç‰¹å®šç›®çš„çš„é€‚ç”¨æ€§å’Œä¸ä¾µæƒçš„æ‹…ä¿ã€‚
æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–ç‰¹æ®Šè¯­æ³•çš„Markdownæ–‡ä»¶ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨(ç±»ä¼¼MDX)å¯èƒ½æ— æ³•æ­£ç¡®
åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ¸²æŸ“ã€‚

-->

# BERT

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=bert">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-bert-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/bert-base-uncased">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## æ¦‚è¿°

BERTæ¨¡å‹åœ¨[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)ä¸€æ–‡ä¸­ç”±Jacob Devlin, Ming-Wei Chang, Kenton Leeå’ŒKristina Toutanovaæå‡ºã€‚å®ƒæ˜¯ä½¿ç”¨å˜å‹å™¨è¿›è¡Œé¢„è®­ç»ƒçš„åŒå‘å˜å‹å™¨ï¼Œé€šè¿‡å¯¹ç”±å¤šä¼¦å¤šå›¾ä¹¦è¯­æ–™åº“å’Œç»´åŸºç™¾ç§‘ç»„æˆçš„å¤§å‹è¯­æ–™åº“è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡å’Œä¸‹ä¸€å¥é¢„æµ‹çš„ç»„åˆè¿›è¡Œé¢„è®­ç»ƒã€‚

åœ¨è®ºæ–‡çš„æ‘˜è¦ä¸­ï¼Œæè¿°äº†BERTï¼š

*æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºBERTçš„æ–°çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼Œå®ƒä»£è¡¨åŒå‘ç¼–ç å™¨å˜æ¢çš„è¡¨ç¤ºã€‚ä¸æœ€è¿‘çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹ä¸åŒï¼ŒBERTæ—¨åœ¨é€šè¿‡åœ¨æ‰€æœ‰å±‚ä¸­è”åˆè°ƒèŠ‚å·¦å³ä¸Šä¸‹æ–‡æ¥é¢„è®­ç»ƒæ·±åº¦åŒå‘è¡¨ç¤ºï¼Œä»è€Œä»æ— æ ‡ç­¾æ–‡æœ¬ä¸­è¿›è¡Œæ·±åº¦é¢„è®­ç»ƒã€‚ç»“æœæ˜¯ï¼Œé¢„è®­ç»ƒçš„BERTæ¨¡å‹å¯ä»¥ä»…é€šè¿‡æ·»åŠ ä¸€ä¸ªé¢å¤–çš„è¾“å‡ºå±‚è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåˆ›å»ºç”¨äºå„ç§ä»»åŠ¡çš„æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä¾‹å¦‚é—®é¢˜å›ç­”å’Œè¯­è¨€æ¨ç†ï¼Œè€Œæ— éœ€è¿›è¡Œé‡å¤§çš„ä»»åŠ¡ç‰¹å®šçš„ä½“ç³»ç»“æ„ä¿®æ”¹ã€‚*

*BERTçš„æ¦‚å¿µç®€å•è€Œå®ç”¨ã€‚å®ƒåœ¨åŒ…æ‹¬æ¨åŠ¨GLUEå¾—åˆ†è¾¾åˆ°80.5% ï¼ˆ7.7%çš„ç»å¯¹æ”¹è¿›ï¼‰ã€å°†MultiNLIçš„å‡†ç¡®ç‡æé«˜åˆ°86.7%ï¼ˆ4.6%çš„ç»å¯¹æ”¹è¿›ï¼‰ã€å°†SQuAD v1.1é—®é¢˜å›ç­”æµ‹è¯•F1æé«˜åˆ°93.2ï¼ˆ1.5%çš„ç»å¯¹æ”¹è¿›ï¼‰å’Œå°†SQuAD v2.0æµ‹è¯•F1æé«˜åˆ°83.1%ï¼ˆ5.1%çš„ç»å¯¹æ”¹è¿›ï¼‰åœ¨å†…çš„åä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°çš„æˆæœã€‚*

æç¤ºï¼š

- BERTæ˜¯ä¸€ä¸ªå¸¦æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œæ‰€ä»¥é€šå¸¸å»ºè®®åœ¨å³è¾¹è€Œä¸æ˜¯å·¦è¾¹å¡«å……è¾“å…¥ã€‚
- BERTæ˜¯é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰å’Œä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„ã€‚å®ƒåœ¨é¢„æµ‹æ©ç æ ‡è®°å’Œè‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¥è¯´å¹¶ä¸æ˜¯æœ€ä¼˜é€‰æ‹©ã€‚
- ä½¿ç”¨éšæœºæ©ç æ¥ç ´åè¾“å…¥ï¼Œæ›´å‡†ç¡®åœ°è¯´ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç»™å®šçš„ä¸€å®šæ¯”ä¾‹çš„æ ‡è®°ï¼ˆé€šå¸¸ä¸º15%ï¼‰é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œæ©ç ï¼š

    * ä½¿ç”¨ç‰¹æ®Šæ©ç æ ‡è®°çš„æ¦‚ç‡ä¸º0.8
    * ä½¿ç”¨ä¸è¢«æ©ç æ ‡è®°ä¸åŒçš„éšæœºæ ‡è®°çš„æ¦‚ç‡ä¸º0.1
    * ä½¿ç”¨ç›¸åŒæ ‡è®°çš„æ¦‚ç‡ä¸º0.1
    
- è¯¥æ¨¡å‹å¿…é¡»é¢„æµ‹åŸå§‹å¥å­ï¼Œä½†è¿˜æœ‰ä¸€ä¸ªç¬¬äºŒä¸ªç›®æ ‡ï¼šè¾“å…¥æ˜¯ä¸¤ä¸ªå¥å­Aå’ŒBï¼ˆä¸­é—´æœ‰ä¸€ä¸ªåˆ†éš”æ ‡è®°ï¼‰ã€‚è¿™ä¸¤ä¸ªå¥å­åœ¨è¯­æ–™åº“ä¸­æœ‰50%çš„æ¦‚ç‡æ˜¯è¿ç»­çš„ï¼Œåœ¨å‰©ä¸‹çš„50%ä¸­ï¼Œå®ƒä»¬ä¸ç›¸å…³ã€‚æ¨¡å‹å¿…é¡»é¢„æµ‹å¥å­æ˜¯å¦è¿ç»­ã€‚

æ­¤æ¨¡å‹ç”±[thomwolf](https://huggingface.co/thomwolf)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/google-research/bert)æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯Hugging Faceå®˜æ–¹èµ„æºå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨BERTã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æäº¤è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼è¯¥èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºäº†ä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚

<PipelineTag pipeline="text-classification"/>

- ä¸€ç¯‡å…³äº[ä»¥ä¸åŒè¯­è¨€è¿›è¡ŒBERTæ–‡æœ¬åˆ†ç±»](https://www.philschmid.de/bert-text-classification-in-a-different-language)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ä¸ªç”¨äº[å¯¹å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»è¿›è¡ŒBERTå¾®è°ƒ](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb)çš„ç¬”è®°æœ¬ã€‚
- ä¸€ä¸ªå…³äºå¦‚ä½•[ä½¿ç”¨PyTorchå¯¹å¤šæ ‡ç­¾åˆ†ç±»è¿›è¡ŒBERTå¾®è°ƒ](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- ä¸€ä¸ªå…³äºå¦‚ä½•[ä½¿ç”¨BERTè¿›è¡ŒEncoder-Decoderæ¨¡å‹çš„æ¸©å¯åŠ¨ï¼ˆç”¨äºæ‘˜è¦ï¼‰](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)çš„ç¬”è®°æœ¬ã€‚
- [`BertForSequenceClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)æ”¯æŒçš„ã€‚
- [`TFBertForSequenceClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)æ”¯æŒçš„ã€‚
- [`FlaxBertForSequenceClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)æ”¯æŒçš„ã€‚
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨[Kerasçš„Hugging Face Transformersè¿›è¡ŒBERTçš„å‘½åå®ä½“è¯†åˆ«](https://www.philschmid.de/huggingface-transformers-keras-tf)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ä¸ªç”¨äº[å¯¹å‘½åå®ä½“è¯†åˆ«è¿›è¡ŒBERTå¾®è°ƒ](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb)çš„ç¬”è®°æœ¬ï¼Œè¯¥ç¬”è®°æœ¬ä»…ä½¿ç”¨å•è¯æ ‡ç­¾çš„ç¬¬ä¸€ä¸ªå­—ç‰‡æ®µè¿›è¡Œæ ‡è®°å™¨ã€‚è¦å°†å•è¯çš„æ ‡ç­¾ä¼ æ’­åˆ°æ‰€æœ‰å­—ç‰‡æ®µï¼Œè¯·å‚é˜…è¯¥[ç‰ˆæœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb)çš„ç¬”è®°æœ¬ã€‚
- [`BertForTokenClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)æ”¯æŒçš„ã€‚
- [`TFBertForTokenClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)æ”¯æŒçš„ã€‚
- [`FlaxBertForTokenClassification`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)æ”¯æŒçš„ã€‚
- [Tokenåˆ†ç±»](https://huggingface.co/course/chapter7/2?fw=pt)ï¼šğŸ¤— Hugging Faceè¯¾ç¨‹çš„ç« èŠ‚ã€‚
- [Tokenåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)

<PipelineTag pipeline="fill-mask"/>

- [`BertForMaskedLM`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æ”¯æŒçš„ã€‚
- [`TFBertForMaskedLM`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)æ”¯æŒçš„ã€‚
- [`FlaxBertForMaskedLM`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)æ”¯æŒçš„ã€‚
- [æ©ç è¯­è¨€å»ºæ¨¡](https://huggingface.co/course/chapter7/3?fw=pt)ï¼šğŸ¤— Hugging Faceè¯¾ç¨‹çš„ç« èŠ‚ã€‚
- [æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/masked_language_modeling)

<PipelineTag pipeline="question-answering"/>

- [`BertForQuestionAnswering`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)æ”¯æŒçš„ã€‚
- [`TFBertForQuestionAnswering`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)æ”¯æŒçš„ã€‚
- [`FlaxBertForQuestionAnswering`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)æ”¯æŒçš„ã€‚
- [é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt)ï¼šğŸ¤— Hugging Faceè¯¾ç¨‹çš„ç« èŠ‚ã€‚
- [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)

**å¤šé¡¹é€‰æ‹©**
- [`BertForMultipleChoice`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)æ”¯æŒçš„ã€‚
- [`TFBertForMultipleChoice`]æ˜¯ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)æ”¯æŒçš„ã€‚
- [å¤šé€‰ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)

âš¡ï¸ **æ¨ç†**
- ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨Hugging Face Transformerså’ŒAWS InferentiaåŠ é€ŸBERTæ¨ç†](https://huggingface.co/blog/bert-inferentia-sagemaker)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨DeepSpeed-Inferenceåœ¨GPUä¸ŠåŠ é€ŸBERTæ¨ç†](https://www.philschmid.de/bert-deepspeed-inference)çš„åšå®¢æ–‡ç« ã€‚

âš™ï¸ **é¢„è®­ç»ƒ**
- ä¸€ç¯‡å…³äº[ä½¿ç”¨Hugging Face Transformerså’ŒHabana Gaudiè¿›è¡ŒBERTé¢„è®­ç»ƒ](https://www.philschmid.de/pre-training-bert-habana)çš„åšå®¢æ–‡ç« ã€‚

ğŸš€ **éƒ¨ç½²**
- ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨Hugging Face Optimumå°†Transformersè½¬æ¢ä¸ºONNX](https://www.philschmid.de/convert-transformers-to-onnx)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨Habana Gaudiåœ¨AWSä¸Šè®¾ç½®Hugging Face Transformersçš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ](https://www.philschmid.de/getting-started-habana-gaudi#conclusion)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äº[ä½¿ç”¨Terraformæ¨¡å—å°†BERTä¸HuggingFaceï¼ŒAmazon SageMakerå’Œè‡ªåŠ¨ä¼¸ç¼©ç›¸ç»“åˆ](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äº[ä½¿ç”¨HuggingFaceã€AWS Lambdaå’ŒDockerå®ç°æ— æœåŠ¡å™¨BERT](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äº[Hugging Face Transformers BERTåœ¨Amazon SageMakerå’ŒTraining Compilerä¸Šè¿›è¡Œå¾®è°ƒ](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler)çš„åšå®¢æ–‡ç« ã€‚
- ä¸€ç¯‡å…³äºä½¿ç”¨Transformerså’ŒAmazon SageMakerè¿›è¡Œ[é¢å‘ä»»åŠ¡çš„BERTçŸ¥è¯†è’¸é¦](https://www.philschmid.de/knowledge-distillation-bert-transformers)çš„åšå®¢æ–‡ç« ã€‚

## BertConfig

[[autodoc]] BertConfig
    - all

## BertTokenizer

[[autodoc]] BertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## BertTokenizerFast

[[autodoc]] BertTokenizerFast

## TFBertTokenizer

[[autodoc]] TFBertTokenizer

## Bertç‰¹å®šçš„è¾“å‡º

[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput

[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput

## BertModel

[[autodoc]] BertModel
    - forward

## BertForPreTraining

[[autodoc]] BertForPreTraining
    - forward

## BertLMHeadModel

[[autodoc]] BertLMHeadModel
    - forward

## BertForMaskedLM

[[autodoc]] BertForMaskedLM
    - forward

## BertForNextSentencePrediction

[[autodoc]] BertForNextSentencePrediction
    - forward

## BertForSequenceClassification

[[autodoc]] BertForSequenceClassification
    - forward

## BertForMultipleChoice

[[autodoc]] BertForMultipleChoice
    - forward

## BertForTokenClassification

[[autodoc]] BertForTokenClassification
    - forward

## BertForQuestionAnswering

[[autodoc]] BertForQuestionAnswering
    - forward

## TFBertModel

[[autodoc]] TFBertModel
    - call

## TFBertForPreTraining

[[autodoc]] TFBertForPreTraining
    - call

## TFBertModelLMHeadModel

[[autodoc]] TFBertLMHeadModel
    - call

## TFBertForMaskedLM

[[autodoc]] TFBertForMaskedLM
    - call

## TFBertForNextSentencePrediction

[[autodoc]] TFBertForNextSentencePrediction
    - call

## TFBertForSequenceClassification

[[autodoc]] TFBertForSequenceClassification
    - call

## TFBertForMultipleChoice

[[autodoc]] TFBertForMultipleChoice
    - call

## TFBertForTokenClassification

[[autodoc]] TFBertForTokenClassification
    - call

## TFBertForQuestionAnswering

[[autodoc]] TFBertForQuestionAnswering
    - call

## FlaxBertModel

[[autodoc]] FlaxBertModel
    - __call__

## FlaxBertForPreTraining

[[autodoc]] FlaxBertForPreTraining
    - __call__

## FlaxBertForCausalLM

[[autodoc]] FlaxBertForCausalLM
    - __call__

## FlaxBertForMaskedLM

[[autodoc]] FlaxBertForMaskedLM
    - __call__

## FlaxBertForNextSentencePrediction

[[autodoc]] FlaxBertForNextSentencePrediction
    - __call__

## FlaxBertForSequenceClassification

[[autodoc]] FlaxBertForSequenceClassification
    - __call__

## FlaxBertForMultipleChoice

[[autodoc]] FlaxBertForMultipleChoice
    - __call__

## FlaxBertForTokenClassification

[[autodoc]] FlaxBertForTokenClassification
    - __call__

## FlaxBertForQuestionAnswering

[[autodoc]] FlaxBertForQuestionAnswering
    - __call__