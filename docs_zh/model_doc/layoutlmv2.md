<!-- ç‰ˆæƒ 2021 å¹´çš„ HuggingFace å›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache æˆæƒè¯è®¸å¯ï¼Œç‰ˆæœ¬ 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨ Markdown æ ¼å¼ï¼Œä½†åŒ…å«ç”¨äºæˆ‘ä»¬æ–‡æ¡£ç”Ÿæˆå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDX çš„è¯­æ³•ï¼‰ï¼Œåœ¨ä½ çš„ Markdown æŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚

-->

# LayoutLMV2

## æ€»è§ˆ

LayoutLMV2 æ¨¡å‹åœ¨ [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) ä¸€æ–‡ä¸­æå‡ºï¼Œä½œè€…æ˜¯ Yang Xuã€Yiheng Xuã€Tengchao Lvã€Lei Cuiã€Furu Weiã€Guoxin Wangã€Yijuan Luã€
Dinei Florencioã€Cha Zhangã€Wanxiang Cheã€Min Zhang å’Œ Lidong Zhouã€‚LayoutLMV2 æ”¹è¿›äº† [LayoutLM](layoutlm) ä»¥åœ¨å¤šä¸ªæ–‡æ¡£å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­è·å¾—æœ€æ–°çš„ç»“æœï¼š

- æ‰«ææ–‡æ¡£çš„ä¿¡æ¯æå–ï¼š[FUNSD](https://guillaumejaume.github.io/FUNSD/) æ•°æ®é›†ï¼ˆåŒ…å« 199 ä¸ªå¸¦æœ‰è¶…è¿‡ 30,000 ä¸ªå•è¯çš„æ³¨é‡Šè¡¨å•ï¼‰ã€[CORD](https://github.com/clovaai/cord) æ•°æ®é›†ï¼ˆåŒ…å« 800 å¼ ç”¨äºè®­ç»ƒã€100 å¼ ç”¨äºéªŒè¯å’Œ 100 å¼ ç”¨äºæµ‹è¯•çš„æ”¶æ®ï¼‰ã€[SROIE](https://rrc.cvc.uab.es/?ch=13) æ•°æ®é›†ï¼ˆåŒ…å« 626 å¼ ç”¨äºè®­ç»ƒå’Œ 347 å¼ ç”¨äºæµ‹è¯•çš„æ”¶æ®ï¼‰å’Œ [Kleister-NDA](https://github.com/applicaai/kleister-nda) æ•°æ®é›†ï¼ˆåŒ…å«æ¥è‡ª EDGAR æ•°æ®åº“çš„éå…¬å¼€åè®®ï¼ŒåŒ…æ‹¬ 254 ä»½è®­ç»ƒæ–‡æ¡£ã€83 ä»½éªŒè¯æ–‡æ¡£å’Œ 203 ä»½æµ‹è¯•æ–‡æ¡£ï¼‰ã€‚
- æ–‡æ¡£å›¾åƒåˆ†ç±»ï¼š[RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) æ•°æ®é›†ï¼ˆåŒ…å« 40 ä¸‡ä¸ªå±äº 16 ä¸ªç±»åˆ«çš„å›¾åƒï¼‰ã€‚
- æ–‡æ¡£è§†è§‰é—®ç­”ï¼š[DocVQA](https://arxiv.org/abs/2007.00398) æ•°æ®é›†ï¼ˆåŒ…å«åœ¨ 12,000 å¤šä¸ªæ–‡æ¡£å›¾åƒä¸Šå®šä¹‰çš„ 50,000 ä¸ªé—®é¢˜ï¼‰ã€‚

è®ºæ–‡ä¸­çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*ç”±äºå…¶æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œå¤§è§„æ¨¡æ— æ ‡æ³¨æ‰«æ/æ•°å­—å‡ºç”Ÿæ–‡æ¡£çš„ä¼˜åŠ¿ï¼Œæ–‡æœ¬å’Œå¸ƒå±€çš„é¢„è®­ç»ƒåœ¨å„ç§è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† LayoutLMv2ï¼Œé€šè¿‡åœ¨å¤šæ¨¡æ€æ¡†æ¶ä¸­é¢„è®­ç»ƒæ–‡æœ¬ã€å¸ƒå±€å’Œå›¾åƒï¼Œåˆ©ç”¨äº†æ–°çš„æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼ŒLayoutLMv2 ä¸ä»…ä½¿ç”¨ç°æœ‰çš„é®ç›–çš„è§†è§‰è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œè¿˜ä½¿ç”¨æ–°çš„æ–‡æœ¬-å›¾åƒå¯¹é½å’Œæ–‡æœ¬-å›¾åƒåŒ¹é…ä»»åŠ¡ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæ›´å¥½åœ°å­¦ä¹ äº†è·¨æ¨¡æ€äº¤äº’ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å°†ç©ºé—´æ„ŸçŸ¥çš„è‡ªæ³¨æ„æœºåˆ¶æ•´åˆåˆ° Transformer æ¶æ„ä¸­ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥å……åˆ†ç†è§£ä¸åŒæ–‡æœ¬å—ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutLMv2 åœ¨å„ç§ä¸‹æ¸¸åŸºäºè§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šä¼˜äºå¼ºåŸºå‡†æ¨¡å‹ï¼Œå¹¶å–å¾—äº†æ–°çš„æœ€æ–°æˆæœï¼Œä¾‹å¦‚ FUNSD (0.7895 -> 0.8420)ã€CORD (0.9493 -> 0.9601)ã€SROIE (0.9524 -> 0.9781)ã€Kleister-NDA (0.834 -> 0.852)ã€RVL-CDIP (0.9443 -> 0.9564) å’Œ DocVQA (0.7295 -> 0.8672)ã€‚è¯¥é¢„è®­ç»ƒçš„ LayoutLMv2 æ¨¡å‹å¯ä»¥åœ¨æ­¤ URL ä¸­å…¬å¼€è·å¾—ã€‚*

LayoutLMv2 ä¾èµ–äº `detectron2`ã€`torchvision` å’Œ `tesseract`ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…å®ƒä»¬ï¼š
```
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
python -m pip install torchvision tesseract
```
ï¼ˆå¦‚æœä½ æ˜¯ä¸º LayoutLMv2 å¼€å‘ï¼Œè¯·æ³¨æ„ï¼Œé€šè¿‡æµ‹è¯•æ–‡æ¡£è¿˜éœ€è¦å®‰è£…è¿™äº›è½¯ä»¶åŒ…ã€‚ï¼‰

æç¤ºï¼š

- LayoutLMv1 å’Œ LayoutLMv2 çš„ä¸»è¦åŒºåˆ«åœ¨äºåè€…åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†è§†è§‰åµŒå…¥ï¼ˆè€Œ LayoutLMv1 ä»…åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ·»åŠ äº†è§†è§‰åµŒå…¥ï¼‰ã€‚
- LayoutLMv2 åœ¨è‡ªæ³¨æ„å±‚çš„æ³¨æ„åˆ†æ•°ä¸­æ·»åŠ äº†ç›¸å¯¹çš„ä¸€ç»´æ³¨æ„åå·®å’Œç©ºé—´äºŒç»´æ³¨æ„åå·®ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…è®ºæ–‡çš„ç¬¬ 5 é¡µã€‚
- æœ‰å…³å¦‚ä½•åœ¨ RVL-CDIPã€FUNSDã€DocVQAã€CORD ä¸Šä½¿ç”¨ LayoutLMv2 æ¨¡å‹çš„æ¼”ç¤ºç¬”è®°æœ¬ï¼Œå¯ä»¥å‚è€ƒ[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials)ã€‚
- LayoutLMv2 ä½¿ç”¨ Facebook AI çš„ [Detectron2](https://github.com/facebookresearch/detectron2/) åŒ…ä½œä¸ºå…¶è§†è§‰éª¨å¹²ã€‚æœ‰å…³å®‰è£…è¯´æ˜ï¼Œè¯·å‚é˜…[æ­¤é“¾æ¥](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)ã€‚
- é™¤ `input_ids` ä¹‹å¤–ï¼Œ[`~LayoutLMv2Model.forward`] é¢„æœŸè¿˜æä¾›å¦å¤–ä¸¤ä¸ªè¾“å…¥ï¼Œå³
  `image` å’Œ `bbox`ã€‚`image` è¾“å…¥å¯¹åº”æ–‡æœ¬æ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£å›¾åƒã€‚æ¨¡å‹æœŸæœ›æ¯ä¸ªæ–‡æ¡£å›¾åƒçš„å¤§å°ä¸º 224x224ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ æœ‰ä¸€æ‰¹æ–‡æ¡£å›¾åƒï¼Œåˆ™ `image` åº”è¯¥æ˜¯å½¢çŠ¶ä¸º (batch_size, 3, 224, 224) çš„å¼ é‡ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ª `torch.Tensor` æˆ–ä¸€ä¸ª `Detectron2.structures.ImageList`ã€‚ä½ æ— éœ€å¯¹é€šé“è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå› ä¸ºæ¨¡å‹ä¼šæ‰§è¡Œæ­¤æ“ä½œã€‚é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œè§†è§‰éª¨å¹²æœŸæœ› BGR é€šé“è€Œä¸æ˜¯ RGB é€šé“ï¼Œå› ä¸º Detectron2 ä¸­çš„æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ BGR æ ¼å¼è¿›è¡Œäº†é¢„è®­ç»ƒã€‚`bbox` è¾“å…¥æ˜¯è¾“å…¥æ–‡æœ¬æ ‡è®°çš„è¾¹ç•Œæ¡†ï¼ˆå³ 2Dä½ç½®ï¼‰ã€‚è¿™ä¸ [`LayoutLMModel`] ä¸­çš„ç›¸åŒã€‚å¯ä»¥ä½¿ç”¨å¤–éƒ¨ OCR å¼•æ“ï¼ˆä¾‹å¦‚ Google çš„ [Tesseract](https://github.com/tesseract-ocr/tesseract)ï¼‰å°†å…¶æ£€ç´¢å‡ºæ¥ï¼ˆæœ‰ä¸€ä¸ªå¯ç”¨çš„ [Python å°è£…](https://pypi.org/project/pytesseract/)ï¼‰ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†çš„æ ¼å¼åº”ä¸º (x0, y0, x1, y1)ï¼Œå…¶ä¸­ (x0, y0) å¯¹åº”äºè¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ä½ç½®ï¼Œ(x1, y1) è¡¨ç¤ºè¾¹ç•Œæ¡†å³ä¸‹è§’çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œé¦–å…ˆéœ€è¦å°†è¾¹ç•Œæ¡†è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä½¿å…¶ä½äº 0-1000 çš„èŒƒå›´å†…ã€‚è¦è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```python
def normalize_bbox(bbox, width, height):
    return [
        int(1000 * (bbox[0] / width)),
        int(1000 * (bbox[1] / height)),
        int(1000 * (bbox[2] / width)),
        int(1000 * (bbox[3] / height)),
    ]
```

è¿™é‡Œï¼Œ`width` å’Œ `height` å¯¹åº”äºæ–‡æœ¬æ ‡è®°å‡ºç°çš„åŸå§‹æ–‡æ¡£çš„å®½åº¦å’Œé«˜åº¦ï¼ˆè°ƒæ•´å›¾åƒå¤§å°ä¹‹å‰ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ Python å›¾åƒåº“ï¼ˆPILï¼‰åº“è·å–è¿™äº›å€¼ï¼Œä¾‹å¦‚ï¼š

```python
from PIL import Image

image = Image.open(
    "name_of_your_document - å¯ä»¥æ˜¯ä½ è¦å¤„ç†çš„æ–‡æ¡£çš„ pngã€jpg ç­‰ï¼ˆPDF å¿…é¡»è½¬æ¢ä¸ºå›¾åƒï¼‰ã€‚"
)

width, height = image.size
```

ä¸è¿‡ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬å…¨æ–°çš„ [`~transformers.LayoutLMv2Processor`]ï¼Œå¯ç”¨äºç›´æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®ï¼ˆåœ¨å¹•ååº”ç”¨ OCRï¼‰ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨ä¸‹é¢çš„â€œç”¨æ³•â€éƒ¨åˆ†ä¸­æ‰¾åˆ°ã€‚

- åœ¨å†…éƒ¨ï¼Œ[`~transformers.LayoutLMv2Model`]ä¼šå°† `image` è¾“å…¥é€šè¿‡å…¶è§†è§‰éª¨å¹²ä¼ é€’ï¼Œä»¥è·å–ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ï¼Œå…¶å½¢çŠ¶ç­‰äº [`~transformers.LayoutLMv2Config`] çš„ `image_feature_pool_shape` å±æ€§ã€‚ç„¶åï¼Œæ­¤ç‰¹å¾å›¾è¢«å±•å¹³ä»¥è·å–å›¾åƒæ ‡è®°çš„åºåˆ—ã€‚ç”±äºé»˜è®¤æƒ…å†µä¸‹ç‰¹å¾å›¾çš„å¤§å°ä¸º 7x7ï¼Œå› æ­¤ä¸€å…±ä¼šå¾—åˆ° 49 ä¸ªå›¾åƒæ ‡è®°ã€‚ç„¶åï¼Œè¿™äº›å›¾åƒæ ‡è®°ä¸æ–‡æœ¬æ ‡è®°è¿æ¥ï¼Œå¹¶é€šè¿‡ Transformer ç¼–ç å™¨ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ å°†æ–‡æœ¬æ ‡è®°æ‰©å±•åˆ°æœ€å¤§é•¿åº¦ï¼Œåˆ™æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€çš„é•¿åº¦å°†ä¸º 512 + 49 = 561ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œæœ€åçš„éšè—çŠ¶æ€å°†å…·æœ‰å½¢çŠ¶ `seq_length` + `image_feature_pool_shape[0]` *
  `config.image_feature_pool_shape[1]`ã€‚
- åœ¨è°ƒç”¨ [`~transformers.LayoutLMv2Model.from_pretrained`] æ—¶ï¼Œå°†æ˜¾ç¤ºä¸€ä¸ªè­¦å‘Šï¼Œå…¶ä¸­åˆ—å‡ºäº†æœªåˆå§‹åŒ–çš„ä¸€é•¿ä¸²å‚æ•°åç§°ã€‚è¿™å¹¶ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºè¿™äº›å‚æ•°æ˜¯æ‰¹é‡å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶å°†å…·æœ‰å€¼ã€‚
- å¦‚æœä½ æƒ³åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨è°ƒç”¨ä¸­çš„æ¨¡å‹ä¸Šè°ƒç”¨ [`synchronize_batch_norm`]ï¼Œä»¥ä¾¿æ­£ç¡®åŒæ­¥å¯è§†åŒ–éª¨å¹²çš„æ‰¹é‡å½’ä¸€åŒ–å±‚ã€‚

æ­¤å¤–ï¼Œè¿˜æœ‰ LayoutXLMï¼Œå®ƒæ˜¯ LayoutLMv2 çš„å¤šè¯­è¨€ç‰ˆæœ¬ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[LayoutXLM çš„æ–‡æ¡£é¡µé¢](layoutxlm)æ‰¾åˆ°ã€‚

## èµ„æº

LayoutLMv2 å…¥é—¨çš„å®˜æ–¹ Hugging Face å’Œç¤¾åŒºèµ„æºåˆ—è¡¨ï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰ã€‚å¦‚æœä½ æœ‰å…´è¶£æäº¤è¦åŒ…å«åœ¨æ­¤å¤„çš„èµ„æºï¼Œè¯·éšæ—¶æå‡ºåˆå¹¶è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æ ¸ï¼è¯¥èµ„æºåº”æœ€å¥½å±•ç¤ºå‡ºæ–°çš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

<PipelineTag pipeline="text-classification"/>

- æœ‰å…³[å¦‚ä½•åœ¨ RVL-CDIP æ•°æ®é›†ä¸Šå¯¹ LayoutLMv2 è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb)ã€‚
- å¦è¯·å‚é˜…ï¼š[æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)

<PipelineTag pipeline="question-answering"/>

- æœ‰å…³[å¦‚ä½•åœ¨ DocVQA æ•°æ®é›†ä¸Šå¯¹ LayoutLMv2 è¿›è¡Œé—®ç­”çš„å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb)ã€‚
- å¦è¯·å‚é˜…ï¼š[é—®é¢˜å›ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)
- å¦è¯·å‚é˜…ï¼š[æ–‡æ¡£é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/document_question_answering)


<PipelineTag pipeline="token-classification"/>

- æœ‰å…³[å¦‚ä½•åœ¨ CORD æ•°æ®é›†ä¸Šå¯¹ LayoutLMv2 è¿›è¡Œä»¤ç‰Œåˆ†ç±»çš„å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb)ã€‚
- æœ‰å…³[å¦‚ä½•åœ¨ FUNSD æ•°æ®é›†ä¸Šå¯¹ LayoutLMv2 è¿›è¡Œä»¤ç‰Œåˆ†ç±»çš„å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb)ã€‚
- å¦è¯·å‚é˜…ï¼š[ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)

## ç”¨æ³•ï¼šLayoutLMv2Processor

å‡†å¤‡æ¨¡å‹æ•°æ®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ [`LayoutLMv2Processor`]ï¼Œå®ƒå†…éƒ¨ç»„åˆäº†å›¾åƒå¤„ç†å™¨ï¼ˆ[`LayoutLMv2ImageProcessor`]ï¼‰å’Œåˆ†è¯å™¨ï¼ˆ[`LayoutLMv2Tokenizer`] æˆ– [`LayoutLMv2TokenizerFast`]ï¼‰ã€‚å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒæ¨¡æ€ï¼Œè€Œåˆ†è¯å™¨å¤„ç†æ–‡æœ¬æ¨¡æ€ã€‚å¤„ç†å™¨å°†ä¸¤è€…ç»“åˆèµ·æ¥ï¼Œè¿™å¯¹äºåƒ LayoutLMv2 è¿™æ ·çš„å¤šæ¨¡æ€æ¨¡å‹éå¸¸ç†æƒ³ã€‚æ³¨æ„ï¼Œä½ ä»ç„¶å¯ä»¥å•ç‹¬ä½¿ç”¨å®ƒä»¬ï¼Œå¦‚æœåªæƒ³å¤„ç†ä¸€ç§æ¨¡æ€ã€‚

```python
from transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor

image_processor = LayoutLMv2ImageProcessor()  # apply_ocr é»˜è®¤ä¸º True
tokenizer = LayoutLMv2TokenizerFast.from_pretrained("microsoft/layoutlmv2-base-uncased")
processor = LayoutLMv2Processor(image_processor, tokenizer)
```

ç®€è€Œè¨€ä¹‹ï¼Œä¸€ä¸ªå¯ä»¥æä¾›æ–‡æ¡£å›¾åƒï¼ˆä»¥åŠå¯èƒ½çš„å…¶ä»–æ•°æ®ï¼‰ç»™ [`LayoutLMv2Processor`]ï¼Œå®ƒå°†åˆ›å»ºæ¨¡å‹æ‰€éœ€çš„è¾“å…¥ã€‚åœ¨å†…éƒ¨ï¼Œå¤„ç†å™¨é¦–å…ˆä½¿ç”¨ [`LayoutLMv2ImageProcessor`] åœ¨å›¾åƒä¸Šåº”ç”¨ OCR ä»¥è·å–å•è¯å’Œè§„èŒƒåŒ–çš„è¾¹ç•Œæ¡†åˆ—è¡¨ï¼Œå¹¶å°†å›¾åƒè°ƒæ•´ä¸ºç»™å®šå¤§å°ä»¥è·å– `image` è¾“å…¥ã€‚ç„¶åï¼Œå°†è¿™äº›å•è¯å’Œè§„èŒƒåŒ–çš„è¾¹ç•Œæ¡†æä¾›ç»™ [`LayoutLMv2Tokenizer`] æˆ– [`LayoutLMv2TokenizerFast`]ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„ `input_ids`ã€`attention_mask`ã€`token_type_ids` å’Œ `bbox`ã€‚å¯é€‰åœ°ï¼Œè¿˜å¯ä»¥å°†å•è¯æ ‡ç­¾æä¾›ç»™å¤„ç†å™¨ï¼Œå®ƒä»¬å°†è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„ `labels`ã€‚

[`LayoutLMv2Processor`] åœ¨å¹•åä½¿ç”¨ [PyTesseract](https://pypi.org/project/pytesseract/)ï¼Œè¿™æ˜¯ä¸€ä¸ªå›´ç»• Google Tesseract OCR å¼•æ“çš„ Python å°è£…ã€‚è¯·æ³¨æ„ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨è‡ªå·±é€‰æ‹©çš„ OCR å¼•æ“ï¼Œå¹¶å°†å•è¯å’Œè§„èŒƒåŒ–çš„è¾¹ç•Œæ¡†æä¾›ç»™å¤„ç†å™¨ã€‚è¿™è¦æ±‚ä½¿ç”¨ `apply_ocr` è®¾ç½®ä¸º `False` åˆå§‹åŒ– [`LayoutLMv2ImageProcessor`]ã€‚

æ€»çš„æ¥è¯´ï¼Œå¤„ç†å™¨æ”¯æŒä»¥ä¸‹ 5 ç§ç”¨ä¾‹ã€‚ä¸‹é¢åˆ—å‡ºäº†æ‰€æœ‰è¿™äº›ç”¨ä¾‹ã€‚è¯·æ³¨æ„ï¼Œæ¯ä¸ªè¿™äº›ç”¨ä¾‹éƒ½é€‚ç”¨äºæ‰¹é‡å’Œéæ‰¹é‡è¾“å…¥ï¼ˆæˆ‘ä»¬ä»¥éæ‰¹é‡è¾“å…¥ä¸ºä¾‹è¿›è¡Œè¯´æ˜ï¼‰ã€‚

**ç”¨ä¾‹ 1ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨æ–­ï¼‰+ ä»¤ç‰Œåˆ†ç±»ï¼ˆæ¨æ–­ï¼‰ï¼Œapply_ocr = True**

è¿™æ˜¯æœ€ç®€å•çš„ç”¨ä¾‹ï¼Œåœ¨è¯¥ç”¨ä¾‹ä¸­å¤„ç†å™¨ï¼ˆå®é™…ä¸Šæ˜¯å›¾åƒå¤„ç†å™¨ï¼‰å°†å¯¹å›¾åƒæ‰§è¡Œ OCR ä»¥è·å–å•è¯å’Œè§„èŒƒåŒ–çš„è¾¹ç•Œæ¡†ã€‚

```python
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - å¯ä»¥æ˜¯ä½ è¦å¤„ç†çš„æ–‡æ¡£çš„ pngã€jpg ç­‰ï¼ˆPDF å¿…é¡»è½¬æ¢ä¸ºå›¾åƒï¼‰ã€‚"
).convert("RGB")
encoding = processor(
    image, return_tensors="pt"
)  # ä½ ä¹Ÿå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ æ‰€æœ‰åˆ†è¯å™¨å‚æ•°ï¼Œå¦‚ paddingã€æˆªæ–­ç­‰
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

**ç”¨ä¾‹ 2ï¼šæ–‡æ¡£å›¾åƒåˆ†ç±»ï¼ˆè®­ç»ƒã€æ¨æ–­ï¼‰+ ä»¤ç‰Œåˆ†ç±»ï¼ˆæ¨æ–­ï¼‰ï¼Œapply_ocr=False**

å¦‚æœä½ æƒ³è‡ªè¡Œæ‰§è¡Œ OCRï¼Œå¯ä»¥ä½¿ç”¨ `apply_ocr` è®¾ç½®ä¸º `False` åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥è‡ªå·±æä¾›å•è¯å’Œç›¸åº”çš„ï¼ˆè§„èŒƒåŒ–çš„ï¼‰è¾¹ç•Œæ¡†ï¼Œä»¥ä¾›å¤„ç†å™¨ä½¿ç”¨ã€‚

```python
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - å¯ä»¥æ˜¯ä½ è¦å¤„ç†çš„æ–‡æ¡£çš„ pngã€jpg ç­‰ï¼ˆPDF å¿…é¡»è½¬æ¢ä¸ºå›¾åƒï¼‰ã€‚"
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # è¯·ç¡®ä¿æ­£è§„åŒ–ä½ çš„è¾¹ç•Œæ¡†
encoding = processor(image, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

**ç”¨ä¾‹ 3ï¼šä»¤ç‰Œåˆ†ç±»ï¼ˆè®­ç»ƒï¼‰ï¼Œapply_ocr=False**

å¯¹äºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚ FUNSDã€CORDã€SROIEã€Kleister-NDAï¼‰ï¼Œè¿˜å¯ä»¥æä¾›ç›¸åº”çš„å•è¯æ ‡ç­¾ä»¥ä¾¿è®­ç»ƒæ¨¡å‹ã€‚å¤„ç†å™¨å°†è¿™äº›è½¬æ¢ä¸ºæ ‡è®°çº§åˆ«çš„ `labels`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä»…æ ‡è®°å•è¯çš„ç¬¬ä¸€ä¸ªè¯ç‰‡ï¼Œç„¶åä½¿ç”¨ -100 æ ‡è®°å‰©ä½™è¯ç‰‡ï¼Œè¿™æ˜¯ PyTorch çš„ CrossEntropyLoss çš„ `ignore_index`ã€‚å¦‚æœè¦å¯¹å•è¯çš„æ‰€æœ‰è¯ç‰‡è¿›è¡Œæ ‡è®°ï¼Œå¯ä»¥å°†åˆ†è¯å™¨çš„ `only_label_first_subword` è®¾ç½®ä¸º `False`ã€‚

```python
from transformers import LayoutLMv2Processor
from PIL import Image

```markdown
å°†ä¸‹é¢è¿™å¥è¯ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ ¼å¼æ˜¯markdownï¼Œ<>é‡Œé¢çš„ä¿ç•™åŸæ–‡ï¼Œä¹Ÿä¸è¦æ·»åŠ é¢å¤–çš„å†…å®¹ï¼š

```python
processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
word_labels = [1, 2]
encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])
```

**ç”¨ä¾‹ 4ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=True**

å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œä½ å¯ä»¥å‘å¤„ç†å™¨æä¾›ä¸€ä¸ªé—®é¢˜ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¤„ç†å™¨å°†åœ¨å›¾åƒä¸Šåº”ç”¨OCRï¼Œå¹¶åˆ›å»º[CLS]é—®é¢˜æ ‡è®°[SEP]å•è¯æ ‡è®°[SEP]ã€‚

```python
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
encoding = processor(image, question, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

**ç”¨ä¾‹ 5ï¼šè§†è§‰é—®ç­”ï¼ˆæ¨ç†ï¼‰ï¼Œapply_ocr=False**

å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆå¦‚DocVQAï¼‰ï¼Œä½ å¯ä»¥å‘å¤„ç†å™¨æä¾›ä¸€ä¸ªé—®é¢˜ã€‚å¦‚æœä½ æƒ³è‡ªå·±æ‰§è¡ŒOCRï¼Œå¯ä»¥å‘å¤„ç†å™¨æä¾›ä½ è‡ªå·±çš„å•è¯å’Œï¼ˆæ ‡å‡†åŒ–çš„ï¼‰è¾¹ç•Œæ¡†ã€‚

```python
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased", revision="no_ocr")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
question = "What's his name?"
words = ["hello", "world"]
boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes
encoding = processor(image, question, words, boxes=boxes, return_tensors="pt")
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

## LayoutLMv2Config

[[autodoc]] LayoutLMv2Config

## LayoutLMv2FeatureExtractor

[[autodoc]] LayoutLMv2FeatureExtractor
    - __call__

## LayoutLMv2ImageProcessor

[[autodoc]] LayoutLMv2ImageProcessor
    - preprocess

## LayoutLMv2Tokenizer

[[autodoc]] LayoutLMv2Tokenizer
    - __call__
    - save_vocabulary

## LayoutLMv2TokenizerFast

[[autodoc]] LayoutLMv2TokenizerFast
    - __call__

## LayoutLMv2Processor

[[autodoc]] LayoutLMv2Processor
    - __call__

## LayoutLMv2Model

[[autodoc]] LayoutLMv2Model
    - forward

## LayoutLMv2ForSequenceClassification

[[autodoc]] LayoutLMv2ForSequenceClassification

## LayoutLMv2ForTokenClassification

[[autodoc]] LayoutLMv2ForTokenClassification

## LayoutLMv2ForQuestionAnswering

[[autodoc]] LayoutLMv2ForQuestionAnswering
```