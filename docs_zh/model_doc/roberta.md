<!-- ç‰ˆæƒ 2020 å¹´ HuggingFace å›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆ ï¼ˆâ€œè®¸å¯è¯â€ï¼‰çš„è§„å®šï¼Œä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åŸºç¡€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£è®¸å¯çš„ç‰¹å®šè¯­è¨€å’Œé™åˆ¶ã€‚

âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨ Markdown ç¼–å†™ï¼Œä½†åŒ…å«æˆ‘ä»¬æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œè¿™å¯èƒ½æ— æ³•åœ¨ä½ çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ¸²æŸ“ã€‚
-->

# RoBERTa

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=roberta">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-roberta-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/roberta-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
<a href="https://huggingface.co/papers/1907.11692">
<img alt="Paper page" src="https://img.shields.io/badge/Paper%20page-1907.11692-green">
</a>
</div>

## æ¦‚è¿°

RoBERTa æ¨¡å‹æ˜¯ç”± Yinhan Liuã€Myle Ottã€Naman Goyalã€Jingfei Duã€Mandar Joshiã€Danqi Chenã€Omer Levyã€Mike Lewisã€Luke Zettlemoyerã€Veselin Stoyanov åœ¨æ–‡ç«  [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) ä¸­æå‡ºçš„ã€‚å®ƒåŸºäº Google åœ¨2018å¹´å‘å¸ƒçš„ BERT æ¨¡å‹ã€‚

RoBERTa åœ¨ BERT çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œä¿®æ”¹äº†å…³é”®è¶…å‚æ•°ï¼Œå»é™¤äº†ä¸‹ä¸€ä¸ªå¥å­é¢„è®­ç»ƒç›®æ ‡ï¼Œå¹¶é‡‡ç”¨äº†æ›´å¤§çš„å°æ‰¹é‡å’Œå­¦ä¹ ç‡è¿›è¡Œè®­ç»ƒã€‚

ä¸‹é¢æ˜¯æ¥è‡ªè®ºæ–‡çš„æ‘˜è¦ï¼š

*è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†æ˜¯ä¸åŒæ–¹æ³•ä¹‹é—´çš„ç»†è‡´æ¯”è¾ƒæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚è®­ç»ƒæ˜¯è®¡ç®—å¯†é›†å‹çš„ï¼Œé€šå¸¸åœ¨ä¸åŒå¤§å°çš„ç§æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¹¶ä¸”æ­£å¦‚æˆ‘ä»¬å°†è¦å±•ç¤ºçš„ï¼Œè¶…å‚æ•°é€‰æ‹©å¯¹æœ€ç»ˆç»“æœæœ‰é‡è¦å½±å“ã€‚æˆ‘ä»¬å¯¹ BERT é¢„è®­ç»ƒï¼ˆDevlin et al., 2019ï¼‰è¿›è¡Œäº†ä¸€é¡¹å¤åˆ¶æ€§ç ”ç©¶ï¼Œè®¤çœŸæµ‹é‡äº†è®¸å¤šå…³é”®è¶…å‚æ•°å’Œè®­ç»ƒæ•°æ®å¤§å°çš„å½±å“ã€‚æˆ‘ä»¬å‘ç° BERT è®­ç»ƒä¸è¶³ï¼Œå¯ä»¥ä¸ä¹‹åå‘å¸ƒçš„æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ç”šè‡³è¶…è¿‡ã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨ GLUEã€RACE å’Œ SQuAD ä¸Šå–å¾—äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä»¥å‰è¢«å¿½è§†çš„è®¾è®¡é€‰æ‹©çš„é‡è¦æ€§ï¼Œå¹¶å¯¹æœ€è¿‘æŠ¥é“çš„æ”¹è¿›æ¥æºæå‡ºäº†è´¨ç–‘ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç ã€‚*

æç¤ºï¼š

- æ­¤å®ç°ä¸ [`BertModel`] ç›¸åŒï¼Œåªæ˜¯è¿›è¡Œäº†å¾®å°çš„åµŒå…¥è°ƒæ•´ï¼Œå¹¶ä¸º RoBERTa é¢„è®­ç»ƒæ¨¡å‹è®¾ç½®ã€‚
- RoBERTa ä¸ BERT å…·æœ‰ç›¸åŒçš„æ¶æ„ï¼Œä½†ä½¿ç”¨å­—èŠ‚çº§çš„ BPE ä½œä¸ºåˆ†è¯å™¨ï¼ˆä¸ GPT-2 ç›¸åŒï¼‰ï¼Œå¹¶é‡‡ç”¨äº†ä¸åŒçš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚
- RoBERTa ä¸ä½¿ç”¨ `token_type_ids`ï¼Œä½ æ— éœ€æŒ‡ç¤ºå“ªä¸ªæ ‡è®°å±äºå“ªä¸ªç‰‡æ®µã€‚åªéœ€ä½¿ç”¨åˆ†éš”æ ‡è®° `tokenizer.sep_token`ï¼ˆæˆ– `</s>`ï¼‰å°†ç‰‡æ®µåˆ†éš”å¼€å³å¯ã€‚
- ä¸ BERT ç›¸åŒï¼Œä½†å…·æœ‰æ›´å¥½çš„é¢„è®­ç»ƒæŠ€å·§ï¼š

    - åŠ¨æ€é®è”½ï¼šåœ¨æ¯ä¸ªæ—¶æœŸå¯¹æ ‡è®°è¿›è¡Œä¸åŒæ–¹å¼çš„é®è”½ï¼Œè€Œ BERT ä»…ä¸€æ¬¡æ€§è¿›è¡Œé®è”½
      - ä¸€èµ·è¾¾åˆ° 512 ä¸ªæ ‡è®°çš„ç›®æ ‡ï¼ˆå› æ­¤å¥å­çš„é¡ºåºå¯èƒ½æ¶µç›–å¤šä¸ªæ–‡æ¡£ï¼‰
      - ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒ
      - ä½¿ç”¨å­—èŠ‚ä½œä¸ºå­å•å…ƒçš„ BPEï¼Œè€Œä¸æ˜¯å­—ç¬¦ï¼ˆå› ä¸ºæ¶‰åŠåˆ° Unicode å­—ç¬¦ï¼‰
- [CamemBERT](camembert) æ˜¯ RoBERTa çš„ä¸€ä¸ªå°è£…ã€‚è¯·å‚è€ƒæ­¤é¡µé¢äº†è§£ä½¿ç”¨ç¤ºä¾‹ã€‚

æ­¤æ¨¡å‹ç”± [julien-c](https://huggingface.co/julien-c) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/pytorch/fairseq/tree/master/examples/roberta) æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ æ ‡è¯†ï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©ä½ å¼€å§‹ä½¿ç”¨ RoBERTaã€‚å¦‚æœä½ æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶å‘èµ·æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼æ­¤èµ„æºåº”è¯¥å±•ç¤ºå‡ºæŸç§æ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚

<PipelineTag pipeline="text-classification"/>

- ä¸€ç¯‡å…³äºä½¿ç”¨ RoBERTa å’Œ [Inference API](https://huggingface.co/inference-api) è¿›è¡Œ Twitter æƒ…æ„Ÿåˆ†æçš„åšå®¢ï¼š[å…¥é—¨ï¼šä½¿ç”¨ RoBERTa å¯¹ Twitter è¿›è¡Œæƒ…æ„Ÿåˆ†æ](https://huggingface.co/blog/sentiment-analysis-twitter)ã€‚
- ä¸€ç¯‡ [ä½¿ç”¨ Kili å’Œ Hugging Face AutoTrain è¿›è¡Œæ„è§åˆ†ç±»](https://huggingface.co/blog/opinion-classification-with-kili) çš„åšå®¢ï¼Œä½¿ç”¨äº† RoBERTaã€‚
- ä¸€ç¯‡å…³äºå¦‚ä½• [å¯¹ RoBERTa è¿›è¡Œæƒ…æ„Ÿåˆ†æå¾®è°ƒ](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) çš„ç¬”è®°æœ¬ã€‚ğŸŒ
- [`RobertaForSequenceClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb) çš„æ”¯æŒã€‚
- [`TFRobertaForSequenceClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb) çš„æ”¯æŒã€‚
- [`FlaxRobertaForSequenceClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb) çš„æ”¯æŒã€‚
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- [`RobertaForTokenClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb) çš„æ”¯æŒã€‚
- [`TFRobertaForTokenClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb) çš„æ”¯æŒã€‚
- [`FlaxRobertaForTokenClassification`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification) çš„æ”¯æŒã€‚
- ğŸ¤—Hugging Face è¯¾ç¨‹ä¸­çš„ [tokenåˆ†ç±»](https://huggingface.co/course/chapter7/2?fw=pt) ç« èŠ‚ã€‚
- [tokenåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)

<PipelineTag pipeline="fill-mask"/>

- ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨ Transformers å’Œ Tokenizers ä»å¤´å¼€å§‹è®­ç»ƒæ–°è¯­è¨€æ¨¡å‹ï¼ˆä½¿ç”¨ RoBERTaï¼‰çš„åšå®¢ï¼š[å¦‚ä½•è®­ç»ƒæ–°çš„è¯­è¨€æ¨¡å‹](https://huggingface.co/blog/how-to-train)ã€‚
- [`RobertaForMaskedLM`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) çš„æ”¯æŒã€‚
- [`TFRobertaForMaskedLM`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb) çš„æ”¯æŒã€‚
- [`FlaxRobertaForMaskedLM`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb) çš„æ”¯æŒã€‚
- ğŸ¤—Hugging Face è¯¾ç¨‹ä¸­çš„ [æ©ç è¯­è¨€å»ºæ¨¡](https://huggingface.co/course/chapter7/3?fw=pt) ç« èŠ‚ã€‚
- [æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/masked_language_modeling)

<PipelineTag pipeline="question-answering"/>

- ä¸€ç¯‡å…³äºä½¿ç”¨ RoBERTa è¿›è¡Œé—®ç­”çš„åšå®¢ï¼š[åˆ©ç”¨Optimumå’ŒTransformers Pipelineså®ç°åŠ é€Ÿæ¨ç†](https://huggingface.co/blog/optimum-inference)ã€‚
- [`RobertaForQuestionAnswering`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb) çš„æ”¯æŒã€‚
- [`TFRobertaForQuestionAnswering`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb) çš„æ”¯æŒã€‚
- [`FlaxRobertaForQuestionAnswering`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering) çš„æ”¯æŒã€‚
- ğŸ¤—Hugging Face è¯¾ç¨‹ä¸­çš„ [é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt) ç« èŠ‚ã€‚
- [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)

**å¤šé€‰**
- [`RobertaForMultipleChoice`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb) çš„æ”¯æŒã€‚
- [`TFRobertaForMultipleChoice`] å—æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb) çš„æ”¯æŒã€‚
- [å¤šé€‰ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)

## RobertaConfig

[[autodoc]] RobertaConfig

## RobertaTokenizer

[[autodoc]] RobertaTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## RobertaTokenizerFast

[[autodoc]] RobertaTokenizerFast
    - build_inputs_with_special_tokens

## RobertaModel

[[autodoc]] RobertaModel
    - forward

## RobertaForCausalLM

[[autodoc]] RobertaForCausalLM
    - forward

## RobertaForMaskedLM

[[autodoc]] RobertaForMaskedLM
    - forward

## RobertaForSequenceClassification

[[autodoc]] RobertaForSequenceClassification
    - forward

## RobertaForMultipleChoice

[[autodoc]] RobertaForMultipleChoice
    - forward

## RobertaForTokenClassification

[[autodoc]] RobertaForTokenClassification
    - forward

## RobertaForQuestionAnswering

[[autodoc]] RobertaForQuestionAnswering
    - forward

## TFRobertaModel

[[autodoc]] TFRobertaModel
    - call

## TFRobertaForCausalLM

[[autodoc]] TFRobertaForCausalLM
    - call

## TFRobertaForMaskedLM

[[autodoc]] TFRobertaForMaskedLM
    - call

## TFRobertaForSequenceClassification

[[autodoc]] TFRobertaForSequenceClassification
    - call

## TFRobertaForMultipleChoice

[[autodoc]] TFRobertaForMultipleChoice
    - call

## TFRobertaForTokenClassification

[[autodoc]] TFRobertaForTokenClassification
    - call

## TFRobertaForQuestionAnswering

[[autodoc]] TFRobertaForQuestionAnswering
    - call

## FlaxRobertaModel

[[autodoc]] FlaxRobertaModel
    - __call__

## FlaxRobertaForCausalLM

[[autodoc]] FlaxRobertaForCausalLM
    - __call__

## FlaxRobertaForMaskedLM

[[autodoc]] FlaxRobertaForMaskedLM
    - __call__

## FlaxRobertaForSequenceClassification

[[autodoc]] FlaxRobertaForSequenceClassification
    - __call__

## FlaxRobertaForMultipleChoice

[[autodoc]] FlaxRobertaForMultipleChoice
    - __call__

## FlaxRobertaForTokenClassification

[[autodoc]] FlaxRobertaForTokenClassification
    - __call__

## FlaxRobertaForQuestionAnswering

[[autodoc]] FlaxRobertaForQuestionAnswering
    - __call__