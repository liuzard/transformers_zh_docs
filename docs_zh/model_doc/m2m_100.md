<!--
版权所有2020年 The HuggingFace团队。版权所有。

根据Apache许可证第2版（“许可证”）授权；除非符合许可证规定，否则您不得使用此文件。您可以在以下网址获得许可证的副本：

http://www.apache.org/licenses/LICENSE-2.0

除非适用的法律要求或书面同意，根据许可证分发的软件是基于“原样”分布的，不附带任何保证或条件，无论是明示的还是暗示的。有关特定语言的具体条款，请参见许可证。

⚠️ 请注意，该文件采用的是Markdown格式，但包含了我们文档生成器（类似于MDX）的特定语法，可能在您的Markdown查看器中无法正确显示。

-->

# M2M100

## 概述

M2M100模型是由Angela Fan、Shruti Bhosale、Holger Schwenk、Zhiyi Ma、Ahmed El-Kishky、Siddharth Goyal、Mandeep Baines、Onur Celebi、Guillaume Wenzek、Vishrav Chaudhary、Naman Goyal、Tom Birch、Vitaliy Liptchinsky、Sergey Edunov、Edouard Grave、Michael Auli和Armand Joulin于[《Beyond English-Centric Multilingual Machine Translation》](https://arxiv.org/abs/2010.11125)提出的。

该文摘如下：

*先前的翻译工作通过训练单个模型，实现了大规模多语言机器翻译的潜力。然而，这项工作大部分是基于英语的，只在从英语翻译出来或翻译成英语的数据上进行训练。尽管这样做有大量的训练数据支持，但它并不能反映出全球范围内的翻译需求。在这项工作中，我们创建了一个真正的Many-to-Many多语言翻译模型，它可以直接在100种语言之间进行翻译。我们构建并开源了一个训练数据集，通过大规模挖掘创建了数千个语言方向的监督数据。然后，我们探索如何通过稠密扩展和语言特定的稀疏参数的组合有效地增加模型容量，从而创建高质量的模型。我们专注于非英语为中心的模型，在直接翻译非英语方向时，相比WMT的最佳单一系统，我们的模型可以获得超过10 BLEU的增益。我们开源了我们的脚本，以便其他人可以复现这些数据、评估和最终的M2M-100模型。*

该模型由[valhalla](https://huggingface.co/valhalla)贡献。

### 训练和生成

M2M100是一个多语言编码器-解码器（seq-to-seq）模型，主要用于翻译任务。由于该模型是多语言的，因此它期望以特定格式接收序列：在源文本和目标文本中都使用特殊的语言id令牌作为前缀。源文本的格式为`[lang_code] X [eos]`，其中`lang_code`是源文本的源语言id或目标文本的目标语言id，`X`是源文本或目标文本。

[`M2M100Tokenizer`]依赖于`sentencepiece`，因此在运行示例之前，请确保先安装它。可以使用`pip install sentencepiece`来安装`sentencepiece`。

- 监督训练

```python
from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer

model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="en", tgt_lang="fr")

src_text = "Life is like a box of chocolates."
tgt_text = "La vie est comme une boîte de chocolat."

model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors="pt")

loss = model(**model_inputs).loss  # 前向传递
```

- 生成

  在生成过程中，M2M100使用`eos_token_id`作为`decoder_start_token_id`，并将目标语言id强制作为生成的第一个token。为了强制使用目标语言id作为生成的第一个token，请将*forced_bos_token_id*参数传递给*generate*方法。以下示例展示了如何使用*facebook/m2m100_418M*检查点进行从印地语到法语和从中文到英语的翻译。

```python
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> hi_text = "जीवन एक चॉकलेट बॉक्स की तरह है।"
>>> chinese_text = "生活就像一盒巧克力。"

>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

>>> # 将印地语翻译为法语
>>> tokenizer.src_lang = "hi"
>>> encoded_hi = tokenizer(hi_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"La vie est comme une boîte de chocolat."

>>> # 将中文翻译为英语
>>> tokenizer.src_lang = "zh"
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Life is like a box of chocolate."
```

## 文档资源

- [翻译任务指南](../tasks/translation)
- [摘要任务指南](../tasks/summarization)

## M2M100Config

[[autodoc]] M2M100Config

## M2M100Tokenizer

[[autodoc]] M2M100Tokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## M2M100Model

[[autodoc]] M2M100Model
    - forward

## M2M100ForConditionalGeneration

[[autodoc]] M2M100ForConditionalGeneration
    - forward
  -->