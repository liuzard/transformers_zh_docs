<!--ç‰ˆæƒæ‰€æœ‰2022å¹´The HuggingFaceå›¢é˜Ÿã€‚ ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›æ‚¨é™¤ééµå®ˆæ­¤è®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€ BASIS æ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…æœ‰å…³è®¸å¯è¯çš„è¯¦ç»†ä¿¡æ¯

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯Markdownæ ¼å¼çš„ï¼Œä½†åŒ…å«é’ˆå¯¹æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äºMDXï¼‰ã€‚å› æ­¤åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£å¸¸å‘ˆç°ã€‚ -->

# åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†

é™¤äº†æœ¬æŒ‡å—å¤–ï¼Œè¿˜å¯ä»¥åœ¨ [ä½¿ç”¨å•ä¸ªGPUè¿›è¡Œè®­ç»ƒæŒ‡å—](perf_train_gpu_one.md) å’Œ [åœ¨CPUä¸Šè¿›è¡Œæ¨ç†æŒ‡å—](perf_infer_cpu.md) ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚

## BetterTransformer

[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) å°† ğŸ¤— Transformers æ¨¡å‹è½¬æ¢ä¸ºä½¿ç”¨ PyTorch æœ¬åœ°çš„å¿«é€Ÿè·¯å¾„æ‰§è¡Œï¼Œè¯¥æ‰§è¡Œè°ƒç”¨äº†ä¼˜åŒ–çš„æ ¸å‡½æ•°ï¼Œå¦‚ Flash Attentionã€‚  

BetterTransformer è¿˜æ”¯æŒæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘æ¨¡å‹çš„åœ¨å•ä¸ªå’Œå¤šä¸ªGPUä¸Šè¿›è¡Œæ›´å¿«çš„æ¨ç†ã€‚

<Tip>

Flash Attention ä»…é€‚ç”¨äºä½¿ç”¨ fp16 æˆ– bf16 æ•°æ®ç±»å‹çš„æ¨¡å‹ã€‚åœ¨ä½¿ç”¨ BetterTransformer ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„ dtypeã€‚
  
</Tip>

### ç¼–ç å™¨æ¨¡å‹

ä½¿ç”¨ PyTorch æœ¬åœ°çš„ [`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) æ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„ï¼Œåä¸º BetterTransformerï¼Œå¯ä»¥é€šè¿‡ [ğŸ¤— Optimum åº“](https://huggingface.co/docs/optimum/bettertransformer/overview) ä¸­çš„é›†æˆä¸ Transformers ç»“åˆä½¿ç”¨ã€‚

PyTorch çš„æ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„é€šè¿‡å†…æ ¸èåˆå’Œä½¿ç”¨ [åµŒå¥—å¼ é‡](https://pytorch.org/docs/stable/nested.html) æ¥åŠ é€Ÿæ¨ç†ã€‚è¯¦ç»†çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨ [æ­¤åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) ä¸­æ‰¾åˆ°ã€‚

åœ¨å®‰è£… [`optimum`](https://github.com/huggingface/optimum) åŒ…ä¹‹åï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ Better Transformerï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨[`~PreTrainedModel.to_bettertransformer`]æ¥æ›¿æ¢ç›¸å…³çš„å†…éƒ¨æ¨¡å—ï¼š

```python
model = model.to_bettertransformer()
```

æ–¹æ³•[`~PreTrainedModel.reverse_bettertransformer`]å¯ä»¥ä½¿æ¨¡å‹æ¢å¤åˆ°åŸå§‹çš„å»ºæ¨¡æ–¹å¼ï¼Œåº”è¯¥åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰ä½¿ç”¨ï¼Œä»¥ä¾¿ä½¿ç”¨è§„èŒƒçš„ transformers å»ºæ¨¡æ–¹å¼ï¼š

```python
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

è¯·æŸ¥çœ‹æ­¤ [åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) ä»¥äº†è§£æœ‰å…³ä½¿ç”¨ `BetterTransformer` API é’ˆå¯¹ç¼–ç å™¨æ¨¡å‹å¯ä»¥åšäº›ä»€ä¹ˆçš„æ›´å¤šä¿¡æ¯ã€‚

### è§£ç å™¨æ¨¡å‹

å¯¹äºæ–‡æœ¬æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºè§£ç å™¨çš„æ¨¡å‹ï¼ˆå¦‚ GPTã€T5ã€Llama ç­‰ï¼‰ï¼ŒBetterTransformer API å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºä½¿ç”¨ [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼ˆSDPAï¼‰è¿ç®—ç¬¦çš„æ“ä½œï¼ˆæ­¤è¿ç®—ç¬¦ä»…åœ¨ PyTorch 2.0 åŠæ›´é«˜ç‰ˆæœ¬ä¸­å¯ç”¨ï¼‰ã€‚

è¦å°†æ¨¡å‹è½¬æ¢ä¸º BetterTransformerï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
# å°†æ¨¡å‹è½¬æ¢ä¸º BetterTransformer
model.to_bettertransformer()

# ç”¨äºè®­ç»ƒæˆ–æ¨ç†
```

SDPA ä¹Ÿå¯ä»¥åœ¨å†…éƒ¨è°ƒç”¨ [Flash Attention](https://arxiv.org/abs/2205.14135) çš„æ ¸å‡½æ•°ã€‚è¦å¯ç”¨ Flash Attention æˆ–æ£€æŸ¥å®ƒåœ¨ç»™å®šç¯å¢ƒï¼ˆç¡¬ä»¶ã€é—®é¢˜è§„æ¨¡ï¼‰ä¸­æ˜¯å¦å¯ç”¨ï¼Œè¯·ä½¿ç”¨ [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š

```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# å°†æ¨¡å‹è½¬æ¢ä¸º BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

å¦‚æœä½ çœ‹åˆ°ä¸€ä¸ªå¸¦æœ‰å›æº¯çš„é”™è¯¯ï¼Œè¯¥é”™è¯¯æç¤ºä¸ºï¼š

```bash
RuntimeError: No available kernel.  Aborting execution.
```

è¯·å°è¯•ä½¿ç”¨ PyTorch æ¯å¤œç‰ˆï¼Œè¯¥ç‰ˆæœ¬çš„ Flash Attention å¯èƒ½å…·æœ‰æ›´å¹¿çš„è¦†ç›–èŒƒå›´ï¼š

```bash
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

æˆ–ç¡®ä¿æ‚¨çš„æ¨¡å‹æ­£ç¡®è½¬æ¢ä¸º float16 æˆ– bfloat16 å­—ç¬¦ä¸²


è¯·æŸ¥çœ‹è¿™ä¸ª [è¯¦ç»†åšæ–‡](https://pytorch.org/blog/out-of-the-box-acceleration/) ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨ `BetterTransformer` + SDPA API æ¥è·å–æ›´å¤šåŠŸèƒ½ã€‚

## ä½¿ç”¨ FP4 half-precision æ··åˆç²¾åº¦è¿›è¡Œæ¨ç†çš„ `bitsandbytes` é›†æˆ

æ‚¨å¯ä»¥å®‰è£… `bitsandbytes` å¹¶ä»ä¸­å—ç›Šï¼Œä»¥ä¾¿åœ¨ GPU ä¸Šè½»æ¾å‹ç¼©æ¨¡å‹ã€‚ ä½¿ç”¨ FP4 é‡åŒ–ï¼Œä¸åŸå§‹å…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œå¯ä»¥å°†æ¨¡å‹å¤§å°å‡å°å¤šè¾¾ 8 å€ã€‚ è¯·æŸ¥çœ‹ä¸‹é¢å¦‚ä½•å¼€å§‹ã€‚

<Tip>

è¯·æ³¨æ„ï¼Œæ­¤åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨å¤šä¸ª GPU è®¾ç½®ä¸­ä½¿ç”¨ã€‚

</Tip>

### è¦æ±‚ [[è¦æ±‚-ç”¨äº-fp4-half-precision-æ¨ç†]]

- æœ€æ–°çš„ `bitsandbytes` åº“
`pip install bitsandbytes>=0.39.0`

- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `accelerate`
`pip install git+https://github.com/huggingface/accelerate.git`

- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `transformers`
`pip install git+https://github.com/huggingface/transformers.git`

### è¿è¡Œ FP4 æ¨¡å‹ - å• GPU è®¾ç½® - å¿«é€Ÿå…¥é—¨

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç å¿«é€Ÿåœ¨å•ä¸ª GPU ä¸Šè¿è¡Œ FP4 æ¨¡å‹ï¼š

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```
è¯·æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†åœ¨æ¨ç†æ—¶è®¾ç½® `device_map = 'auto'` æ˜¯æ¨èçš„ï¼Œå› ä¸ºå®ƒå°†æ¨¡å‹æœ‰æ•ˆåœ°åˆ†æ´¾åˆ°å¯ç”¨èµ„æºä¸Šã€‚

### è¿è¡Œ FP4 æ¨¡å‹ - å¤š GPU è®¾ç½®

å°†æ··åˆ 4 ä½æ¨¡å‹åŠ è½½åˆ°å¤šä¸ª GPU ä¸­çš„æ–¹æ³•ä¸å•ä¸ª GPU è®¾ç½®ç›¸åŒï¼ˆä¸å• GPU è®¾ç½®ç›¸åŒçš„å‘½ä»¤ï¼‰ï¼š
```py
model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```
ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `accelerate` æ¥æ§åˆ¶æ¯ä¸ª GPU ä¸Šè¦åˆ†é…çš„ GPU å†…å­˜ã€‚ä½¿ç”¨ `max_memory` å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```
åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç¬¬ä¸€ä¸ª GPU å°†ä½¿ç”¨ 600MB çš„å†…å­˜ï¼Œç¬¬äºŒä¸ª GPU å°†ä½¿ç”¨ 1GBã€‚

### é«˜çº§ç”¨æ³•

æœ‰å…³æ­¤æ–¹æ³•çš„æ›´é«˜çº§ç”¨æ³•ï¼Œè¯·å‚é˜… [é‡åŒ–](main_classes/quantization) æ–‡æ¡£é¡µé¢ã€‚

## ä½¿ç”¨ Int8 æ··åˆç²¾åº¦çŸ©é˜µåˆ†è§£çš„ `bitsandbytes` é›†æˆ

<Tip>

è¯·æ³¨æ„ï¼Œæ­¤åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨å¤šä¸ª GPU è®¾ç½®ä¸­ä½¿ç”¨ã€‚

</Tip>

ä»è®ºæ–‡ [`LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale`](https://arxiv.org/abs/2208.07339)ï¼Œæˆ‘ä»¬æ”¯æŒåœ¨å‡ è¡Œä»£ç ä¸­è¿›è¡Œ Hugging Face é›†æˆã€‚è¯¥æ–¹æ³•é€šè¿‡ 8 ä½å¼ é‡æ ¸å¿ƒï¼ˆfp16 æƒé‡ä¸º 2 å€ï¼Œfp32 æƒé‡ä¸º 4 å€ï¼‰æ“ä½œåŠç²¾åº¦çš„è€æ‚æŠ€æ¥æä¾›ç»“æœæ€§èƒ½çš„æ–¹æ³•ã€‚

![HFxbitsandbytes.png](https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png)

Int8 æ··åˆç²¾åº¦çŸ©é˜µåˆ†è§£é€šè¿‡å°†çŸ©é˜µä¹˜æ³•åˆ†è§£ä¸ºä¸¤ä¸ªæµè¿›è¡Œæ“ä½œï¼šï¼ˆ1ï¼‰åœ¨ fp16 ä¸­è¿›è¡Œçš„ç³»ç»ŸåŒ–ç‰¹å¾å¼‚å¸¸å€¼æµçŸ©é˜µä¹˜æ³•ï¼ˆ0.01%ï¼‰ï¼Œï¼ˆ2ï¼‰int8 çŸ©é˜µä¹˜æ³•æ“ä½œçš„å¸¸è§„æµï¼ˆ99.9%ï¼‰ã€‚å€ŸåŠ©è¯¥æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸æŸå¤±é¢„æµ‹æ€§èƒ½çš„æƒ…å†µä¸‹è¿›è¡Œé€‚ç”¨äºéå¸¸å¤§æ¨¡å‹çš„ int8 æ¨ç†ã€‚æœ‰å…³è¯¥æ–¹æ³•çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[è®ºæ–‡](https://arxiv.org/abs/2208.07339) æˆ–æˆ‘ä»¬çš„[å…³äºé›†æˆçš„åšæ–‡](https://huggingface.co/blog/hf-bitsandbytes-integration)ã€‚

![MixedInt8.gif](https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif)

è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ GPU æ‰èƒ½è¿è¡Œæ··åˆ 8 ä½æ¨¡å‹ï¼Œå› ä¸ºæ ¸å·²ç¼–è¯‘ä¸ºä»…é€‚ç”¨äº GPUã€‚ç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ¥å­˜å‚¨æ¨¡å‹çš„å››åˆ†ä¹‹ä¸€ï¼ˆå¦‚æœæ‚¨çš„æ¨¡å‹æƒé‡ä¸ºåŠç²¾åº¦ï¼Œåˆ™æ˜¯äºŒåˆ†ä¹‹ä¸€ï¼‰ä¹‹å‰ï¼Œä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›æç¤ºï¼Œä»¥å¸®åŠ©æ‚¨ä½¿ç”¨æ­¤æ¨¡å—ï¼Œæˆ–è€…æŒ‰ç…§ [Google colab çš„æ¼”ç¤º](#colab-demos) è¿›è¡Œæ¼”ç¤ºã€‚

### è¦æ±‚ [[requirements-for-int8-mixedprecision-matrix-decomposition]]

- å¦‚æœæ‚¨çš„ `bitsandbytes<0.37.0`ï¼Œè¯·ç¡®ä¿æ‚¨åœ¨æ”¯æŒ 8 ä½å¼ é‡æ ¸å¿ƒçš„ NVIDIA GPU ä¸Šè¿è¡Œï¼ˆå›¾çµã€å®‰åŸ¹æˆ–æ›´æ–°æ¶æ„ - ä¾‹å¦‚ T4ã€RTX20sã€RTX30sã€A40-A100ï¼‰ã€‚å¯¹äº `bitsandbytes>=0.37.0`ï¼Œåº”æ”¯æŒæ‰€æœ‰ GPUã€‚
- é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…æ­£ç¡®çš„ `bitsandbytes` ç‰ˆæœ¬ï¼š
`pip install bitsandbytes>=0.31.5`
- å®‰è£… `accelerate`
`pip install accelerate>=0.12.0`

### è¿è¡Œæ··åˆ Int8 æ¨¡å‹ - å• GPU è®¾ç½®

åœ¨å®‰è£…æ‰€éœ€åº“ä¹‹åï¼ŒåŠ è½½æ··åˆ 8 ä½æ¨¡å‹çš„æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

å¯¹äºæ–‡æœ¬ç”Ÿæˆï¼Œæˆ‘ä»¬å»ºè®®ï¼š

* ä½¿ç”¨æ¨¡å‹çš„ `generate()` æ–¹æ³•è€Œä¸æ˜¯ `pipeline()` å‡½æ•°ã€‚å°½ç®¡ä½¿ç”¨ `pipeline()` å‡½æ•°å¯ä»¥è¿›è¡Œæ¨ç†ï¼Œä½†å®ƒä¸é’ˆå¯¹æ··åˆ 8 ä½æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤ä¸ä½¿ç”¨ `generate()` æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å¤–ï¼Œä¸€äº›é‡‡æ ·ç­–ç•¥ï¼Œå¦‚å…·æœ‰æ ¸å¿ƒåºåˆ—çš„é‡‡æ ·ï¼Œåœ¨æ··åˆ 8 ä½æ¨¡å‹çš„ `pipeline()` å‡½æ•°ä¸­ä¸å—æ”¯æŒã€‚
* å°†æ‰€æœ‰è¾“å…¥éƒ½æ”¾åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Šã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

### è¿è¡Œæ··åˆ Int8 æ¨¡å‹ - å¤š GPU è®¾ç½®

åœ¨å¤šä¸ª GPU ä¸­åŠ è½½æ··åˆ 8 ä½æ¨¡å‹çš„æ–¹æ³•ä¸å•ä¸ª GPU è®¾ç½®ç›¸åŒï¼ˆä¸å• GPU è®¾ç½®ç›¸åŒçš„å‘½ä»¤ï¼‰ï¼š
```py
model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `accelerate` æ¥æ§åˆ¶è¦åœ¨æ¯ä¸ª GPU ä¸Šåˆ†é…çš„ GPU å†…å­˜ã€‚ä½¿ç”¨ `max_memory` å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```
åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç¬¬ä¸€ä¸ª GPU å°†ä½¿ç”¨ 1GB å†…å­˜ï¼Œç¬¬äºŒä¸ª GPU å°†ä½¿ç”¨ 2GBã€‚

### Google Colab æ¼”ç¤º

ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥å‰æ— æ³•åœ¨ Google Colab ä¸Šè¿è¡Œçš„æ¨¡å‹ä¸Šè¿›è¡Œæ¨ç†ã€‚
æŸ¥çœ‹åœ¨ Google Colab ä¸Šè¿è¡Œ T5-11bï¼ˆ42GB çš„ fp32ï¼‰çš„æ¼”ç¤ºï¼ä½¿ç”¨ 8 ä½é‡åŒ–ï¼š

[![åœ¨ Colab ä¸­æ‰“å¼€ï¼šT5-11b æ¼”ç¤º](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)

æˆ–è€…æŸ¥çœ‹ BLOOM-3B çš„æ¼”ç¤ºï¼š

[![åœ¨ Colab ä¸­æ‰“å¼€ï¼šBLOOM-3b æ¼”ç¤º](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)

## é«˜çº§ç”¨æ³•ï¼šFP4ï¼ˆæˆ–Int8ï¼‰å’ŒBetterTransformer æ··åˆ

æ‚¨å¯ä»¥ç»„åˆä¸Šè¿°ä¸åŒçš„æ–¹æ³•ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åœ¨ FP4 æ··åˆç²¾åº¦æ¨ç† + flash attention ä¸­ä½¿ç”¨ BetterTransformerï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
