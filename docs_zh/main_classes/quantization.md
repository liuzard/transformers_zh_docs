<!--ç‰ˆæƒ2023å¹´HuggingFaceå›¢é˜Ÿã€‚ç‰ˆæƒæ‰€æœ‰ã€‚

æ ¹æ®Apacheè®¸å¯è¯ï¼Œç¬¬2ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯;é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä½ å°†ä¸èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯ä»¥â€œæŒ‰åŸæ ·â€åˆ†å‘çš„ï¼Œ
ä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–
ç‰¹å®šè¯­è¨€ä¸‹æˆæƒçš„æƒé™å’Œé™åˆ¶ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨Markdownæ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½åœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ— æ³•æ­£ç¡®å‘ˆç°ã€‚

-->

# é‡åŒ–ğŸ¤—Transformersæ¨¡å‹

## `AutoGPTQ`é›†æˆ

ğŸ¤—Transformerså·²ç»é›†æˆäº†`optimum` APIï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹æ‰§è¡ŒGPTQé‡åŒ–ã€‚ä½ å¯ä»¥åœ¨8ã€4ã€3ç”šè‡³2ä¸ªæ¯”ç‰¹ä¸­åŠ è½½å’Œé‡åŒ–æ¨¡å‹ï¼Œè€Œæ€§èƒ½ä¸‹é™å¾ˆå°ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼è¿™æ˜¯ç”±å¤§å¤šæ•°GPUç¡¬ä»¶æ”¯æŒçš„ã€‚

è¦äº†è§£æœ‰å…³é‡åŒ–æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š
- [GPTQ](https://arxiv.org/pdf/2210.17323.pdf)è®ºæ–‡
- `optimum` [æŒ‡å—](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)ä¸Šçš„GPTQé‡åŒ–
- [`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ)åº“ç”¨ä½œåç«¯

### è¦æ±‚

è¦è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œä½ éœ€è¦å®‰è£…ä»¥ä¸‹è¦æ±‚ï¼š

- å®‰è£…æœ€æ–°çš„`AutoGPTQ`åº“
`pip install auto-gptq`

- å®‰è£…æœ€æ–°çš„`optimum`æºç 
`pip install git+https://github.com/huggingface/optimum.git`

- å®‰è£…æœ€æ–°çš„`transformers`æºç 
`pip install git+https://github.com/huggingface/transformers.git`

- å®‰è£…æœ€æ–°çš„`accelerate`åº“
`pip install --upgrade accelerate`

è¯·æ³¨æ„ï¼ŒGPTQé›†æˆç›®å‰ä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼Œå¯¹äºè§†è§‰ã€è¯­éŸ³æˆ–å¤šæ¨¡å¼æ¨¡å‹ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°æ„å¤–è¡Œä¸ºã€‚

### åŠ è½½å’Œé‡åŒ–æ¨¡å‹

GPTQæ˜¯ä¸€ç§é‡åŒ–æ–¹æ³•ï¼Œåœ¨ä½¿ç”¨é‡åŒ–æ¨¡å‹ä¹‹å‰éœ€è¦è¿›è¡Œæƒé‡æ ¡å‡†ã€‚å¦‚æœä½ æƒ³ä»å¤´å¼€å§‹é‡åŒ–transformersæ¨¡å‹ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆå¯¹äº`facebook/opt-350m`æ¨¡å‹åœ¨Google colabä¸Šå¤§çº¦éœ€è¦5åˆ†é’Ÿï¼‰ã€‚

å› æ­¤ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æƒ…å†µéœ€è¦ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ã€‚ç¬¬ä¸€ç§æƒ…å†µæ˜¯åŠ è½½å·²ç»ç”±å…¶ä»–ç”¨æˆ·é‡åŒ–çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨Hubä¸Šæ‰¾åˆ°ï¼›ç¬¬äºŒç§æƒ…å†µæ˜¯é‡åŒ–è‡ªå·±çš„æ¨¡å‹å¹¶ä¿å­˜æˆ–å°†å…¶æ¨é€åˆ°Hubï¼Œä»¥ä¾¿å…¶ä»–ç”¨æˆ·ä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚

#### GPTQé…ç½®

ä¸ºäº†åŠ è½½å’Œé‡åŒ–æ¨¡å‹ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ª[`GPTQConfig`]ã€‚ä½ éœ€è¦ä¼ é€’`bits`çš„æ•°é‡ã€ä¸€ä¸ª`dataset`ç”¨äºæ ¡å‡†é‡åŒ–ä»¥åŠæ¨¡å‹çš„`tokenizer`ä»¥å‡†å¤‡æ•°æ®é›†ã€‚

```python
model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)
```

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä½œä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ã€‚ç„¶è€Œï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨GPTQè®ºæ–‡ä¸­çš„æ•°æ®é›†ã€‚
```python
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
quantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)
```

#### é‡åŒ–

ä½ å¯ä»¥ä½¿ç”¨`from_pretrained`æ¥é‡åŒ–æ¨¡å‹ï¼Œå¹¶è®¾ç½®`quantization_config`ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)
```
è¯·æ³¨æ„ï¼Œä½ å°†éœ€è¦ä¸€ä¸ªGPUæ¥é‡åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¨¡å‹æ”¾åˆ°CPUä¸Šï¼Œç„¶åå°†æ¨¡å—æ¥å›ç§»åŠ¨åˆ°GPUä¸Šä»¥è¿›è¡Œé‡åŒ–ã€‚

å¦‚æœä½ æƒ³åœ¨ä½¿ç”¨CPUéšè—æ—¶æœ€å¤§é™åº¦åœ°ä½¿ç”¨GPUï¼Œå¯ä»¥è®¾ç½®`device_map = "auto"`ã€‚
```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```
è¯·æ³¨æ„ï¼Œç›®å‰ä¸æ”¯æŒç£ç›˜æ˜ å°„ã€‚æ­¤å¤–ï¼Œå¦‚æœç”±äºæ•°æ®é›†è€Œå†…å­˜ä¸è¶³ï¼Œä½ å¯èƒ½éœ€è¦åœ¨`from_pretained`ä¸­ä¼ é€’`max_memory`ã€‚è¯·æŸ¥é˜…æ­¤[æŒ‡å—](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map)äº†è§£æœ‰å…³`device_map`å’Œ`max_memory`çš„æ›´å¤šä¿¡æ¯ã€‚

<Tip warning={true}>
ç›®å‰ï¼ŒGPTQé‡åŒ–ä»…é€‚ç”¨äºæ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ ¹æ®ç¡¬ä»¶çš„ä¸åŒï¼Œé‡åŒ–è¿‡ç¨‹å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼ˆ175Bæ¨¡å‹ = ä½¿ç”¨NVIDIA A100çš„4ä¸ªGPUå°æ—¶ï¼‰ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å¯ä»¥åœ¨GitHubä¸Šæäº¤éœ€æ±‚ã€‚
</Tip>

### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤—Hub

ä½ å¯ä»¥åƒå°†ä»»ä½•ğŸ¤—æ¨¡å‹æ¨é€åˆ°Hubä¸€æ ·æ¨é€é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨`push_to_hub`æ–¹æ³•ã€‚é‡åŒ–é…ç½®å°†è¢«ä¿å­˜å¹¶ä¸æ¨¡å‹ä¸€èµ·æ¨é€ã€‚

```python
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

å¦‚æœä½ æƒ³åœ¨æœ¬åœ°ä¿å­˜é‡åŒ–æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨`save_pretrained`ï¼š
```python
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")
```

è¯·æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨`device_map`å¯¹æ¨¡å‹è¿›è¡Œäº†é‡åŒ–ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°å…¶ä¸­ä¸€ä¸ªGPUæˆ–`cpu`ã€‚
```python
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

### ä»ğŸ¤—HubåŠ è½½é‡åŒ–æ¨¡å‹

ä½ å¯ä»¥ä½¿ç”¨`from_pretrained`ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚
é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨å±æ€§`quantization_config`ï¼Œç¡®ä¿æ¨é€çš„æƒé‡å·²ç»è¿›è¡Œäº†é‡åŒ–ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq")
```

å¦‚æœä½ æƒ³æ›´å¿«åœ°åŠ è½½æ¨¡å‹ï¼Œè€Œä¸”ä¸ä¼šåˆ†é…é¢å¤–çš„å†…å­˜ï¼Œè¯·åœ¨é‡åŒ–æ¨¡å‹ä¸Šä½¿ç”¨`device_map`å‚æ•°ã€‚è¯·ç¡®ä¿å·²å®‰è£…`accelerate`åº“ã€‚
```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### ç”¨äºæ›´å¿«æ¨æ–­çš„å¿«é€Ÿæ¨æ–­å†…æ ¸

å¯¹äº4æ¯”ç‰¹æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨å¿«é€Ÿæ¨æ–­å†…æ ¸ä»¥æé«˜æ¨æ–­é€Ÿåº¦ã€‚å®ƒé»˜è®¤ä¸ºæ¿€æ´»çŠ¶æ€ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨[`GPTQConfig`]ä¸­ä¼ é€’`disable_exllama`æ¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚è¿™å°†è¦†ç›–åœ¨é…ç½®ä¸­å­˜å‚¨çš„é‡åŒ–é…ç½®ã€‚è¯·æ³¨æ„ï¼Œåªèƒ½è¦†ç›–ä¸å†…æ ¸ç›¸å…³çš„å±æ€§ã€‚æ­¤å¤–ï¼Œå¦‚æœè¦ä½¿ç”¨exllamaå†…æ ¸ï¼Œéœ€è¦å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨gpuä¸Šã€‚

```py
import torch
gptq_config = GPTQConfig(bits=4, disable_exllama=False)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config = gptq_config)
```

è¯·æ³¨æ„ï¼Œç›®å‰ä»…æ”¯æŒ4æ¯”ç‰¹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¦‚æœè¦å¾®è°ƒå…·æœ‰PEFTçš„é‡åŒ–æ¨¡å‹ï¼Œå»ºè®®ç¦ç”¨exllamaå†…æ ¸ã€‚

#### å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒ

åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­æ­£å¼æ”¯æŒadapteråï¼Œä½ å¯ä»¥å¾®è°ƒå·²ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–çš„æ¨¡å‹ã€‚
æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`peft`[åº“](https://github.com/huggingface/peft)ã€‚

### ç¤ºä¾‹æ¼”ç¤º

æŸ¥çœ‹Google Colabçš„[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨peftå¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

### GPTQConfig

[[autodoc]] GPTQConfig


## `bitsandbytes`é›†æˆ

ğŸ¤—Transformersä¸`bitsandbytes`ä¸Šæœ€å¸¸ç”¨çš„æ¨¡å—ç´§å¯†é›†æˆã€‚åªéœ€å‡ è¡Œä»£ç ï¼Œä½ å°±å¯ä»¥ä»¥8ä½ç²¾åº¦åŠ è½½ä½ çš„æ¨¡å‹ã€‚
è‡ªâ€œ0.37.0â€ç‰ˆæœ¬ä»¥æ¥ï¼Œ`bitsandbytes`å·²ç»æ”¯æŒå¤§å¤šæ•°GPUç¡¬ä»¶ã€‚

äº†è§£æœ‰å…³é‡åŒ–æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[LLM.int8()](https://arxiv.org/abs/2208.07339)è®ºæ–‡ï¼Œæˆ–æœ‰å…³æ­¤åˆä½œçš„[åšæ–‡](https://huggingface.co/blog/hf-bitsandbytes-integration)ã€‚

ä»â€œ0.39.0â€ç‰ˆæœ¬å¼€å§‹ï¼Œä½ å¯ä»¥ä½¿ç”¨4ä½é‡åŒ–åŠ è½½æ”¯æŒ`device_map`çš„ä»»ä½•æ¨¡å‹ï¼Œä»è€Œæä¾›FP4æ•°æ®ç±»å‹ã€‚

å¦‚æœä½ æƒ³é‡åŒ–è‡ªå·±çš„pytorchæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ğŸ¤—åŠ é€Ÿåº“çš„[æ­¤æ–‡æ¡£](https://huggingface.co/docs/accelerate/main/zh/usage_guides/quantization)ã€‚ 

ä»¥ä¸‹æ˜¯ä½¿ç”¨`bitsandbytes`é›†æˆå¯ä»¥å®ç°çš„åŠŸèƒ½

### ä¸€èˆ¬ç”¨æ³•

ä½ å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨[`~PreTrainedModel.from_pretrained`]æ–¹æ³•æ—¶ä½¿ç”¨`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼Œå°†æ¨¡å‹é‡åŒ–ä¸º8ä½ç²¾åº¦ã€‚åªè¦ä½ çš„æ¨¡å‹æ”¯æŒä½¿ç”¨ğŸ¤—åŠ é€ŸåŠ è½½å¹¶åŒ…å«`torch.nn.Linear`å±‚å³å¯ã€‚è¿™å¯¹äºä»»ä½•æ¨¡æ€ä¹Ÿé€‚ç”¨ã€‚

```python
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True)
model_4bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_4bit=True)
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆå¦‚`torch.nn.LayerNorm`ï¼‰å°†è½¬æ¢ä¸º`torch.float16`ï¼Œä½†æ˜¯å¦‚æœè¦æ›´æ”¹å®ƒä»¬çš„`dtype`ï¼Œå¯ä»¥è¦†ç›–`torch_dtype`å‚æ•°ï¼š

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM

>>> model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True, torch_dtype=torch.float32)
>>> model_8bit.model.decoder.layers]lstm(12/987)(
                               lstm_feedfo1r
```


### ä½¿ç”¨FP4é‡åŒ–

#### è¦æ±‚

åœ¨è¿è¡Œä¸‹é¢çš„ä»£ç ç‰‡æ®µä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹è¦æ±‚ã€‚

- æœ€æ–°çš„`bitsandbytes`åº“
`pip install bitsandbytes>=0.39.0`

- å®‰è£…æœ€æ–°çš„`accelerate`
`pip install --upgrade accelerate`

- å®‰è£…æœ€æ–°çš„`transformers`
`pip install --upgrade transformers`

#### æç¤ºå’Œæœ€ä½³å®è·µ

- **é«˜çº§ç”¨æ³•ï¼š**è¯·å‚è€ƒ[æ­¤Google Colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)ï¼Œäº†è§£4ä½é‡åŒ–çš„é«˜çº§ç”¨æ³•å’Œæ‰€æœ‰å¯èƒ½çš„é€‰é¡¹ã€‚

- **ä½¿ç”¨`batch_size = 1`è¿›è¡Œæ›´å¿«çš„æ¨ç†ï¼š**è‡ª`bitsandbytes`çš„`0.40.0`ç‰ˆæœ¬ä»¥æ¥ï¼Œå¦‚æœ`batch_size = 1`ï¼Œä½ å¯ä»¥è·å¾—å¿«é€Ÿæ¨ç†çš„å¥½å¤„ã€‚æŸ¥çœ‹[è¿™äº›å‘å¸ƒè¯´æ˜](https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0)ï¼Œç¡®ä¿ä½ çš„ç‰ˆæœ¬å¤§äº`0.40.0`ï¼Œä»¥ä¾¿ä»æ—©æœŸå¼€å§‹æ— ç¼åœ°ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

- **è®­ç»ƒï¼š**æ ¹æ®[QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)ï¼Œå¯¹äºè®­ç»ƒ4ä½åŸºæœ¬æ¨¡å‹ï¼ˆä¾‹å¦‚ä½¿ç”¨LoRAé€‚é…å™¨ï¼‰ï¼Œåº”ä½¿ç”¨`bnb_4bit_quant_type='nf4'`ã€‚

- **æ¨ç†ï¼š**å¯¹äºæ¨ç†ï¼Œ`bnb_4bit_quant_type`å¯¹æ€§èƒ½æ²¡æœ‰å¤ªå¤§å½±å“ã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä¸æ¨¡å‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„`bnb_4bit_compute_dtype`å’Œ`torch_dtype`å‚æ•°ã€‚

#### åœ¨4ä½ä¸­åŠ è½½å¤§æ¨¡å‹

é€šè¿‡åœ¨è°ƒç”¨`.from_pretrained`æ–¹æ³•æ—¶ä½¿ç”¨`load_in_4bit=True`ï¼Œä½ å¯ä»¥å°†å†…å­˜ä½¿ç”¨å‡å°‘çº¦4å€ï¼ˆå¤§è‡´ï¼‰ã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True)
```

<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦æ¨¡å‹å·²ç»ä»¥4ä½åŠ è½½ï¼Œå½“å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubä¸Šã€‚è¯·æ³¨æ„ï¼Œå°šä¸æ”¯æŒå¯¹4ä½æƒé‡è¿›è¡Œè®­ç»ƒã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥ä½¿ç”¨4ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚

</Tip>

### åœ¨8ä½ä¸­åŠ è½½å¤§æ¨¡å‹

é€šè¿‡åœ¨è°ƒç”¨`.from_pretrained`æ–¹æ³•æ—¶ä½¿ç”¨`load_in_8bit=True`å‚æ•°ï¼Œä½ å¯ä»¥å°†å†…å­˜éœ€æ±‚å‡å°çº¦ä¸€åŠã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_8bit=True)
```

ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨[`PreTrainedModel`]ä¸€æ ·ä½¿ç”¨ä½ çš„æ¨¡å‹ã€‚

ä½ å¯ä»¥ä½¿ç”¨`get_memory_footprint`æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨æƒ…å†µã€‚

```python
print(model.get_memory_footprint())
```

é€šè¿‡æ­¤é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§æ¨¡å‹å¹¶é¡ºåˆ©è¿è¡Œã€‚

<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦æ¨¡å‹å·²ç»ä»¥8ä½åŠ è½½ï¼Œç›®å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubä¸Šï¼Œé™¤éä½¿ç”¨æœ€æ–°çš„`transformers`å’Œ`bitsandbytes`ã€‚è¯·æ³¨æ„ï¼Œå°šä¸æ”¯æŒå¯¹8ä½æƒé‡è¿›è¡Œè®­ç»ƒã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥ä½¿ç”¨8ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚
æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼Œ`device_map`æ˜¯å¯é€‰çš„ï¼Œä½†æ˜¯å°†`device_map = 'auto'`è®¾ç½®ä¸ºæ¨ç†æ˜¯æœ€å¥½çš„ï¼Œå› ä¸ºå®ƒå°†æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨èµ„æºã€‚

</Tip>

#### é«˜çº§ç”¨ä¾‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½ å¯ä»¥ä½¿ç”¨FP4é‡åŒ–æ‰§è¡Œçš„ä¸€äº›é«˜çº§ç”¨ä¾‹

##### æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹

è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ›´æ”¹è®¡ç®—è¿‡ç¨‹ä¸­è¦ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œéšè—çŠ¶æ€å¯ä»¥æ˜¯`float32`ï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨bf16è¿›è¡Œè®¡ç®—ä»¥åŠ å¿«é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è®¾ç½®ä¸º`float32`ã€‚

```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

##### ä½¿ç”¨NF4ï¼ˆNormal Float 4ï¼‰æ•°æ®ç±»å‹

ä½ è¿˜å¯ä»¥ä½¿ç”¨NF4æ•°æ®ç±»å‹ï¼Œè¯¥æ•°æ®ç±»å‹æ˜¯é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡è€Œè®¾è®¡çš„æ–°çš„4ä½æ•°æ®ç±»å‹ã€‚è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

##### ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´èŠ‚çœå†…å­˜çš„æ¨ç†

æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚è¿™æ ·å¯ä»¥èŠ‚çœæ›´å¤šçš„å†…å­˜ï¼Œè€Œä¸ä¼šæœ‰ä»»ä½•é¢å¤–çš„æ€§èƒ½â€”â€”æ ¹æ®æˆ‘ä»¬çš„å®è¯è§‚å¯Ÿï¼Œè¿™ä½¿å¾—åœ¨NVIDIA-T4 16GBä¸Šä½¿ç”¨`sequence_length = 1024`ã€`batch_size = 1`å’Œ`gradient accumulation_steps = 4`å¾®è°ƒllama-13bæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

```python
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)
```


### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤—Hub

ä½ å¯ä»¥é€šè¿‡ç®€å•åœ°ä½¿ç”¨`push_to_hub`æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°Hubä¸Šã€‚è¿™å°†é¦–å…ˆæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚
ç¡®ä¿ä½¿ç”¨ `bitsandbytes>0.37.2`ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬åœ¨`bitsandbytes==0.38.0.post1`ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼‰ï¼Œä»¥ä¾¿èƒ½å¤Ÿä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

<Tip warning={true}>

å¼ºçƒˆé¼“åŠ±å°†8ä½æ¨¡å‹æ¨é€åˆ°Hubä¸Šä»¥é€‚åº”å¤§å‹æ¨¡å‹ã€‚è¿™å°†ä½¿ç¤¾åŒºèƒ½å¤Ÿå—ç›Šäºå†…å­˜å ç”¨çš„å‡å°‘ï¼Œä¾‹å¦‚åœ¨Google Colabä¸ŠåŠ è½½å¤§å‹æ¨¡å‹ã€‚

</Tip>

### ä»ğŸ¤—HubåŠ è½½é‡åŒ–æ¨¡å‹

ä½ å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚ç¡®ä¿æ¨é€çš„æƒé‡å·²ç»è¢«é‡åŒ–ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±æ€§ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```
è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦æŒ‡å®š`load_in_8bit=True`å‚æ•°ï¼Œä½†æ˜¯ä½ éœ€è¦ç¡®ä¿å·²å®‰è£…äº†`bitsandbytes`å’Œ`accelerate`ã€‚
è¿˜è¦æ³¨æ„ï¼Œ`device_map`æ˜¯å¯é€‰çš„ï¼Œä½†æ˜¯è®¾ç½®`device_map = 'auto'`å¯¹äºæ¨æ–­æ¥è¯´æ˜¯é¦–é€‰çš„ï¼Œå› ä¸ºå®ƒå°†åœ¨å¯ç”¨èµ„æºä¸Šé«˜æ•ˆåœ°åˆ†æ´¾æ¨¡å‹ã€‚

### é«˜çº§ç”¨ä¾‹

æœ¬èŠ‚æ—¨åœ¨ä¸ºå¸Œæœ›åœ¨åŠ è½½å’Œè¿è¡Œ8ä½æ¨¡å‹ä¹‹å¤–æ¢ç´¢æ›´å¤šåŠŸèƒ½çš„é«˜çº§ç”¨æˆ·æä¾›ã€‚

#### åœ¨`cpu`å’Œ`gpu`ä¹‹é—´è¿›è¡Œå¸è½½

å…¶ä¸­ä¸€ä¸ªé«˜çº§ç”¨ä¾‹æ˜¯èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶åœ¨`CPU`å’Œ`GPU`ä¹‹é—´åˆ†æ´¾æƒé‡ã€‚è¯·æ³¨æ„ï¼Œåœ¨CPUä¸Šåˆ†æ´¾çš„æƒé‡**ä¸ä¼š**è½¬æ¢ä¸º8ä½ï¼Œè€Œæ˜¯ä¿æŒä¸º`float32`ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºå¸Œæœ›é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶åœ¨GPUå’ŒCPUä¹‹é—´åˆ†æ´¾æ¨¡å‹çš„ç”¨æˆ·ã€‚

é¦–å…ˆï¼Œä»`transformers`ä¸­åŠ è½½[`BitsAndBytesConfig`]å¹¶å°†å±æ€§`llm_int8_enable_fp32_cpu_offload`è®¾ç½®ä¸º`True`ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

å‡è®¾ä½ æƒ³è¦åŠ è½½`bigscience/bloom-1b7`æ¨¡å‹ï¼Œå¹¶ä¸”ä½ çš„GPU RAMåˆšå¥½è¶³ä»¥å®¹çº³æ•´ä¸ªæ¨¡å‹ï¼Œé™¤äº†`lm_head`ã€‚å› æ­¤ï¼ŒæŒ‰å¦‚ä¸‹æ‰€ç¤ºç¼–å†™è‡ªå®šä¹‰çš„`device_map`ï¼š
```python
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

ç„¶åæŒ‰å¦‚ä¸‹æ‰€ç¤ºåŠ è½½æ¨¡å‹ï¼š
```python
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

å°±æ˜¯è¿™æ ·ï¼äº«å—ä½ çš„æ¨¡å‹å§ï¼

#### è°ƒæ•´`llm_int8_threshold`

ä½ å¯ä»¥è°ƒæ•´`llm_int8_threshold`å‚æ•°æ¥æ›´æ”¹å¼‚å¸¸å€¼çš„é˜ˆå€¼ã€‚"å¼‚å¸¸å€¼"æ˜¯å¤§äºæŸä¸ªé˜ˆå€¼çš„éšè—çŠ¶æ€å€¼ã€‚ 
è¿™å¯¹åº”äº`LLM.int8()`è®ºæ–‡ä¸­æè¿°çš„ç”¨äºå¼‚å¸¸å€¼æ£€æµ‹çš„å¼‚å¸¸å€¼é˜ˆå€¼ã€‚ä»»ä½•è¶…è¿‡æ­¤é˜ˆå€¼çš„éšè—çŠ¶æ€å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¹¶ä¸”å¯¹è¿™äº›å€¼çš„æ“ä½œå°†ä»¥fp16è¿›è¡Œã€‚è¿™äº›å€¼é€šå¸¸æœä»æ­£æ€åˆ†å¸ƒï¼Œå³å¤§å¤šæ•°å€¼ä½äº[-3.5, 3.5]èŒƒå›´å†…ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹æ¥è¯´ï¼ŒæŸäº›å¼‚å¸¸ç³»ç»Ÿå¼‚å¸¸å€¼çš„åˆ†å¸ƒå¯èƒ½å®Œå…¨ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´[-60, -6]æˆ–[6, 60]ä¸­ã€‚å¯¹äºç»å¯¹å€¼åœ¨~5èŒƒå›´å†…çš„å€¼ï¼ŒInt8é‡åŒ–æ•ˆæœè‰¯å¥½ï¼Œä½†æ˜¯è¶…è¿‡è¯¥èŒƒå›´åï¼Œæ€§èƒ½æŸå¤±æ˜¾è‘—ã€‚ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯6ï¼Œä½†å¯¹äºä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼Œå¾®è°ƒï¼‰å¯èƒ½éœ€è¦è¾ƒä½çš„é˜ˆå€¼ã€‚
è¿™ä¸ªå‚æ•°å¯ä»¥å½±å“æ¨¡å‹çš„æ¨æ–­é€Ÿåº¦ã€‚æˆ‘ä»¬å»ºè®®æ ¹æ®ä½ çš„ä½¿ç”¨æƒ…å†µè°ƒæ•´æ­¤å‚æ•°ï¼Œæ‰¾åˆ°æœ€åˆé€‚çš„å‚æ•°ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢

æŸäº›æ¨¡å‹æœ‰å¤šä¸ªæ¨¡å—ï¼Œè¿™äº›æ¨¡å—ä¸éœ€è¦è½¬æ¢ä¸º8ä½ä»¥ç¡®ä¿ç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼ŒJukeboxæ¨¡å‹æœ‰å‡ ä¸ªåº”è¯¥è·³è¿‡çš„`lm_head`æ¨¡å—ã€‚å¯ä»¥ä½¿ç”¨`llm_int8_skip_modules`è¿›è¡Œè°ƒæ•´ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### åœ¨8ä½åŠ è½½çš„æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒ

æœ‰äº†Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­çš„å®˜æ–¹é€‚é…å™¨æ”¯æŒï¼Œä½ å¯ä»¥å¾®è°ƒå·²ä»¥8ä½åŠ è½½çš„æ¨¡å‹ã€‚
è¿™ä½¿ä½ å¯ä»¥åœ¨å•ä¸ªGoogle Colabä¸­å¾®è°ƒå¤§å‹æ¨¡å‹ï¼Œä¾‹å¦‚`flan-t5-large`æˆ–`facebook/opt-6.7b`ã€‚è¯·æŸ¥çœ‹[`peft`](https://github.com/huggingface/peft)åº“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½ç”¨äºè®­ç»ƒçš„æ¨¡å‹æ—¶ä¸éœ€è¦ä¼ é€’`device_map`ã€‚å®ƒä¼šè‡ªåŠ¨å°†ä½ çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚å¦‚æœéœ€è¦ï¼Œä½ è¿˜å¯ä»¥å°†è®¾å¤‡æ˜ å°„è®¾ç½®ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚`cuda:0`ï¼Œ`0`ï¼Œ`torch.device('cuda:0')`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œ`device_map=auto`åº”ä»…ç”¨äºæ¨æ–­ã€‚

### BitsAndBytesConfig

[[autodoc]] BitsAndBytesConfig

## ä½¿ç”¨ğŸ¤—`optimum` è¿›è¡Œé‡åŒ–

è¯·å‚é˜…[Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/index)ä»¥äº†è§£`optimum`æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºä½ çš„ç”¨ä¾‹ã€‚