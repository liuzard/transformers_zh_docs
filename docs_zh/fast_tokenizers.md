<!--ç‰ˆæƒæ‰€æœ‰2020å¹´HuggingFaceå›¢é˜Ÿä¿ç•™ã€‚

æ ¹æ®Apache Licenseï¼ŒVersion 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼Œé™¤éç¬¦åˆè®¸å¯è¯æ¡æ¬¾ï¼Œå¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„è½¯ä»¶æŒ‰â€œåŸæ ·â€åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯¦ç»†äº†è§£è®¸å¯è¯ä¸­çš„ç‰¹å®šè¯­è¨€ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨Markdownæ ¼å¼ï¼Œä½†åŒ…å«ç”¨äºæˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œåœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®æ¸²æŸ“ã€‚-->

# ä½¿ç”¨ğŸ¤— Tokenizersçš„åˆ†è¯å™¨

[`PreTrainedTokenizerFast`](https://huggingface.co/docs/tokenizer/installation)ä¾èµ–äº[ğŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers)åº“ã€‚ä»ğŸ¤— Tokenizersåº“è·å–çš„åˆ†è¯å™¨å¯ä»¥å¾ˆç®€å•åœ°åŠ è½½åˆ°ğŸ¤— Transformersä¸­ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡å‡ è¡Œä»£ç åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿåˆ†è¯å™¨ï¼š

```python
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒå¥½çš„åˆ†è¯å™¨ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å½“å‰è¿è¡Œæ—¶ç»§ç»­ä½¿ç”¨å®ƒï¼Œæˆ–å°†å…¶ä¿å­˜ä¸ºJSONæ–‡ä»¶ä»¥å¤‡å°†æ¥ä½¿ç”¨ã€‚

## ç›´æ¥ä»åˆ†è¯å™¨å¯¹è±¡åŠ è½½

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ğŸ¤— Transformersåº“ä¸­åˆ©ç”¨è¿™ä¸ªåˆ†è¯å™¨å¯¹è±¡ã€‚[`PreTrainedTokenizerFast`](https://huggingface.co/docs/tokenizers/quicktour#loading-the-tokenizer)ç±»å¯ä»¥é€šè¿‡æ¥å—åˆå§‹åŒ–çš„*tokenizer*å¯¹è±¡ä½œä¸ºå‚æ•°æ¥è½»æ¾å®ä¾‹åŒ–ï¼š

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä¸ğŸ¤— Transformersåˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ä¸€èµ·ä½¿ç”¨ï¼è¯·å‰å¾€[åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## ä»JSONæ–‡ä»¶åŠ è½½

ä¸ºäº†ä»JSONæ–‡ä»¶åŠ è½½åˆ†è¯å™¨ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä¿å­˜æˆ‘ä»¬çš„åˆ†è¯å™¨ï¼š

```python
>>> tokenizer.save("tokenizer.json")
```

æˆ‘ä»¬ä¿å­˜è¿™ä¸ªæ–‡ä»¶çš„è·¯å¾„å¯ä»¥é€šè¿‡`tokenizer_file`å‚æ•°ä¼ é€’ç»™[`PreTrainedTokenizerFast`](https://huggingface.co/docs/tokenizers/quicktour#loading-the-tokenizer)åˆå§‹åŒ–æ–¹æ³•ï¼š

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä¸ğŸ¤— Transformersåˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ä¸€èµ·ä½¿ç”¨ï¼è¯·å‰å¾€[åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)äº†è§£æ›´å¤šä¿¡æ¯ã€‚