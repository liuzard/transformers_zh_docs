<!--è‘—ä½œæƒ 2020 å¹´ HuggingFace å›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼ŒæŒ‰åŸæ ·å‘å¸ƒçš„è½¯ä»¶åˆ†å‘åœ¨â€œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•æ‹…ä¿æˆ–æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œåœ¨
åŸºç¡€â€åŸºç¡€ä¸Šã€‚è¯·å‚é˜…è®¸å¯è¯ä¸‹çš„ç‰¹å®šè¯­è¨€ä»¥åŠè®¸å¯é™åˆ¶ç­‰é”™è¯¯ã€‚

âš ï¸ è¯·æ³¨æ„è¿™ä¸ªæ–‡ä»¶æ˜¯åœ¨ Markdown ä¸­ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œè¿™å¯èƒ½ä¸ä¼šåœ¨ä½ çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚

-->

# å¯¼å‡ºåˆ° ONNX

å°† ğŸ¤— Transformers æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒé€šå¸¸éœ€è¦å°†æ¨¡å‹å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ä¸“ç”¨è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œã€‚

ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œå®ƒé€šè¿‡å…¶ `exporters` æ¨¡å—ä½¿æ¨¡å‹èƒ½å¤Ÿä» PyTorch æˆ– TensorFlow å¯¼å‡ºä¸º ONNX å’Œ TFLite ç­‰åºåˆ—åŒ–æ ¼å¼ã€‚ğŸ¤— Optimum è¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œä»¥å®ç°åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€é«˜æ•ˆç‡è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¿è¡Œã€‚

æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œå…³äºå°†æ¨¡å‹å¯¼å‡ºä¸º TFLite è¯·å‚é˜…[å¯¼å‡ºåˆ° TFLite é¡µé¢](tflite.md)ã€‚

## å¯¼å‡ºåˆ° ONNX

[ONNXï¼ˆOpen Neural Network eXchangeï¼‰](http://onnx.ai)æ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºå®šä¹‰ä¸€ç»„é€šç”¨ç®—å­å’Œé€šç”¨æ–‡ä»¶æ ¼å¼ï¼Œä»¥åœ¨åŒ…æ‹¬ PyTorch å’Œ TensorFlow åœ¨å†…çš„å„ç§æ¡†æ¶ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“ä¸€ä¸ªæ¨¡å‹è¢«å¯¼å‡ºä¸º ONNX æ ¼å¼æ—¶ï¼Œè¿™äº›ç®—å­ä¼šè¢«ç”¨äºæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸ºâ€œä¸­é—´è¡¨ç¤ºâ€ï¼‰ï¼Œä»£è¡¨æ•°æ®åœ¨ç¥ç»ç½‘ç»œä¸­çš„æµåŠ¨ã€‚

é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–ç®—å­å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNX ä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥è¢«å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç„¶ååœ¨ TensorFlow ä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å¯¼å‡ºä¸º ONNX æ ¼å¼åï¼Œå¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
- é€šè¿‡è¯¸å¦‚ [å›¾ä¼˜åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) å’Œ [é‡åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) ç­‰æŠ€æœ¯å¯¹æ¨ç†è¿›è¡Œä¼˜åŒ–ã€‚
- ä½¿ç”¨ ONNX Runtime é€šè¿‡ [`ORTModelForXXX` ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)è¿è¡Œï¼Œ
å®ƒä»¬ä¸ ğŸ¤— Transformers ä¸­ä½ ä¹ æƒ¯ä½¿ç”¨çš„ `AutoModel` API ç›¸åŒã€‚
- ä½¿ç”¨[ä¼˜åŒ–çš„æ¨ç†pipeline](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)ï¼Œå…¶ä¸ ğŸ¤— Transformers ä¸­çš„ [`pipeline`] å‡½æ•°å…·æœ‰ç›¸åŒçš„ APIã€‚

ğŸ¤— Optimum é€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›å¯¹ ONNX å¯¼å‡ºçš„æ”¯æŒã€‚è¿™äº›é…ç½®å¯¹è±¡é’ˆå¯¹è®¸å¤šæ¨¡å‹ä½“ç³»ç»“æ„éƒ½å·²å‡†å¤‡å¥½ï¼Œå¹¶è®¾è®¡æ˜“äºæ‰©å±•åˆ°å…¶ä»–ä½“ç³»ç»“æ„ã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå±•ç¤ºä¸¤ç§æ–¹æ³•ï¼š

- ä½¿ç”¨ ğŸ¤— Optimum çš„ CLI å¯¼å‡ºã€‚
- ä½¿ç”¨ `optimum.onnxruntime` å¯¼å‡ºã€‚

### ä½¿ç”¨ CLI å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNX

è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼Œé¦–å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```bash
pip install optimum[exporters]
```

è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„å‚æ•°ï¼Œè¯·å‚é˜…[ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ï¼Œ
æˆ–åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š

```bash
optimum-cli export onnx --help
```

è¦ä» ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚ `distilbert-base-uncased-distilled-squad`ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
optimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

ä½ åº”è¯¥ä¼šçœ‹åˆ°æ—¥å¿—æ˜¾ç¤ºè¿›åº¦å¹¶æ˜¾ç¤ºä¿å­˜äº†ç»“æœ `model.onnx` çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼š

```bash
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šé¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•å¯¼å‡ºæ¥è‡ª ğŸ¤— Hub çš„æ£€æŸ¥ç‚¹ã€‚å½“å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œé¦–å…ˆç¡®ä¿å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ï¼ˆ`local_path`ï¼‰ã€‚å½“ä½¿ç”¨ CLI æ—¶ï¼Œå°† `local_path` ä¼ é€’ç»™ `model` å‚æ•°ï¼Œè€Œä¸æ˜¯æ£€æŸ¥ç‚¹åç§°åœ¨ ğŸ¤— Hub ä¸­ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚ä½ å¯ä»¥åœ¨[ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager)ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚å¦‚æœæœªæä¾› `task` å‚æ•°ï¼Œå®ƒå°†é»˜è®¤ä¸ºä¸å…·æœ‰ä»»ä½•ä»»åŠ¡ç‰¹å®šå¤´çš„æ¨¡å‹ä½“ç³»ç»“æ„ã€‚

```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ç„¶åå¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„[è®¸å¤šåŠ é€Ÿå™¨](https://onnx.ai/supported-tools.html#deployModel)ä¹‹ä¸€ä¸Šè¿è¡Œç»“æœ `model.onnx`ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX Runtime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

åœ¨ ğŸ¤— Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ç›¸åŒã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•å¯¼å‡ºæ¥è‡ª [Keras ç»„ç»‡](https://huggingface.co/keras-io)çš„çº¯ TensorFlow æ£€æŸ¥ç‚¹ï¼š

```bash
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### ä½¿ç”¨ `optimum.onnxruntime` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNX

ä¸ CLI ç›¸æ¯”ï¼Œä½ ä¹Ÿå¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼ä»¥ç¼–ç¨‹æ–¹å¼å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼š

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # ä» transformers åŠ è½½æ¨¡å‹å¹¶å¯¼å‡ºä¸º ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # ä¿å­˜ ONNX æ¨¡å‹å’Œåˆ†è¯å™¨
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### å¯¼å‡ºä¸æ”¯æŒçš„ä½“ç³»ç»“æ„çš„æ¨¡å‹

å¦‚æœä½ å¸Œæœ›é€šè¿‡ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒæ¥è¿›è¡Œè´¡çŒ®ï¼Œä½ åº”é¦–å…ˆæ£€æŸ¥ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) æ˜¯å¦æ”¯æŒè¯¥æ¨¡å‹ï¼Œå¦‚æœä¸æ”¯æŒï¼Œä½ å¯ä»¥ç›´æ¥[å¯¹ ğŸ¤— Optimum è¿›è¡Œè´¡çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚

### ä½¿ç”¨ `transformers.onnx` å¯¼å‡ºæ¨¡å‹

<Tip warning={true}>

`tranformers.onnx` ä¸å†ç»´æŠ¤ï¼Œè¯·å‚è€ƒä¸Šè¿°ä½¿ç”¨ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹çš„æ–¹æ³•ã€‚è¯¥éƒ¨åˆ†å°†åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­è¢«åˆ é™¤ã€‚

</Tip>

è¦ä½¿ç”¨ `tranformers.onnx` å¯¼å‡º ğŸ¤— Transformers æ¨¡å‹åˆ° ONNXï¼Œè¯·å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```bash
pip install transformers[onnx]
```

ä½¿ç”¨ `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ï¼Œé€šè¿‡ä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š

```bash
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

è¿™ä¼šå¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ é€’ä»»ä½•åœ¨ ğŸ¤— Hub ä¸Šæˆ–æœ¬åœ°å­˜å‚¨çš„æ£€æŸ¥ç‚¹ã€‚
ç„¶åï¼Œå¯ä»¥åœ¨è®¸å¤šæ”¯æŒ ONNX æ ‡å‡†çš„åŠ é€Ÿå™¨ä¸ŠåŠ è½½å’Œè¿è¡Œç”Ÿæˆçš„ `model.onnx`ï¼Œä¾‹å¦‚ä½¿ç”¨ ONNX Runtime è¿è¡Œæ¨¡å‹çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime æœŸæœ›è¾“å…¥ä¸º NumPy æ•°ç»„
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

æ‰€éœ€çš„è¾“å‡ºåç§°ï¼ˆå¦‚ `["last_hidden_state"]`ï¼‰å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®å¾—åˆ°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œæˆ‘ä»¬æœ‰ï¼š

```python
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

åœ¨ ğŸ¤— Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹ä¸Šï¼Œè¯¥è¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œå¯¼å‡ºçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„æ–¹æ³•å¦‚ä¸‹ï¼š

```bash
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

è¦å¯¼å‡ºæœ¬åœ°å­˜å‚¨çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨ç›¸åŒç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx` åŒ…çš„ `--model` å‚æ•°æŒ‡å‘æ‰€éœ€ç›®å½•æ¥å°†å…¶å¯¼å‡ºä¸º ONNXï¼š

```bash
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```