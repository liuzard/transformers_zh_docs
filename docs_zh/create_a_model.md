<!--ç‰ˆæƒ2022ç¥å¤§å®¶è¶Šæ¥è¶Šå¥½ï¼Œä¿æŒå¼€å¿ƒï¼Œå…è´£å£°æ˜ï¼Œä¸è¦ç”¨è¿™ä¸ªä»£ç å“¦-->

# åˆ›å»ºè‡ªå®šä¹‰æ¶æ„

[`AutoClass`](model_doc/auto)ä¼šè‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒé…ç½®å’Œæƒé‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`AutoClass`æ¥äº§ç”Ÿä¸æ£€æŸ¥ç‚¹æ— å…³çš„ä»£ç ã€‚ä½†å¯¹äºå¸Œæœ›å¯¹ç‰¹å®šæ¨¡å‹å‚æ•°æœ‰æ›´å¤šæ§åˆ¶çš„ç”¨æˆ·æ¥è¯´ï¼Œå¯ä»¥ä»å‡ ä¸ªåŸºç±»åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ğŸ¤— Transformersæ¨¡å‹ã€‚è¿™å¯¹äºå¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡Œç ”ç©¶ã€è®­ç»ƒæˆ–å®éªŒçš„ä»»ä½•äººæ¥è¯´éƒ½éå¸¸æœ‰ç”¨ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ·±å…¥äº†è§£å¦‚ä½•åˆ›å»ºæ²¡æœ‰`AutoClass`çš„è‡ªå®šä¹‰æ¨¡å‹ã€‚å­¦ä¹ å¦‚ä½•ï¼š

- åŠ è½½å’Œè‡ªå®šä¹‰æ¨¡å‹é…ç½®ã€‚
- åˆ›å»ºæ¨¡å‹æ¶æ„ã€‚
- ä¸ºæ–‡æœ¬åˆ›å»ºæ…¢é€Ÿå’Œå¿«é€Ÿåˆ†è¯å™¨ã€‚
- ä¸ºè§†è§‰ä»»åŠ¡åˆ›å»ºå›¾åƒå¤„ç†å™¨ã€‚
- ä¸ºéŸ³é¢‘ä»»åŠ¡åˆ›å»ºç‰¹å¾æå–å™¨ã€‚
- ä¸ºå¤šæ¨¡å¼ä»»åŠ¡åˆ›å»ºå¤„ç†å™¨ã€‚

## é…ç½®

[é…ç½®](main_classes/configuration)æ˜¯æŒ‡æ¨¡å‹çš„ç‰¹å®šå±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§ï¼›ä¾‹å¦‚ï¼Œæ‰€æœ‰NLPæ¨¡å‹éƒ½å…·æœ‰`hidden_size`ã€`num_attention_heads`ã€`num_hidden_layers`å’Œ`vocab_size`å±æ€§ã€‚è¿™äº›å±æ€§æŒ‡å®šäº†æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ³¨æ„åŠ›å¤´æˆ–éšè—å±‚çš„æ•°é‡ã€‚

é€šè¿‡è®¿é—® [`DistilBertConfig`] æ¥æŸ¥çœ‹ [DistilBERT](model_doc/distilbert) çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ä»¥æ£€æŸ¥å®ƒçš„å±æ€§ï¼š

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`] æ˜¾ç¤ºäº†ç”¨äºæ„å»ºåŸºç¡€ [`DistilBertModel`] çš„æ‰€æœ‰é»˜è®¤å±æ€§ã€‚æ‰€æœ‰å±æ€§éƒ½æ˜¯å¯è‡ªå®šä¹‰çš„ï¼Œä¸ºå®éªŒæä¾›äº†ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ `activation` å‚æ•°å°è¯•å…¶ä»–ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…ä½¿ç”¨ `attention_dropout` å‚æ•°æ¥è®¾ç½®æ›´é«˜çš„æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ä¾‹ã€‚

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

é¢„è®­ç»ƒæ¨¡å‹å±æ€§å¯ä»¥åœ¨ [`~PretrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

ä¸€æ—¦ä½ å¯¹æ¨¡å‹é…ç½®æ„Ÿåˆ°æ»¡æ„ï¼Œå°±å¯ä»¥ä½¿ç”¨ [`~PretrainedConfig.save_pretrained`] æ¥ä¿å­˜é…ç½®ã€‚ä½ çš„é…ç½®æ–‡ä»¶å°†ä½œä¸ºä¸€ä¸ª JSON æ–‡ä»¶å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š

```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PretrainedConfig.from_pretrained`] åŠ è½½å®ƒï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

<Tip>

ä½ è¿˜å¯ä»¥å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸ºå­—å…¸ï¼Œç”šè‡³åªä¿å­˜è‡ªå®šä¹‰é…ç½®å±æ€§ä¸é»˜è®¤é…ç½®å±æ€§ä¹‹é—´çš„å·®å¼‚ï¼æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[é…ç½®](main_classes/configuration)æ–‡æ¡£ã€‚

</Tip>

## æ¨¡å‹

ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª[æ¨¡å‹](main_classes/models)ã€‚æ¨¡å‹ï¼ˆä¹Ÿå¯ä»¥å®½æ³›åœ°ç§°ä¸ºæ¶æ„ï¼‰å®šä¹‰äº†æ¯ä¸ªå±‚çš„å·¥ä½œå’Œæ“ä½œã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨é…ç½®ä¸­çš„`num_hidden_layers`å®šä¹‰äº†æ¶æ„ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…±äº«åŸºç±» [`PreTrainedModel`] å’Œä¸€äº›å¸¸è§çš„æ–¹æ³•ï¼Œå¦‚è°ƒæ•´è¾“å…¥åµŒå…¥çš„å¤§å°å’Œä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)ã€[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) æˆ– [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#module) çš„å­ç±»ã€‚è¿™æ„å‘³ç€å„è‡ªæ¡†æ¶çš„æ¨¡å‹å¯ä»¥ä¸å…¶å„è‡ªæ¡†æ¶çš„ç”¨æ³•å…¼å®¹ã€‚

<frameworkcontent>
<pt>
å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨ä½ è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä½ å°†æ— æ³•å¯¹å…¶è¿›è¡Œä»»ä½•æœ‰ç”¨çš„æ“ä½œã€‚è®­ç»ƒæ˜¯ä¸€ä¸ªæ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå»ºè®®ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¾¿æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚

ä½¿ç”¨ [`~PreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹ç”± ğŸ¤— Transformers æä¾›ï¼Œåˆ™ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥æ›¿æ¢-æŸäº›æˆ–å…¨éƒ¨-é»˜è®¤æ¨¡å‹é…ç½®å±æ€§ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</pt>
<tf>
å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨ä½ è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä½ å°†æ— æ³•å¯¹å…¶è¿›è¡Œä»»ä½•æœ‰ç”¨çš„æ“ä½œã€‚è®­ç»ƒæ˜¯ä¸€ä¸ªæ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå»ºè®®ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¾¿æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚

ä½¿ç”¨ [`~TFPreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹ç”± ğŸ¤— Transformers æä¾›ï¼Œåˆ™ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥æ›¿æ¢-æŸäº›æˆ–å…¨éƒ¨-é»˜è®¤æ¨¡å‹é…ç½®å±æ€§ï¼š

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</tf>
</frameworkcontent>

### æ¨¡å‹å¤´

æ­¤æ—¶ï¼Œä½ å·²ç»æœ‰äº†ä¸€ä¸ªåŸºæœ¬çš„ DistilBERT æ¨¡å‹ï¼Œå®ƒè¾“å‡º *éšè—çŠ¶æ€*ã€‚éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¤´ä»¥äº§ç”Ÿæœ€ç»ˆçš„è¾“å‡ºã€‚åªè¦æ¨¡å‹æ”¯æŒä»»åŠ¡ï¼ŒğŸ¤— Transformers ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›äº†ä¸€ä¸ªä¸åŒçš„æ¨¡å‹å¤´ï¼ˆä¾‹å¦‚ï¼Œä½ ä¸èƒ½ä¸º DistilBERT è¿™æ ·çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ï¼‰ä½¿ç”¨å®ƒï¼‰ã€‚

<frameworkcontent>
<pt>
ä¾‹å¦‚ï¼Œ[`DistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„åŸºæœ¬ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯ä½äºæ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹ç”¨äºå¦ä¸€ä¸ªä»»åŠ¡ã€‚å¯¹äºé—®é¢˜å›ç­”ä»»åŠ¡ï¼Œä½ å°†ä½¿ç”¨ [`DistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®é¢˜å›ç­”å¤´ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œåªæ˜¯å®ƒæ˜¯ä½äºéšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</pt>
<tf>
ä¾‹å¦‚ï¼Œ[`TFDistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„åŸºæœ¬ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯ä½äºæ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹ç”¨äºå¦ä¸€ä¸ªä»»åŠ¡ã€‚å¯¹äºé—®é¢˜å›ç­”ä»»åŠ¡ï¼Œä½ å°†ä½¿ç”¨ [`TFDistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®é¢˜å›ç­”å¤´ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œåªæ˜¯å®ƒæ˜¯ä½äºéšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</tf>
</frameworkcontent>

## åˆ†è¯å™¨

åœ¨ä½¿ç”¨æ¨¡å‹å¤„ç†æ–‡æœ¬æ•°æ®ä¹‹å‰ï¼Œä½ éœ€è¦ä½¿ç”¨ä¸€ä¸ª[åˆ†è¯å™¨](main_classes/tokenizer)å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡ã€‚ğŸ¤— Transformers æä¾›äº†ä¸¤ç§ç±»å‹çš„åˆ†è¯å™¨ï¼š

- [`PreTrainedTokenizer`]ï¼šåˆ†è¯å™¨çš„ Python å®ç°ã€‚
- [`PreTrainedTokenizerFast`]ï¼šæ¥è‡ªæˆ‘ä»¬çš„åŸºäº Rust çš„ [ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) åº“çš„åˆ†è¯å™¨ã€‚ç”±äºå…¶ Rust å®ç°ï¼Œè¿™ç§åˆ†è¯å™¨ç±»å‹åœ¨æ‰¹é‡åˆ†è¯æ—¶é€Ÿåº¦æ˜æ˜¾æ›´å¿«ã€‚å¿«é€Ÿåˆ†è¯å™¨è¿˜æä¾›äº†é¢å¤–çš„æ–¹æ³•ï¼Œå¦‚ *offset mapping*ï¼Œç”¨äºå°†æ ‡è®°æ˜ å°„åˆ°å®ƒä»¬çš„åŸå§‹å•è¯æˆ–å­—ç¬¦ã€‚

è¿™ä¸¤ç§åˆ†è¯å™¨éƒ½æ”¯æŒå¸¸è§çš„æ–¹æ³•ï¼Œå¦‚ç¼–ç å’Œè§£ç ã€æ·»åŠ æ–°çš„æ ‡è®°ã€ç®¡ç†ç‰¹æ®Šæ ‡è®°ã€‚

<Tip warning={true}>

å¹¶éæ¯ä¸ªæ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚è¯·æŸ¥çœ‹æ­¤[è¡¨æ ¼](index_zh#supported-frameworks)ä»¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚

</Tip>

å¦‚æœä½ è®­ç»ƒäº†è‡ªå·±çš„åˆ†è¯å™¨ï¼Œå¯ä»¥æ ¹æ®ä½ çš„ *è¯æ±‡* æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œè‡ªå®šä¹‰åˆ†è¯å™¨çš„è¯æ±‡å°†ä¸é¢„è®­ç»ƒæ¨¡å‹åˆ†è¯å™¨ç”Ÿæˆçš„è¯æ±‡ä¸åŒã€‚å¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯æ±‡ï¼Œå¦åˆ™è¾“å…¥å°†æ¯«æ— æ„ä¹‰ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯æ±‡åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼Œä½¿ç”¨ [`DistilBertTokenizer`] ç±»ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
```

ä½¿ç”¨ [`DistilBertTokenizerFast`] ç±»åˆ›å»ºå¿«é€Ÿåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
```

<Tip>

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`AutoTokenizer`] å°†å°è¯•åŠ è½½å¿«é€Ÿåˆ†è¯å™¨ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ `from_pretrained` ä¸­è®¾ç½® `use_fast=False` æ¥ç¦ç”¨æ­¤è¡Œä¸ºã€‚

</Tip>

## å›¾åƒå¤„ç†å™¨

å›¾åƒå¤„ç†å™¨å¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒæ˜¯ä»åŸºç±» [`~image_processing_utils.ImageProcessingMixin`] ç»§æ‰¿çš„ã€‚

è¦ä½¿ç”¨ï¼Œåˆ›å»ºä¸ä½ æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹ç›¸å…³è”çš„å›¾åƒå¤„ç†å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ä½¿ç”¨ [ViT](model_doc/vit) è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œåˆ™å¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`ViTImageProcessor`]ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

å¦‚æœä½ ä¸æ‰“ç®—è¿›è¡Œä»»ä½•è‡ªå®šä¹‰æ“ä½œï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½é»˜è®¤å›¾åƒå¤„ç†å™¨å‚æ•°å³å¯ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`ViTImageProcessor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰å›¾åƒå¤„ç†å™¨ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

## ç‰¹å¾æå–å™¨

ç‰¹å¾æå–å™¨å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚å®ƒæ˜¯ä»åŸºç±» [`~feature_extraction_utils.FeatureExtractionMixin`] ç»§æ‰¿çš„ï¼Œå¹¶ä¸”è¿˜å¯ä»¥ä» [`SequenceFeatureExtractor`] ç±»ç»§æ‰¿ä»¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚

è¦ä½¿ç”¨ï¼Œåˆ›å»ºä¸ä½ æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹ç›¸å…³è”çš„ç‰¹å¾æå–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ä½¿ç”¨ [Wav2Vec2](model_doc/wav2vec2) è¿›è¡ŒéŸ³é¢‘åˆ†ç±»ï¼Œåˆ™å¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`Wav2Vec2FeatureExtractor`]ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

<Tip>

å¦‚æœä½ ä¸æ‰“ç®—è¿›è¡Œä»»ä½•è‡ªå®šä¹‰æ“ä½œï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½é»˜è®¤ç‰¹å¾æå–å™¨å‚æ•°å³å¯ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`Wav2Vec2FeatureExtractor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰ç‰¹å¾æå–å™¨ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 8000
}
```


## å¤„ç†å™¨

å¯¹äºæ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œæ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œæ ‡è®°å™¨ç­‰å¤„ç†ç±»å°è£…æˆä¸€ä¸ªå•ä¸€å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼ˆASRï¼‰ä½¿ç”¨ [`Wav2Vec2Processor`]ã€‚ASR å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå› æ­¤ä½ éœ€è¦ä¸€ä¸ªç‰¹å¾æå–å™¨å’Œä¸€ä¸ªæ ‡è®°å™¨ã€‚

åˆ›å»ºä¸€ä¸ªç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘è¾“å…¥:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

åˆ›å»ºä¸€ä¸ªæ ‡è®°å™¨æ¥å¤„ç†æ–‡æœ¬è¾“å…¥:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

å°†ç‰¹å¾æå–å™¨å’Œæ ‡è®°å™¨ç»„åˆåœ¨ [`Wav2Vec2Processor`] ä¸­:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

é€šè¿‡é…ç½®å’Œæ¨¡å‹è¿™ä¸¤ä¸ªåŸºæœ¬ç±»ï¼Œä»¥åŠä¸€ä¸ªé¢å¤–çš„é¢„å¤„ç†ç±»ï¼ˆæ ‡è®°å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨æˆ–å¤„ç†å™¨ï¼‰ï¼Œä½ å¯ä»¥åˆ›å»ºğŸ¤— Transformers æ”¯æŒçš„ä»»ä½•æ¨¡å‹ã€‚æ¯ä¸ªåŸºç±»éƒ½æ˜¯å¯é…ç½®çš„ï¼Œ allowing you to use the specific attributes you wantã€‚ ä½ å¯ä»¥è½»æ¾è®¾ç½®ä¸€ä¸ªç”¨äºè®­ç»ƒçš„æ¨¡å‹æˆ–ä¿®æ”¹ä¸€ä¸ªç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚