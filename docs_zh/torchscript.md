<!-- ç‰ˆæƒ 2022 HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache Licenseï¼Œç¬¬2ç‰ˆ ï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œåœ¨æ—¨åœ¨æé«˜æ•ˆç‡çš„ C + + ç¨‹åºç­‰å…¶ä»–ç¨‹åºä¸­ï¼Œä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚

ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œåŸæ ·â€åˆ†å‘çš„ï¼Œå¹¶ä¸åšä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚

è¯·æ³¨æ„ï¼Œè¯¥æ–‡ä»¶ä¸º Markdownï¼Œä½†åŒ…å«ä¸æˆ‘ä»¬çš„å®šä¹‰æ„å»ºå™¨ç›¸å…³çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œå¯èƒ½æ— æ³•åœ¨ä½ çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚

-->

# å¯¼å‡ºè‡³ TorchScript

<Tip>

è¿™åªæ˜¯æˆ‘ä»¬åœ¨ä½¿ç”¨ TorchScript è¿›è¡Œå®éªŒçš„åˆæ­¥é˜¶æ®µï¼Œæˆ‘ä»¬ä»ç„¶åœ¨æ¢ç´¢å…¶åœ¨å¯å˜è¾“å…¥å¤§å°æ¨¡å‹ä¸­çš„èƒ½åŠ›ã€‚è¿™å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸€ä¸ªæ„Ÿå…´è¶£çš„ç„¦ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­æ·±å…¥åˆ†æï¼Œæä¾›æ›´å¤šä»£ç ç¤ºä¾‹ã€æ›´çµæ´»çš„å®ç°ä»¥åŠä½¿ç”¨ç¼–è¯‘çš„ TorchScript ä¸åŸºäº Python çš„ä»£ç è¿›è¡Œæ¯”è¾ƒçš„æ€§èƒ½åŸºå‡†ã€‚

</Tip>

æ ¹æ®[TorchScriptæ–‡æ¡£](https://pytorch.org/docs/stable/jit.html)ï¼š

> TorchScript æ˜¯ä¸€ç§ä» PyTorch ä»£ç åˆ›å»ºå¯åºåˆ—åŒ–å’Œå¯ä¼˜åŒ–æ¨¡å‹çš„æ–¹å¼ã€‚

PyTorch æä¾›äº†ä¸¤ä¸ªæ¨¡å—ï¼Œ[JIT and TRACE](https://pytorch.org/docs/stable/jit.html)ï¼Œå…è®¸å¼€å‘è€…å°†ä»–ä»¬çš„æ¨¡å‹å¯¼å‡ºä»¥åœ¨å…¶ä»–ç¨‹åºä¸­é‡ç”¨ï¼Œä¾‹å¦‚é¢å‘æ•ˆç‡çš„ C++ ç¨‹åºã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå¯è®©ä½ å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° TorchScriptï¼Œä»¥ä¾¿åœ¨ä¸åŸºäº PyTorch çš„ Python ç¨‹åºä¸åŒçš„ç¯å¢ƒä¸­é‡ç”¨å®ƒä»¬ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§£é‡Šå¦‚ä½•ä½¿ç”¨ TorchScript å¯¼å‡ºå’Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

å¯¼å‡ºæ¨¡å‹éœ€è¦ä¸¤ä¸ªæ¡ä»¶ï¼š

- ä½¿ç”¨ `torchscript` æ ‡å¿—å®ä¾‹åŒ–æ¨¡å‹
- ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿›è¡Œå‰å‘ä¼ é€’

è¿™äº›æ¡ä»¶æ„å‘³ç€å¼€å‘è€…éœ€è¦æ³¨æ„ä¸€äº›ç»†èŠ‚ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚

## TorchScript æ ‡å¿—å’Œç»‘å®šçš„æƒé‡

`torchscript` æ ‡å¿—æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå¤§å¤šæ•° ğŸ¤— Transformers è¯­è¨€æ¨¡å‹çš„ `Embedding` å±‚å’Œ `Decoding` å±‚ä¹‹é—´å­˜åœ¨ç»‘å®šçš„æƒé‡ã€‚TorchScript ä¸å…è®¸å¯¼å‡ºå…·æœ‰ç»‘å®šæƒé‡çš„æ¨¡å‹ï¼Œå› æ­¤éœ€è¦åœ¨å¯¼å‡ºä¹‹å‰è§£å¼€å¹¶å…‹éš†è¿™äº›æƒé‡ã€‚

ä½¿ç”¨ `torchscript` æ ‡å¿—å®ä¾‹åŒ–çš„æ¨¡å‹å°†å®ƒä»¬çš„ `Embedding` å±‚å’Œ `Decoding` å±‚åˆ†å¼€ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸åº”è¯¥è¿›è¡Œåç»­è®­ç»ƒã€‚è®­ç»ƒä¼šå¯¼è‡´ä¸¤ä¸ªå±‚ä¹‹é—´çš„ä¸åŒæ­¥ï¼Œå¯¼è‡´æ„å¤–çš„ç»“æœã€‚

å¯¹äºæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´çš„æ¨¡å‹æ¥è¯´ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ²¡æœ‰ç»‘å®šçš„æƒé‡ã€‚è¿™äº›æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰ `torchscript` æ ‡å¿—çš„æƒ…å†µä¸‹å®‰å…¨åœ°å¯¼å‡ºã€‚

## è™šæ‹Ÿè¾“å…¥å’Œæ ‡å‡†é•¿åº¦

è™šæ‹Ÿè¾“å…¥ç”¨äºæ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚å½“è¾“å…¥çš„å€¼åœ¨å±‚ä¹‹é—´ä¼ æ’­æ—¶ï¼ŒPyTorch ä¼šè·Ÿè¸ªåœ¨æ¯ä¸ªå¼ é‡ä¸Šæ‰§è¡Œçš„ä¸åŒæ“ä½œã€‚ç„¶åï¼Œè¿™äº›è®°å½•çš„æ“ä½œç”¨äºåˆ›å»ºæ¨¡å‹çš„ *trace*ã€‚

trace æ˜¯ç›¸å¯¹äºè¾“å…¥ç»´åº¦åˆ›å»ºçš„ã€‚å› æ­¤ï¼Œå®ƒå—åˆ°è™šæ‹Ÿè¾“å…¥ç»´åº¦çš„é™åˆ¶ï¼Œå¯¹äºå…¶ä»–åºåˆ—é•¿åº¦æˆ–æ‰¹æ¬¡å¤§å°å°†æ— æ³•å·¥ä½œã€‚åœ¨å°è¯•ä¸åŒå¤§å°æ—¶ï¼Œä¼šå¼•å‘ä»¥ä¸‹é”™è¯¯ï¼š

```
`åœ¨ä¸€ä¸ªéå•ä¾‹ç»´åº¦ 2 ä¸­ï¼Œå¼ é‡çš„æ‰©å±•å°ºå¯¸ï¼ˆ3ï¼‰å¿…é¡»åŒ¹é…ç°æœ‰å°ºå¯¸ï¼ˆ7ï¼‰`
```

æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨è™šæ‹Ÿè¾“å…¥å¤§å°è‡³å°‘ä¸åœ¨æ¨æ–­è¿‡ç¨‹ä¸­å°†æä¾›ç»™æ¨¡å‹çš„æœ€å¤§è¾“å…¥ä¸€æ ·å¤§è¿›è¡Œæ¨¡å‹çš„è¿½è¸ªã€‚å¯ä»¥ä½¿ç”¨å¡«å……æ¥å¡«å……ç¼ºå¤±çš„å€¼ã€‚ä½†æ˜¯ï¼Œç”±äºæ¨¡å‹ä¸è¾ƒå¤§çš„è¾“å…¥å¤§å°è¿›è¡Œè¿½è¸ªï¼ŒçŸ©é˜µçš„ç»´åº¦ä¹Ÿä¼šå¾ˆå¤§ï¼Œå¯¼è‡´è®¡ç®—é‡æ›´å¤§ã€‚

è¯·æ³¨æ„æ¯ä¸ªè¾“å…¥ä¸Šæ‰§è¡Œçš„æ“ä½œçš„æ€»æ•°ï¼Œå¹¶åœ¨å¯¼å‡ºä¸åŒåºåˆ—é•¿åº¦çš„æ¨¡å‹æ—¶ä»”ç»†ç›‘æ§æ€§èƒ½ã€‚

## åœ¨ Python ä¸­ä½¿ç”¨ TorchScript

æœ¬èŠ‚æ¼”ç¤ºäº†å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ trace è¿›è¡Œæ¨æ–­ã€‚

### ä¿å­˜æ¨¡å‹

è¦ä½¿ç”¨ TorchScript å¯¼å‡º `BertModel`ï¼Œè¯·ä» `BertConfig` ç±»å®ä¾‹åŒ– `BertModel`ï¼Œç„¶åå°†å…¶ä¿å­˜åˆ°æ–‡ä»¶åä¸º `traced_bert.pt` çš„ç£ç›˜ä¸Šï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### åŠ è½½æ¨¡å‹

ç°åœ¨ï¼Œä½ å¯ä»¥ä»ç£ç›˜ä¸ŠåŠ è½½ä¹‹å‰ä¿å­˜çš„ `BertModel`ï¼Œå³ `traced_bert.pt`ï¼Œå¹¶å°†å…¶ç”¨äºä¹‹å‰åˆå§‹åŒ–çš„ `dummy_input` ä¸Šï¼š

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### ä½¿ç”¨è¿½è¸ªæ¨¡å‹è¿›è¡Œæ¨æ–­

é€šè¿‡ä½¿ç”¨è¿½è¸ªæ¨¡å‹çš„ `__call__` æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨è¿½è¸ªæ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š

```python
traced_model(tokens_tensor, segments_tensors)
```

## ä½¿ç”¨ Neuron SDK å°† Hugging Face TorchScript æ¨¡å‹éƒ¨ç½²åˆ° AWS

AWS æ¨å‡ºäº† [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) å®ä¾‹ç³»åˆ—ï¼Œç”¨äºåœ¨äº‘ä¸­è¿›è¡Œä½æˆæœ¬ã€é«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ æ¨ç†ã€‚Inf1 å®ä¾‹ç”± AWS Inferentia èŠ¯ç‰‡æä¾›æ”¯æŒï¼Œè¯¥èŠ¯ç‰‡æ˜¯ä¸“é—¨ä¸ºæ·±åº¦å­¦ä¹ æ¨ç†å·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„å®šåˆ¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) æ˜¯ç”¨äº Inferentia çš„ SDKï¼Œæ”¯æŒè·Ÿè¸ªå’Œä¼˜åŒ– Transformers æ¨¡å‹ä»¥åœ¨ Inf1 ä¸Šéƒ¨ç½²ã€‚Neuron SDK æä¾›ï¼š

1. æ˜“äºä½¿ç”¨çš„ APIï¼Œåªéœ€æ›´æ”¹ä¸€è¡Œä»£ç å³å¯è·Ÿè¸ªå’Œä¼˜åŒ– TorchScript æ¨¡å‹ï¼Œç”¨äºäº‘ç«¯æ¨ç†ã€‚
2. é’ˆå¯¹ [æ”¹è¿›çš„æˆæœ¬æ€§èƒ½](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/)çš„ç°æˆæ€§èƒ½ä¼˜åŒ–ã€‚
3. å¯¹ä½¿ç”¨ [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) æˆ– [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html) æ„å»ºçš„ Hugging Face transformers æ¨¡å‹çš„æ”¯æŒã€‚

### å½±å“

åŸºäº [BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰](https://huggingface.co/docs/transformers/main/model_doc/bert) æ¶æ„æˆ–å…¶å˜ç§ï¼Œä¾‹å¦‚ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) å’Œ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)ï¼Œåœ¨ Inf1 ä¸Šè¿è¡Œæ—¶æœ€é€‚ç”¨äºéç”Ÿæˆå‹ä»»åŠ¡ï¼Œä¾‹å¦‚æå–å¼é—®ç­”ã€åºåˆ—åˆ†ç±»å’Œæ ‡è®°åˆ†ç±»ã€‚ä½†æ˜¯ï¼Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä»ç„¶å¯ä»¥è°ƒæ•´ä»¥åœ¨ Inf1 ä¸Šè¿è¡Œï¼Œå‚è€ƒè¯¥ [AWS Neuron Marian MT æ•™ç¨‹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)ã€‚æœ‰å…³å¯ä»¥ç›´æ¥åœ¨ Inferentia ä¸Šè½¬æ¢çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… Neuron æ–‡æ¡£çš„ [æ¨¡å‹æ¶æ„é€‚é…](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) éƒ¨åˆ†ã€‚

### ä¾èµ–é¡¹

ä½¿ç”¨ AWS Neuron è½¬æ¢æ¨¡å‹éœ€è¦ [Neuron SDK ç¯å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ï¼Œåœ¨ [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html) ä¸Šé¢„é…ç½®ã€‚

### è½¬æ¢ä¸º AWS Neuron çš„æ¨¡å‹

ä½¿ç”¨ [åœ¨ Python ä¸­ä½¿ç”¨ TorchScript](torchscript.md#åœ¨-python-ä¸­ä½¿ç”¨-torchscript) ä¸­çš„ç›¸åŒä»£ç æ¥è·Ÿè¸ª `BertModel` æ¥å°†æ¨¡å‹è½¬æ¢ä¸º AWS Neuronã€‚å¯¼å…¥ `torch.neuron` æ¡†æ¶æ‰©å±•ä»¥é€šè¿‡ Python API è®¿é—® Neuron SDK çš„ç»„ä»¶ï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

åªéœ€ä¿®æ”¹ä»¥ä¸‹è¡Œï¼š

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

è¿™æ ·å°±ä½¿ Neuron SDK èƒ½å¤Ÿè·Ÿè¸ªæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥åœ¨ Inf1 å®ä¾‹ä¸Šè¿è¡Œã€‚

æœ‰å…³ AWS Neuron SDK åŠŸèƒ½ã€å·¥å…·ã€ç¤ºä¾‹æ•™ç¨‹å’Œæœ€æ–°æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [AWS NeuronSDK æ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ã€‚