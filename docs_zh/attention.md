<!--版权所有2023年的爱抱团队。保留所有权利。

根据Apache许可证第2.0版（“许可证”），除非
许可证。你可以获取许可证的副本

http://www.apache.org/licenses/LICENSE-2.0

除非适用法律要求或书面同意，软件根据许可证条款分发，分发的软件是
“按原样”基础，没有任何明示或暗示的担保或条件。请参阅许可证
有关特定语言以及许可证下的限制的详细信息。

⚠️请注意，此文件采用Markdown格式，但包含特定于我们的doc-builder（类似于MDX）的语法，可能无法在你的Markdown查看器中正确呈现。-->

# 注意机制

大多数Transformer模型在注意力矩阵是方形的意义上使用全注意力。当你有很长的文本时，这可能会成为一个巨大的计算瓶颈。Longformer和Reformer是尝试更高效并使用注意力矩阵的稀疏版本来加速训练的模型。

## LSH注意力

[Reformer](#reformer)使用LSH注意力。在softmax(QK^t)中，只有矩阵QK^t中最大的元素（在softmax维度上）会有有用的贡献。因此，对于Q中的每个查询q，我们只考虑接近q的键k。哈希函数用于确定q和k是否接近。注意力掩码被修改为屏蔽当前标记（除第一个位置外），因为它会给出相等的查询和键（因此非常相似）。由于哈希函数可能有些随机性，实践中使用多个哈希函数（由n_rounds参数确定），然后将它们平均在一起。

## 本地注意力

[Longformer](#longformer)采用本地注意力：通常情况下，本地上下文（例如，左边和右边的两个标记是什么？）就足以针对给定的标记采取行动。此外，通过堆叠具有小窗口的注意力层，最后一层将具有超出窗口中仅有的标记的感受野，从而使它们能够构建整个句子的表示。

一些预选的输入标记还会引起全局注意力：对于这些少量标记，注意力矩阵可以访问所有标记，而且这个过程是对称的：所有其他标记都可以访问这些特定标记（除了在其本地窗口中的标记之外）。该论文的Figure 2d中显示了这一点，请参阅下面的样本注意力掩码：

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

使用这些具有较少参数的注意力矩阵使得模型能够接受更长的输入序列长度。

## 其他技巧

### 轴向位置编码

[Reformer](#reformer)使用轴向位置编码：在传统的Transformer模型中，位置编码E是一个大小为\(l\)乘以\(d\)的矩阵，\(l\)是序列长度，\(d\)是隐藏状态的维度。如果文本非常长，这个矩阵可能非常庞大，并占据GPU上太多的空间。为了减轻这个问题，轴向位置编码将大矩阵E分解为两个较小的矩阵E1和E2，它们的维度分别为\(l_{1} \times d_{1}\)和\(l_{2} \times d_{2}\)，使得\(l_{1} \times l_{2} = l\)且\(d_{1} + d_{2} = d\)（用于长度的乘积使得这个矩阵变得更小）。E中第\(j\)个时间步的嵌入是通过将E1中的\(j \% l1\)时间步的嵌入和E2中的\(j // l1\)时间步的嵌入进行连接获得的。
