<!--⚠️ 注意，此文件是Markdown格式的，但包含特定的语法以便于我们的文档生成器（类似于MDX）渲染，在你的Markdown查看器中可能无法正常显示。-->

# 社区

此页面汇集了由社区开发的🤗Transformers资源。

## 社区资源：

| 资源     |      描述      |      作者      |
|:----------|:-------------|------:|
| [Hugging Face Transformers 术语表速记卡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 一套基于[Transformers Docs 术语表](glossary.md)的速记卡，它们以易于学习/复习的形式展示，并使用了[Anki](https://apps.ankiweb.net/)这个针对长期知识保留而专门设计的开源、跨平台应用程序。请参阅此[介绍视频，了解如何使用这些速记卡](https://www.youtube.com/watch?v=Dji_h7PILrw)。 | [Darigov Research](https://www.darigovresearch.com/) |

## 社区笔记本：

| 笔记本 | 描述 | 作者 | |
|:----------|:-------------|:-------------|------:|
| [微调预训练的Transformer生成歌词](https://github.com/AlekseyKorshuk/huggingartists) | 通过微调GPT-2模型来生成你最喜欢艺术家的风格的歌词 | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |
| [在Tensorflow 2上训练T5模型](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用Tensorflow 2训练T5模型来完成各种任务。该笔记本示例展示了使用SQUAD在Tensorflow 2中实现的问答任务 | [Muhammad Harris](https://github.com/HarrisDePerceptron) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |
| [在TPU上训练T5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) | 如何使用Transformers和Nlp在SQUAD上训练T5模型 | [Suraj Patil](https://github.com/patil-suraj) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |
| [微调T5进行分类和多选](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) | 如何使用PyTorch Lightning的文本-文本格式微调T5进行分类和多选任务 |  [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |
| [在新数据集和语言上微调DialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) | 如何在新的数据集上微调DialoGPT模型来构建具有开放对话功能的聊天机器人 |  [Nathan Cooper](https://github.com/ncoop57) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |
| [使用Reformer进行长序列建模](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) | 如何使用Reformer模型训练长度达到500,000个标记的序列 |  [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  |
| [微调BART进行摘要生成](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | 如何使用blurr使用fastai微调BART进行摘要生成 | [Wayde Gilliam](https://ohmeow.com/) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |
| [微调预训练的Transformer生成推特的tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | 通过微调GPT-2模型生成与你最喜欢的突击队推特帐号风格相似的推文 |  [Boris Dayma](https://github.com/borisdayma) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |
| [使用Weights & Biases优化🤗Hugging Face模型](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | 一个完整的教程，展示了如何将Weights & Biases与Hugging Face集成 | [Boris Dayma](https://github.com/borisdayma) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |
| [针对现有预训练模型进行长序列建模](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) | 如何为现有的预训练模型构建一个“长”版本 |  [Iz Beltagy](https://beltagy.net) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |
| [微调Longformer进行问答](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | 如何微调Longformer模型进行问答任务 | [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |
| [使用🤗nlp评估模型](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | 如何使用`nlp`对Longformer在TriviaQA上进行评估 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |
| [微调T5进行情感跨度提取](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) | 如何使用PyTorch Lightning的文本-文本格式微调T5进行情感跨度提取 |  [Lorenzo Ampil](https://github.com/enzoampil) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |
| [微调DistilBert进行多类别分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | 如何使用PyTorch微调DistilBert进行多类别分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) |  [![[open-in-colab]](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)|
|[微调BERT进行多标签分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)| 如何使用PyTorch微调BERT进行多标签分类 |[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![[open-in-colab]](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|
|[微调T5进行摘要生成](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)| 如何使用PyTorch微调T5进行摘要生成，并使用WandB来跟踪实验|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![[open-in-colab]](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|
|[使用动态填充/分桶加速Transformers微调](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)| 如何使用动态填充/分桶将Transformers微调时间加速2倍或更多|[Michael Benesty](https://github.com/pommedeterresautee) |[![[open-in-colab]](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)|
|[为MLM预训练Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)| 如何训练具有双向自注意层的Reformer模型 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)|
|[扩展并Fine-tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)| 如何使用AllenAI上的预训练SciBERT模型增加CORD数据集的词汇量和流水线处理|[Tanmay Thakur](https://github.com/lordtt13) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)|
|[使用Trainer API微调BlenderBotSmall进行摘要生成](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)| 如何在自定义数据集上微调BlenderBotSmall以进行摘要生成，使用Trainer API |[Tanmay Thakur](https://github.com/lordtt13) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)|
|[微调Electra并使用Integrated Gradients解释](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | 如何使用Captum的Integrated Gradients解释微调Electra进行情感分析的预测 | [Eliza Szczechla](https://elsanns.github.io) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)|
|[对非英语GPT-2模型使用Trainer类进行微调](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | 如何对非英语GPT-2模型使用Trainer类进行微调 | [Philipp Schmid](https://www.philschmid.de) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)|
|[微调DistilBERT进行多标签分类任务](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | 如何微调DistilBERT模型进行多标签分类任务 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)|
|[微调ALBERT进行句对分类](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | 如何微调ALBERT模型或其他基于BERT的模型进行句对分类任务 | [Nadir El Manouzi](https://github.com/NadirEM) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1oVdJFLnM-HreLCltA5N6Io8qlLxfwtKd?usp=sharing)|
|[微调Roberta进行情感分析](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | 如何微调Roberta模型进行情感分析任务 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1L9ZuT31RuMBzyvBSsZa0otJ3G-Z2RBpo?usp=sharing)|
|[使用Wav2Vec2创建YouTube字幕](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何通过Wav2Vec的音频转录功能为YouTube视频创建字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing) |
| [在CIFAR-10上微调Vision Transformer，使用PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | 如何使用HuggingFace Transformers、Datasets和PyTorch Lightning在CIFAR-10上微调Vision Transformer（ViT）| [Niels Rogge](https://github.com/nielsrogge) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11of0ra42ETXpBhHaF8woCZkGzPLgRmej?usp=sharing) |
| [在CIFAR-10上微调Vision Transformer，使用🤗Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | 如何使用HuggingFace Transformers、Datasets和🤗Trainer在CIFAR-10上微调Vision Transformer（ViT）| [Niels Rogge](https://github.com/nielsrogge) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1AqsALKOzr-4OPtoXWfxKR6o_RM_dwJBl?usp=sharing) |
| [评估LUKE在Open Entity（一个实体类型数据集）上](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | 如何在Open Entity数据集上评估LukeForEntityClassification模型 | [Ikuya Yamada](https://github.com/ikuyamada) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |
| [评估LUKE在TACRED（一个关系提取数据集）上](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | 如何在TACRED数据集上评估LukeForEntityPairClassification模型 | [Ikuya Yamada](https://github.com/ikuyamada) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |
| [评估LUKE在CoNLL-2003（一个重要的命名实体识别基准数据集）上](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | 如何在CoNLL-2003数据集上评估LukeForEntitySpanClassification模型 | [Ikuya Yamada](https://github.com/ikuyamada) |[![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |
| [根据现有模型显示Big Bird在Trivia QA上的评估](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | 如何评估BigBird在Trivia QA上进行长文档问答任务 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ekkRzqWu7tDPrsG0vVpJO7oTt0XbAEKd?usp=sharing) |
| [创建和调整下游任务的视频检索](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何通过Wav2Vec的音频转录功能为YouTube视频创建视频字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing) |
