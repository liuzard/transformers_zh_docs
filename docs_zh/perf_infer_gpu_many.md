<!--ç‰ˆæƒæ‰€æœ‰ 2022 The HuggingFace Teamã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼Œç½‘å€ä¸º

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ç…§
â€œæŒ‰åŸæ ·â€ BASISæä¾›çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–é»˜ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·çœ‹è®¸å¯è¯çš„è¦æ±‚

âš ï¸ è¯·æ³¨æ„ï¼Œæœ¬æ–‡ä»¶åœ¨Markdownä¸­ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äºMDXï¼‰ï¼Œå¯èƒ½æ— æ³•
åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚

-->

# Efficient Inference on a Multiple GPUs

æœ¬æ–‡æ¡£åŒ…å«æœ‰å…³å¦‚ä½•åœ¨å¤šä¸ªGPUä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†çš„ä¿¡æ¯ã€‚
<Tip>

æ³¨æ„ï¼šå¤šGPUè®¾ç½®å¯ä»¥ä½¿ç”¨åœ¨[single GPU section](perf_infer_gpu_one.md)ä¸­æè¿°çš„å¤§éƒ¨åˆ†ç­–ç•¥ã€‚ä¸è¿‡ï¼Œæ‚¨å¿…é¡»äº†è§£ä¸€äº›ç®€å•çš„æŠ€æœ¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä½¿ç”¨ã€‚

</Tip>

## BetterTransformer

[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) å°†ğŸ¤— Transformersæ¨¡å‹è½¬æ¢ä¸ºä½¿ç”¨åŸºäºPyTorchçš„å¿«é€Ÿæ‰§è¡Œè·¯å¾„ï¼ˆåº•å±‚è°ƒç”¨ä¼˜åŒ–çš„å†…æ ¸ï¼Œå¦‚Flash Attentionï¼‰ã€‚

BetterTransformerè¿˜æ”¯æŒæ›´å¿«çš„æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘æ¨¡å‹çš„å•ä¸ªGPUå’Œå¤šä¸ªGPUæ¨ç†ã€‚

<Tip>

Flash Attentionåªèƒ½ç”¨äºä½¿ç”¨fp16æˆ–bf16 dtypeçš„æ¨¡å‹ã€‚åœ¨ä½¿ç”¨BetterTransformerä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„dtypeã€‚
  
</Tip>

### è§£ç å™¨æ¨¡å‹

å¯¹äºæ–‡æœ¬æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è§£ç å™¨æ¨¡å‹ï¼ˆGPTã€T5ã€Llamaç­‰ï¼‰ï¼ŒBetterTransformer APIå°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºä½¿ç”¨[`torch.nn.functional.scaled_dot_product_attention`è¿ç®—ç¬¦](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) (SDPA) ï¼Œè¯¥è¿ç®—ç¬¦ä»…åœ¨PyTorch 2.0åŠä»¥ä¸Šç‰ˆæœ¬ä¸­å¯ç”¨ã€‚

è¦å°†æ¨¡å‹è½¬æ¢ä¸ºBetterTransformerï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
# è½¬æ¢æ¨¡å‹ä¸ºBetterTransformer
model.to_bettertransformer()

# ç”¨äºè®­ç»ƒæˆ–æ¨ç†
```

SDPAè¿˜å¯ä»¥åœ¨åº•å±‚è°ƒç”¨[Flash Attention](https://arxiv.org/abs/2205.14135)å†…æ ¸ã€‚è¦å¯ç”¨Flash Attentionæˆ–æ£€æŸ¥åœ¨ç»™å®šè®¾ç½®ï¼ˆç¡¬ä»¶ã€é—®é¢˜å¤§å°ï¼‰ä¸­æ˜¯å¦å¯ç”¨ï¼Œå¯ä»¥ä½¿ç”¨`torch.backends.cuda.sdp_kernel`ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š

```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to("cuda")
# è½¬æ¢æ¨¡å‹ä¸ºBetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

å¦‚æœå‡ºç°å¸¦æœ‰è·Ÿè¸ªä¿¡æ¯ï¼ˆtracebackï¼‰çš„é”™è¯¯æç¤ºå¦‚ä¸‹ï¼š

```bash
RuntimeError: No available kernel.  Aborting execution.
```

å°è¯•ä½¿ç”¨PyTorchçš„å¤œç‰ˆï¼ˆnightly versionï¼‰ï¼Œè¯¥ç‰ˆæœ¬å¯èƒ½å¯¹Flash Attentionæœ‰æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼š

```bash
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

é˜…è¯»æ­¤[åšæ–‡](https://pytorch.org/blog/out-of-the-box-acceleration/)ä»¥äº†è§£æœ‰å…³é€šè¿‡BetterTransformer + SDPA APIå¯ä»¥å®ç°çš„æ›´å¤šå†…å®¹ã€‚

### ç¼–ç å™¨æ¨¡å‹

å¯¹äºç¼–ç å™¨æ¨¡å‹çš„æ¨ç†ï¼ŒBetterTransformerå°†ç¼–ç å™¨å±‚çš„å‰å‘è°ƒç”¨åˆ†æ´¾ç»™ç­‰æ•ˆçš„[`torch.nn.TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)ï¼Œè¯¥å±‚å°†æ‰§è¡Œç¼–ç å™¨å±‚çš„å¿«é€Ÿè·¯å¾„å®ç°ã€‚

ç”±äº`torch.nn.TransformerEncoderLayer`çš„å¿«é€Ÿè·¯å¾„ä¸æ”¯æŒè®­ç»ƒï¼Œå› æ­¤ä¼šå°†å…¶è°ƒåº¦åˆ°`torch.nn.functional.scaled_dot_product_attention`ï¼Œåè€…ä¸åˆ©ç”¨åµŒå¥—å¼ é‡ï¼Œä½†å¯ä»¥ä½¿ç”¨Flash Attentionæˆ–Memory-Efficient Attentionèåˆå†…æ ¸ã€‚

æœ‰å…³BetterTransformeræ€§èƒ½çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§æ­¤[åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤[åšæ–‡](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)ä¸­äº†è§£æœ‰å…³ç¼–ç å™¨æ¨¡å‹çš„BetterTransformerä¿¡æ¯ã€‚


## é«˜çº§ç”¨æ³•ï¼šæ··åˆFP4ï¼ˆæˆ–Int8ï¼‰å’ŒBetterTransformer

æ‚¨å¯ä»¥ç»“åˆä¸Šè¿°ä¸åŒçš„æ–¹æ³•æ¥è·å¾—æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†FP4æ··åˆç²¾åº¦æ¨ç†+flash attentionä¸BetterTransformerä¸€èµ·ä½¿ç”¨ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```