<!--ç‰ˆæƒæ‰€æœ‰2023 The HuggingFaceå›¢é˜Ÿã€‚ç‰ˆæƒæ‰€æœ‰ã€‚
æ ¹æ®Apacheè®¸å¯è¯ï¼Œç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éä½ éµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æŒ‰åŸæ ·åˆ†å‘è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€æ–¹å¼åˆ†å‘çš„ï¼Œ
ä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯¦ç»†äº†è§£è®¸å¯è¯ä¸­çš„é™åˆ¶å’Œæ¡ä»¶ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä»¥Markdownæ ¼å¼ç¼–å†™ï¼Œä½†åŒ…å«æˆ‘ä»¬ doc-builder çš„ç‰¹æ®Šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œè¿™å¯èƒ½åœ¨ä½ çš„ Markdown è§†å›¾å™¨ä¸­æ— æ³•æ­£ç¡®å‘ˆç°ã€‚-->

# ä½¿ç”¨ğŸ¤—PEFTåŠ è½½adapters

[[open-in-colab]]

[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰](https://huggingface.co/blog/peft) æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¹‹ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆadaptersï¼‰ã€‚adaptersç”¨äºå­¦ä¹ ç‰¹å®šäºä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²ç»è¢«è¯æ˜åœ¨ä½¿ç”¨æ›´ä½çš„è®¡ç®—èµ„æºæ—¶å¯ä»¥éå¸¸èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚

ä½¿ç”¨PEFTè®­ç»ƒçš„adaptersé€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œè¿™ä½¿å¾—åˆ†äº«ã€å­˜å‚¨å’ŒåŠ è½½å®ƒä»¬éå¸¸æ–¹ä¾¿ã€‚

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">å­˜å‚¨åœ¨Hubä¸Šçš„OPTForCausalLMæ¨¡å‹çš„adaptersæƒé‡ä»…ä¸º~6MBï¼Œè€Œæ¨¡å‹æƒé‡çš„å®Œæ•´å¤§å°å¯ä»¥è¾¾åˆ°~700MBã€‚</figcaption>
</div>

å¦‚æœä½ æƒ³äº†è§£æœ‰å…³ğŸ¤—PEFTåº“çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚

## è®¾ç½®

é¦–å…ˆï¼Œé€šè¿‡å®‰è£…ğŸ¤—PEFTæ¥å¼€å§‹ï¼š

```bash
pip install peft
```

å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„åŠŸèƒ½ï¼Œå¯ä»¥è€ƒè™‘ä»æºä»£ç å®‰è£…åº“ï¼š

```bash
pip install git+https://github.com/huggingface/peft.git
```

## æ”¯æŒçš„PEFTæ¨¡å‹

ğŸ¤—TransformersåŸç”Ÿæ”¯æŒä¸€äº›PEFTæ–¹æ³•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åŠ è½½æœ¬åœ°æˆ–Hubä¸Šå­˜å‚¨çš„adaptersæƒé‡ï¼Œå¹¶ä½¿ç”¨å°‘é‡ä»£ç è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚æ”¯æŒä»¥ä¸‹æ–¹æ³•ï¼š

- [ä½ç§©adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•ï¼ˆå¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºè°ƒæ•´ï¼‰æˆ–äº†è§£æœ‰å…³ğŸ¤—PEFTåº“çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œè¯·å‚é˜…æ–‡æ¡£ã€‚

## åŠ è½½PEFTadapters

è¦ä»ğŸ¤—transformersåŠ è½½å’Œä½¿ç”¨PEFTadaptersæ¨¡å‹ï¼Œè¯·ç¡®ä¿Hubä»“åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«`adapter_config.json`æ–‡ä»¶å’Œadaptersæƒé‡ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½PEFTadaptersæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦ä¸ºå› æœè¯­è¨€æ¨¡å‹åŠ è½½PEFTadaptersæ¨¡å‹ï¼š

1. æŒ‡å®šPEFTæ¨¡å‹ID
2. å°†å…¶ä¼ é€’ç»™[`AutoModelForCausalLM`]ç±»

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```


>ä½ å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºæœ¬æ¨¡å‹ç±»ï¼Œå¦‚ `OPTForCausalLM` æˆ– `LlamaForCausalLM`æ¥åŠ è½½PEFTadaptersã€‚


ä½ è¿˜å¯ä»¥é€šè¿‡è°ƒç”¨`load_adapter`æ–¹æ³•æ¥åŠ è½½PEFTadaptersï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## ä»¥8ä½æˆ–4ä½åŠ è½½

`bitsandbytes`é›†æˆæ”¯æŒ8ä½å’Œ4ä½ç²¾åº¦æ•°æ®ç±»å‹ï¼Œå¯¹äºåŠ è½½å¤§å‹æ¨¡å‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒèŠ‚çœäº†å†…å­˜ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`bitsandbytes`é›†æˆ[æŒ‡å—](./quantization#bitsandbytes-integration)ï¼‰ã€‚å°†`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°æ·»åŠ åˆ°[`~PreTrainedModel.from_pretrained`]ä¸­ï¼Œå¹¶å°† `device_map="auto"` è®¾ç½®ä¸ºæœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°ä½ çš„ç¡¬ä»¶ä¸Šï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map="auto", load_in_8bit=True)
```

## æ·»åŠ æ–°çš„adapters

åªè¦æ–°adaptersçš„ç±»å‹ä¸å½“å‰adaptersç›¸åŒï¼Œä½ å°±å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.add_adapter`]å°†æ–°adaptersæ·»åŠ åˆ°å¸¦æœ‰ç°æœ‰adaptersçš„æ¨¡å‹ä¸­ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨æ¨¡å‹ä¸Šå·²æ·»åŠ äº†ç°æœ‰çš„LoRAadaptersï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

è¦æ·»åŠ æ–°adaptersï¼š

```py
# ä½¿ç”¨ç›¸åŒçš„é…ç½®é™„åŠ æ–°çš„adapters
model.add_adapter(lora_config, adapter_name="adapter_2")
```

ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.set_adapter`]æ¥è®¾ç½®è¦ä½¿ç”¨çš„adaptersï¼š

```py
# ä½¿ç”¨adapters_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# ä½¿ç”¨adapters_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## å¯ç”¨å’Œç¦ç”¨adapters

å‘æ¨¡å‹ä¸­æ·»åŠ adaptersåï¼Œä½ å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨adaptersæ¨¡å—ã€‚è¦å¯ç”¨adaptersæ¨¡å—ï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# ç”¨éšæœºæƒé‡åˆå§‹åŒ–
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

è¦ç¦ç”¨adaptersæ¨¡å—ï¼š

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## è®­ç»ƒPEFT adapters

PEFTadaptersç”±[`Trainer`]ç±»æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥é’ˆå¯¹ç‰¹å®šç”¨ä¾‹è®­ç»ƒadaptersã€‚åªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç å³å¯ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ªLoRAadaptersï¼š



>å¦‚æœä½ ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]è¿›è¡Œå¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](training.md) æ•™ç¨‹ã€‚



1. ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰adaptersé…ç½®ï¼ˆæœ‰å…³è¶…å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[`~peft.LoraConfig`]ï¼‰ã€‚

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. å°†adaptersæ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚

```py
model.add_adapter(peft_config)
```

3. ç°åœ¨ä½ å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™[`Trainer`]ï¼

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

è¦ä¿å­˜å·²è®­ç»ƒçš„adapterså¹¶é‡æ–°åŠ è½½ï¼š

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer  
-   8-bit / 4-bit examples ?
-->