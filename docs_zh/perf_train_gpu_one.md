<!--
ç‰ˆæƒæ‰€æœ‰2022 HuggingFaceå›¢é˜Ÿã€‚ ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œä½ é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™ä¾æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œåŸæ ·â€åŸåˆ™ï¼Œå³æ²¡æœ‰ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚å…³äºè¯æœ¬èº«çš„æ¹¿æç¤ºï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹é€‚é”€æ€§ã€ç‰¹å®šç›®çš„çš„é€‚ç”¨æ€§å’Œæ— ä¾µæƒçš„ä¿è¯ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…è®¸å¯è¯ã€‚

è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä¸ºMarkdownæ ¼å¼ï¼Œä½†åŒ…å«ç‰¹å®šäºæˆ‘ä»¬doc-builderçš„è¯­æ³•ï¼ˆç±»ä¼¼äºMDXï¼‰ï¼Œè¿™åœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®å‘ˆç°ã€‚
-->

# åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒçš„æ–¹æ³•å’Œå·¥å…·

æœ¬æŒ‡å—æ¼”ç¤ºäº†ä½ å¯ä»¥ä½¿ç”¨çš„å®ç”¨æŠ€æœ¯ï¼Œä»¥é€šè¿‡ä¼˜åŒ–å†…å­˜åˆ©ç”¨ç‡ã€åŠ å¿«è®­ç»ƒé€Ÿåº¦æˆ–ä¸¤è€…å…¼é¡¾æ¥æé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ã€‚å¦‚æœä½ æƒ³äº†è§£åœ¨è®­ç»ƒæœŸé—´å¦‚ä½•åˆ©ç”¨GPUï¼Œè¯·é¦–å…ˆå‚è€ƒ[æ¨¡å‹è®­ç»ƒè§£å‰–](model_memory_anatomy.md)æ¦‚å¿µæŒ‡å—ã€‚æœ¬æŒ‡å—ä¾§é‡äºå®ç”¨æŠ€æœ¯ã€‚

<Tip>

å¦‚æœä½ å¯ä»¥è®¿é—®å…·æœ‰å¤šä¸ªGPUçš„è®¡ç®—æœºï¼Œåˆ™è¿™äº›æ–¹æ³•ä»ç„¶æœ‰æ•ˆï¼Œå¹¶ä¸”ä½ è¿˜å¯ä»¥åˆ©ç”¨åœ¨[å¤šGPUéƒ¨åˆ†](perf_train_gpu_many.md)ä¸­æ¦‚è¿°çš„å…¶ä»–æ–¹æ³•ã€‚

</Tip>

åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œåº”åŒæ—¶è€ƒè™‘ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢ï¼š

* æ•°æ®ååé‡/è®­ç»ƒæ—¶é—´
* æ¨¡å‹æ€§èƒ½

æœ€å¤§åŒ–ååé‡ï¼ˆæ ·æœ¬/ç§’ï¼‰å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚é€šå¸¸ï¼Œè¿™é€šè¿‡å°½å¯èƒ½å¤šåœ°åˆ©ç”¨GPUå¹¶å°†å…¶å¡«å……åˆ°å…¶æé™æ¥å®ç°ã€‚å¦‚æœæ‰€éœ€çš„æ‰¹æ¬¡å¤§å°è¶…å‡ºäº†GPUå†…å­˜çš„é™åˆ¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚æ¸å˜ç´¯ç§¯ï¼‰æ¥å¸®åŠ©è§£å†³å†…å­˜é—®é¢˜ã€‚

ä½†æ˜¯ï¼Œå¦‚æœé¦–é€‰çš„æ‰¹æ¬¡å¤§å°é€‚åˆå†…å­˜ï¼Œé‚£ä¹ˆå°±æ²¡æœ‰ç†ç”±åº”ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚ä»…ä»…å› ä¸ºå¯ä»¥ä½¿ç”¨å¤§æ‰¹é‡å¤§å°ï¼Œå¹¶ä¸æ„å‘³ç€å¿…é¡»ä½¿ç”¨ã€‚ä½œä¸ºè¶…å‚æ•°è°ƒæ•´çš„ä¸€éƒ¨åˆ†ï¼Œä½ åº”ç¡®å®šå“ªä¸ªæ‰¹æ¬¡å¤§å°äº§ç”Ÿæœ€ä½³ç»“æœï¼Œç„¶åç›¸åº”åœ°ä¼˜åŒ–èµ„æºã€‚æœ¬æŒ‡å—ä¸­æ¶‰åŠçš„æ–¹æ³•å’Œå·¥å…·å¯ä»¥æ ¹æ®å®ƒä»¬å¯¹è®­ç»ƒè¿‡ç¨‹çš„å½±å“è¿›è¡Œåˆ†ç±»ï¼š

| æ–¹æ³•/å·¥å…·                                                   | æé«˜è®­ç»ƒé€Ÿåº¦      | ä¼˜åŒ–å†…å­˜åˆ©ç”¨ç‡            |
|:-----------------------------------------------------------|:------------------------|:-----------------------------|
| [é€‰æ‹©æ‰¹é‡å¤§å°](#é€‰æ‹©æ‰¹é‡å¤§å°)                               | æ˜¯                     | æ˜¯                          |
| [æ¸å˜ç´¯ç§¯](#æ¸å˜ç´¯ç§¯)                                      | å¦                     | æ˜¯                          |
| [æ¸å˜æ£€æŸ¥ç‚¹](#æ¸å˜æ£€æŸ¥ç‚¹)                                  | å¦                     | æ˜¯                          |
| [æ··åˆç²¾åº¦è®­ç»ƒ](#æ··åˆç²¾åº¦è®­ç»ƒ)                              | æ˜¯                     | ï¼ˆå¦ï¼‰                       |
| [é€‰æ‹©ä¼˜åŒ–å™¨](#é€‰æ‹©ä¼˜åŒ–å™¨)                                   | æ˜¯                     | æ˜¯                          |
| [æ•°æ®é¢„åŠ è½½](#æ•°æ®é¢„åŠ è½½)                                     | æ˜¯                     | å¦                          |
| [DeepSpeed Zero](#deepspeed-zero)                          | å¦                     | æ˜¯                          |
| [torch.compile](#ä½¿ç”¨-torchcompile)                       | æ˜¯                     | å¦                           |

<Tip>

æ³¨æ„ï¼šå½“ä½¿ç”¨å°å‹æ¨¡å‹å’Œå¤§æ‰¹æ¬¡å¤§å°è¿›è¡Œæ··åˆç²¾åº¦æ—¶ï¼Œå°†èŠ‚çœä¸€äº›å†…å­˜ï¼Œä½†åœ¨ä½¿ç”¨å¤§å‹æ¨¡å‹å’Œå°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå†…å­˜ä½¿ç”¨é‡å°†æ›´å¤§ã€‚

</Tip>

ä½ å¯ä»¥ç»„åˆä¸Šè¿°æ–¹æ³•ä»¥è·å¾—ç´¯ç§¯æ•ˆæœã€‚æ— è®ºä½ æ˜¯ä½¿ç”¨[`Trainer`]è®­ç»ƒæ¨¡å‹è¿˜æ˜¯ç¼–å†™çº¯PyTorchå¾ªç¯ï¼Œéƒ½å¯ä»¥ä½¿ç”¨è¿™äº›æŠ€æœ¯ã€‚ï¼Œåœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ğŸ¤—Accelerate[é…ç½®è¿™äº›ä¼˜åŒ–ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•è·å¾—è¶³å¤Ÿçš„æ”¶ç›Šï¼Œä½ å¯ä»¥å°è¯•ä»¥ä¸‹é€‰é¡¹ï¼š
* [æŸ¥çœ‹ä½¿ç”¨é«˜æ•ˆè½¯ä»¶é¢„æ„å»ºæ„å»ºè‡ªå®šä¹‰Dockerå®¹å™¨](#é«˜æ•ˆè½¯ä»¶é¢„æ„å»º)
* [è€ƒè™‘ä½¿ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹](#ä¸“å®¶æ··åˆ)
* [å°†æ¨¡å‹è½¬æ¢ä¸ºBetterTransformerä»¥åˆ©ç”¨PyTorchæœ¬æœºæ³¨æ„åŠ›](#ä½¿ç”¨pytorchæœ¬æœºæ³¨æ„åŠ›)

æœ€åï¼Œå³ä½¿åœ¨åˆ‡æ¢åˆ°åƒA100è¿™æ ·çš„æœåŠ¡å™¨çº§GPUä¹‹åï¼Œå¦‚æœä»¥ä¸Šæ‰€æœ‰æ–¹æ³•ä»ç„¶ä¸è¶³å¤Ÿï¼Œè¯·è€ƒè™‘åˆ‡æ¢åˆ°å¤šGPUè®¾ç½®ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•åœ¨å¤šGPUè®¾ç½®ä¸­ä»ç„¶æœ‰æ•ˆï¼Œæ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥åˆ©ç”¨åœ¨[å¤šGPUéƒ¨åˆ†](perf_train_gpu_many.md)ä¸­æ¦‚è¿°çš„å…¶ä»–å¹¶è¡ŒæŠ€æœ¯ã€‚

## é€‰æ‹©æ‰¹é‡å¤§å°

ä¸ºäº†å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¯·é¦–å…ˆç¡®å®šé€‚å½“çš„æ‰¹é‡å¤§å°ã€‚å»ºè®®ä½¿ç”¨å¤§å°ä¸º2^Nçš„æ‰¹é‡å¤§å°å’Œè¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°ã€‚é€šå¸¸ï¼Œå®ƒæ˜¯8çš„å€æ•°ï¼Œä½†å¯èƒ½æ›´é«˜ï¼Œå…·ä½“å–å†³äºæ‰€ä½¿ç”¨çš„ç¡¬ä»¶å’Œæ¨¡å‹çš„æ•°æ®ç±»å‹ã€‚

æœ‰å…³å‚è€ƒï¼Œè¯·æŸ¥çœ‹NVIDIAå…³äºå…¨è¿æ¥å±‚è¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°ï¼ˆæ¶‰åŠGEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ï¼‰çš„[å»ºè®®](
https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features)å’Œ[æ‰¹é‡å¤§å°](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size)ã€‚

å¼ é‡æ ¸å¿ƒè¦æ±‚æ ¹æ®æ•°æ®ç±»å‹å’Œç¡¬ä»¶æ¥å®šä¹‰ä¹˜æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äºfp16æ•°æ®ç±»å‹ï¼Œæ¨èä½¿ç”¨8çš„å€æ•°ï¼Œé™¤éä½¿ç”¨çš„æ˜¯A100 GPUï¼Œæ­¤æ—¶è¯·ä½¿ç”¨64çš„å€æ•°ã€‚

å¯¹äºè¾ƒå°çš„å‚æ•°ï¼Œè¯·è€ƒè™‘[ç»´åº¦é‡åŒ–æ•ˆåº”](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization)ã€‚è¿™æ˜¯å¹³é“ºå‘ç”Ÿçš„åœ°æ–¹ï¼Œæ­£ç¡®çš„ä¹˜æ•°å¯ä»¥æ˜¾ç€åŠ å¿«é€Ÿåº¦ã€‚

## æ¸å˜ç´¯ç§¯

**æ¸å˜ç´¯ç§¯**æ–¹æ³•æ—¨åœ¨é€šè¿‡å°æ‰¹æ¬¡é€æ­¥è®¡ç®—æ¸å˜ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§ä¸ºæ•´ä¸ªæ‰¹æ¬¡è®¡ç®—æ¸å˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åå¤åœ¨æ›´å°çš„æ‰¹æ¬¡ä¸­æ‰§è¡Œæ¨¡å‹çš„å‰å‘å’Œåå‘ä¼ æ’­ã€å¹¶ç´¯ç§¯æ¢¯åº¦ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ç§¯ç´¯è¶³å¤Ÿæ•°é‡çš„æ¢¯åº¦åï¼Œæ‰§è¡Œæ¨¡å‹çš„ä¼˜åŒ–æ­¥éª¤ã€‚é€šè¿‡ä½¿ç”¨æ¸å˜ç´¯ç§¯ï¼Œå¯ä»¥å°†**æœ‰æ•ˆæ‰¹æ¬¡å¤§å°**å¢åŠ åˆ°GPUçš„å†…å­˜å®¹é‡æ‰€é™åˆ¶çš„èŒƒå›´ä¹‹å¤–ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¸å˜ç´¯ç§¯å¼•å…¥çš„é™„åŠ å‰å‘å’Œåå‘ä¼ æ’­å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚

å¯ä»¥é€šè¿‡å‘[`TrainingArguments`]æ·»åŠ `gradient_accumulation_steps`å‚æ•°æ¥å¯ç”¨æ¸å˜ç´¯ç§¯ï¼š

```py
training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ä¸º4ã€‚

æˆ–è€…ï¼Œä½¿ç”¨ğŸ¤—Accelerateå¯¹è®­ç»ƒå¾ªç¯è¿›è¡Œå…¨é¢æ§åˆ¶ã€‚åœ¨æœ¬æŒ‡å—çš„[further down](#using-accelerate)ä¸­æŸ¥æ‰¾ğŸ¤—Accelerateç¤ºä¾‹ã€‚

è™½ç„¶å»ºè®®å°½å¯èƒ½å……åˆ†åˆ©ç”¨GPUçš„ä½¿ç”¨ç‡ï¼Œä½†é«˜æ•°é‡çš„æ¸å˜ç´¯ç§¯æ­¥éª¤å¯èƒ½å¯¼è‡´è®­ç»ƒå‡é€Ÿæ›´æ˜æ˜¾ã€‚è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹ã€‚å‡è®¾`per_device_train_batch_size=4`è€Œæ²¡æœ‰æ¸å˜ç´¯ç§¯æ—¶è¾¾åˆ°äº†GPUçš„é™åˆ¶ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨å¤§å°ä¸º64çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒï¼Œè¯·å‹¿å°†`per_device_train_batch_size`è®¾ç½®ä¸º1ï¼Œå¹¶å°†`gradient_accumulation_steps`è®¾ç½®ä¸º64ã€‚ç›¸åï¼Œä¿æŒ`per_device_train_batch_size=4`ï¼Œå¹¶è®¾ç½®`gradient_accumulation_steps=16`ã€‚è¿™æ ·å¯ä»¥è·å¾—ç›¸åŒçš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼ŒåŒæ—¶æ›´å¥½åœ°åˆ©ç”¨å¯ç”¨çš„GPUèµ„æºã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹æ‰¹å¤„ç†å¤§å°å’Œæ¸å˜ç´¯ç§¯åŸºå‡†[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)å’Œ[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957)ã€‚

## æ¸å˜æ£€æŸ¥ç‚¹

å³ä½¿å°†æ‰¹æ¬¡å¤§å°è®¾ç½®ä¸º1å¹¶ä½¿ç”¨æ¸å˜ç´¯ç§¯ï¼Œä¸€äº›å¤§å‹æ¨¡å‹ä»å¯èƒ½é‡åˆ°å†…å­˜é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºè¿˜æœ‰å…¶ä»–ç»„ä»¶ä¹Ÿéœ€è¦å†…å­˜å­˜å‚¨ã€‚

åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¿å­˜æ‰€æœ‰æ¿€æ´»ä»¥åœ¨åå‘ä¼ æ’­ä¸­è®¡ç®—æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´æ˜¾ç€çš„å†…å­˜å¼€é”€ã€‚æŠ›å¼ƒè¿™äº›æ¿€æ´»å¹¶åœ¨åå‘ä¼ æ’­æœŸé—´éœ€è¦æ—¶é‡æ–°è®¡ç®—å®ƒä»¬çš„æ›¿ä»£æ–¹æ³•ä¼šå¼•å…¥è®¡ç®—å¼€é”€å¹¶å‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚

**æ¸å˜æ£€æŸ¥ç‚¹**åœ¨è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´æä¾›äº†ä¸€ç§å¦¥åæ–¹æ¡ˆï¼Œå¹¶åœ¨è®¡ç®—å›¾ä¸­ä¿å­˜äº†é€‰å®šçš„æ¿€æ´»å…ƒç´ ï¼Œå› æ­¤åªéœ€é‡æ–°è®¡ç®—æ¢¯åº¦æ‰€éœ€çš„æ¿€æ´»å…ƒç´ çš„ä¸€éƒ¨åˆ†ã€‚æœ‰å…³æ¸å˜æ£€æŸ¥ç‚¹çš„è¯¦ç»†è§£é‡Šï¼Œè¯·å‚é˜…[è¿™ç¯‡ç²¾å½©çš„æ–‡ç« ](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)ã€‚

è¦åœ¨[`Trainer`]ä¸­å¯ç”¨æ¸å˜æ£€æŸ¥ç‚¹ï¼Œè¯·å°†ç›¸åº”çš„æ ‡å¿—ä¼ é€’ç»™[`TrainingArguments`]ï¼š

```py
training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)
```

æˆ–è€…ï¼Œä½¿ç”¨ğŸ¤—Accelerate - åœ¨æœ¬æŒ‡å—çš„è¾ƒè¿œå¤„æŸ¥æ‰¾ğŸ¤—Accelerateç¤ºä¾‹ã€‚

<Tip>

å°½ç®¡æ¸å˜æ£€æŸ¥ç‚¹å¯èƒ½æé«˜å†…å­˜æ•ˆç‡ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šå‡æ…¢çº¦20%ã€‚

</Tip>

## æ··åˆç²¾åº¦è®­ç»ƒ

**æ··åˆç²¾åº¦è®­ç»ƒ**æ˜¯ä¸€ç§é€šè¿‡ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°å€¼æ ¼å¼æ¥ä¼˜åŒ–è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æ•ˆç‡çš„æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œå¤§å¤šæ•°æ¨¡å‹ä½¿ç”¨32ä½æµ®ç‚¹ç²¾åº¦ï¼ˆfp32æˆ–float32ï¼‰æ¥è¡¨ç¤ºå’Œå¤„ç†å˜é‡ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å˜é‡éƒ½éœ€è¦æ­¤é«˜ç²¾åº¦çº§åˆ«æ‰èƒ½è·å¾—å‡†ç¡®çš„ç»“æœã€‚é€šè¿‡å°†æŸäº›å˜é‡çš„ç²¾åº¦é™ä½åˆ°è¾ƒä½çš„æ•°å€¼æ ¼å¼ï¼ˆå¦‚16ä½æµ®ç‚¹æˆ–åŠç²¾åº¦ï¼Œfp16æˆ–float16ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åŠ å¿«è®¡ç®—è¿‡ç¨‹ã€‚ç”±äºåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸€äº›è®¡ç®—ä½¿ç”¨åŠç²¾åº¦è¿›è¡Œï¼Œè€Œä¸€äº›è®¡ç®—ä»ç„¶ä½¿ç”¨å…¨ç²¾åº¦è¿›è¡Œï¼Œæ‰€ä»¥è¯¥æ–¹æ³•è¢«ç§°ä¸ºæ··åˆç²¾åº¦è®­ç»ƒã€‚

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ··åˆç²¾åº¦è®­ç»ƒæ˜¯é€šè¿‡ä½¿ç”¨fp16ï¼ˆ16ä½æµ®ç‚¹æ•°ï¼‰æ•°æ®ç±»å‹æ¥å®ç°çš„ï¼Œä½†æŸäº›GPUæ¶æ„ï¼ˆå¦‚Ampereæ¶æ„ï¼‰æä¾›äº†bf16å’Œtf32ï¼ˆCUDAå†…éƒ¨æ•°æ®ç±»å‹ï¼‰æ•°æ®ç±»å‹ã€‚æŸ¥çœ‹[NVIDIAåšå®¢](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)ä»¥äº†è§£è¿™äº›æ•°æ®ç±»å‹ä¹‹é—´çš„åŒºåˆ«ã€‚

### fp16

æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸»è¦ä¼˜åŠ¿æ¥è‡ªäºåœ¨åŠç²¾åº¦ï¼ˆfp16ï¼‰ä¸­ä¿å­˜æ¿€æ´»å‡½æ•°çš„èƒ½åŠ›ã€‚å°½ç®¡æ¸å˜ä¹Ÿæ˜¯ä»¥åŠç²¾åº¦è®¡ç®—çš„ï¼Œä½†åœ¨ä¼˜åŒ–æ­¥éª¤ä¹‹å‰ä¼šå°†å…¶è½¬æ¢å›å…¨ç²¾åº¦ï¼Œå› æ­¤åœ¨è¿™é‡Œä¸ä¼šèŠ‚çœå†…å­˜ã€‚å°½ç®¡æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œä½†å®ƒå¯èƒ½ä¼šå¯¼è‡´ä½¿ç”¨çš„GPUå†…å­˜å¢åŠ ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒå°çš„æ‰¹æ¬¡å¤§å°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹ç°åœ¨åœ¨GPUä¸ŠåŒæ—¶ä»¥16ä½å’Œ32ä½ç²¾åº¦å­˜åœ¨ï¼ˆåœ¨GPUä¸ŠåŸå§‹æ¨¡å‹çš„1.5å€ï¼‰ã€‚

è¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¯·å°†`fp16`æ ‡å¿—è®¾ç½®ä¸º`True`ï¼š

```py
training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)
```

å¦‚æœä½ æ›´å–œæ¬¢ä½¿ç”¨ğŸ¤—Accelerateï¼Œè¯·åœ¨æœ¬æŒ‡å—çš„è¿›ä¸€æ­¥ä½¿ç”¨[ further in this guide](#using-accelerate)æ‰¾åˆ°ğŸ¤—Accelerateç¤ºä¾‹ã€‚

### BF16

å¦‚æœä½ å¯ä»¥ä½¿ç”¨Ampereæˆ–æ›´æ–°çš„ç¡¬ä»¶ï¼Œå¯ä»¥ä½¿ç”¨bf16è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒå’Œè¯„ä¼°ã€‚å°½ç®¡bf16çš„ç²¾åº¦æ¯”fp16æ›´å·®ï¼Œä½†åŠ¨æ€èŒƒå›´æ›´å¤§ã€‚åœ¨fp16ä¸­ï¼Œä½ å¯ä»¥æ‹¥æœ‰çš„æœ€å¤§æ•°å­—ä¸º`65535`ï¼Œè€Œè¶…è¿‡è¯¥æ•°å­—çš„ä»»ä½•æ•°å­—éƒ½å°†å¯¼è‡´æº¢å‡ºã€‚bf16æ•°å­—å¯ä»¥è¾¾åˆ°`3.39e+38`ï¼ˆï¼ï¼‰ï¼Œä¸fp32å¤§è‡´ç›¸åŒ-å› ä¸ºä¸¤è€…éƒ½ä½¿ç”¨äº†8ä½æ¥è¡¨ç¤ºæ•°å€¼èŒƒå›´ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨ğŸ¤—Trainerä¸­å¯ç”¨BF16ï¼š

```python
training_args = TrainingArguments(bf16=True, **default_args)
```
### TF32

Ampereç¡¬ä»¶ä½¿ç”¨ä¸€ç§è¢«ç§°ä¸ºtf32çš„ç¥å¥‡æ•°æ®ç±»å‹ã€‚å®ƒå…·æœ‰ä¸fp32ç›¸åŒçš„æ•°å­—èŒƒå›´ï¼ˆ8ä½ï¼‰ï¼Œä½†æ˜¯ç²¾åº¦ä¸º23ä½ï¼ˆä¸fp16ç›¸åŒï¼‰è€Œä¸æ˜¯19ä½ (ä¸fp16ç›¸åŒ).ã€‚å®ƒæ˜¯â€œç¥å¥‡â€çš„ï¼Œæ˜¯å› ä¸ºä½ å¯ä»¥ä½¿ç”¨ä¸å¹³å¸¸ä½¿ç”¨çš„fp32è®­ç»ƒå’Œ/æˆ–æ¨ç†ä»£ç ç›¸åŒçš„ä»£ç ï¼Œå¹¶é€šè¿‡å¯ç”¨tf32æ”¯æŒï¼Œå¯ä»¥è·å¾—é«˜è¾¾3å€çš„ååé‡æ”¹è¿›ã€‚éœ€è¦åšçš„å°±æ˜¯åœ¨ä»£ç ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

CUDAå°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä½¿ç”¨tf32è€Œä¸æ˜¯ä½¿ç”¨fp32ï¼ˆå‡è®¾ä½¿ç”¨çš„GPUæ˜¯Ampereç³»åˆ—ï¼‰ã€‚

æ ¹æ®[NVIDIAç ”ç©¶](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)ï¼Œç»å¤§å¤šæ•°æœºå™¨å­¦ä¹ è®­ç»ƒå·¥ä½œè´Ÿè½½ä»¥tf32è®­ç»ƒä¸fp32ç›¸åŒçš„å›°æƒ‘åº¦å’Œæ”¶æ•›ã€‚å¦‚æœä½ å·²ç»ä½¿ç”¨fp16æˆ–bf16æ··åˆç²¾åº¦ï¼Œåˆ™å®ƒä¹Ÿå¯ä»¥æé«˜ååé‡ã€‚

ä½ å¯ä»¥åœ¨ğŸ¤—Trainerä¸­å¯ç”¨æ­¤æ¨¡å¼ï¼š

```python
TrainingArguments(tf32=True, **default_args)
```

<Tip>

æ— æ³•ç›´æ¥é€šè¿‡`tensor.to(dtype=torch.tf32)`è®¿é—®tf32ï¼Œå› ä¸ºå®ƒæ˜¯å†…éƒ¨CUDAæ•°æ®ç±»å‹ã€‚ä½ éœ€è¦`torch >= 1.7`æ‰èƒ½ä½¿ç”¨tf32æ•°æ®ç±»å‹ã€‚

</Tip>

æœ‰å…³tf32ä¸å…¶ä»–ç²¾åº¦çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ä»¥ä¸‹åŸºå‡†æµ‹è¯•ï¼š
[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803)å’Œ
[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189)ã€‚

## ä¼˜åŒ–å™¨é€‰æ‹©

ç”¨äºè®­ç»ƒå˜æ¢å™¨æ¨¡å‹çš„æœ€å¸¸ç”¨ä¼˜åŒ–å™¨æ˜¯Adamæˆ–AdamWï¼ˆå¸¦æœ‰æƒé‡è¡°å‡çš„Adamï¼‰ã€‚Adamé€šè¿‡å­˜å‚¨å…ˆå‰æ¢¯åº¦çš„æ»šåŠ¨å¹³å‡å€¼å®ç°è‰¯å¥½çš„æ”¶æ•›ï¼›ç„¶è€Œï¼Œå®ƒå¢åŠ äº†çº¦æ¨¡å‹å‚æ•°æ•°é‡çš„å†…å­˜å ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ å¯ä»¥ä½¿ç”¨æ›¿ä»£çš„ä¼˜åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å®‰è£…äº†[NVIDIA/apex](https://github.com/NVIDIA/apex)ï¼Œ`adamw_apex_fused`å°†ä¸ºä½ æä¾›æ‰€æœ‰æ”¯æŒçš„AdamWä¼˜åŒ–å™¨ä¸­çš„æœ€å¿«è®­ç»ƒä½“éªŒã€‚

[`Trainer`]é›†æˆäº†å„ç§ä¼˜åŒ–å™¨ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š`adamw_hf`ã€`adamw_torch`ã€`adamw_torch_fused`ã€`adamw_apex_fused`ã€`adamw_anyprecision`ã€`adafactor`æˆ–`adamw_bnb_8bit`ã€‚å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å®ç°æ’å…¥æ›´å¤šä¼˜åŒ–å™¨ã€‚

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°çœ‹ä¸€ä¸‹æ›¿ä»£AdamWä¼˜åŒ–å™¨çš„ä¸¤ä¸ªé€‰æ‹©ï¼š
1.`adafactor` å¯åœ¨[`Trainer`]ä¸­ä½¿ç”¨
2. `adamw_bnb_8bit` åœ¨Trainerä¸­ä¹Ÿå¯ç”¨ï¼Œä½†ä»¥ä¸‹æ˜¯æä¾›çš„ç¬¬ä¸‰æ–¹æ•´åˆã€‚


å¯¹æ¯”è€Œè¨€ï¼Œå¯¹äºä¸€ä¸ª3Bå‚æ•°æ¨¡å‹ï¼Œå¦‚â€œt5-3bâ€ï¼š
* ä¸€ä¸ªæ ‡å‡†çš„AdamWä¼˜åŒ–å™¨éœ€è¦24GBçš„GPUå†…å­˜ï¼Œå› ä¸ºå®ƒå¯¹æ¯ä¸ªå‚æ•°ä½¿ç”¨äº†8å­—èŠ‚ï¼ˆ8*3 => 24GBï¼‰
* Adafactorä¼˜åŒ–å™¨éœ€è¦è¶…è¿‡12GBã€‚å®ƒå¯¹æ¯ä¸ªå‚æ•°ä½¿ç”¨ç•¥å¤šäº4å­—èŠ‚ï¼Œå³4*3ï¼Œè¿˜æœ‰ä¸€äº›é¢å¤–çš„ç©ºé—´ã€‚
* å¦‚æœæ‰€æœ‰ä¼˜åŒ–å™¨çŠ¶æ€éƒ½è¢«é‡åŒ–ï¼Œ8bit BNBé‡åŒ–ä¼˜åŒ–å™¨åªä¼šä½¿ç”¨6GBçš„å†…å­˜ã€‚ï¼ˆ2*3ï¼‰ã€‚

### Adafactor

Adafactorä¸ä¼šä¸ºæ¯ä¸ªæƒé‡çŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ ä¿ç•™æ»šåŠ¨å¹³å‡å€¼ã€‚ç›¸åï¼Œå®ƒä¿ç•™äº†èšåˆä¿¡æ¯ï¼ˆé€è¡Œå’Œé€åˆ—çš„æ»šåŠ¨å¹³å‡å€¼ä¹‹å’Œï¼‰ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†å…¶å ç”¨ç©ºé—´ã€‚ç„¶è€Œï¼Œä¸Adamç›¸æ¯”ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒAdafactorå¯èƒ½æ”¶æ•›è¾ƒæ…¢ã€‚

ä½ å¯ä»¥é€šè¿‡åœ¨[`TrainingArguments`]ä¸­è®¾ç½®`optim="adafactor"`æ¥åˆ‡æ¢åˆ°Adafactorï¼š

```py
training_args = TrainingArguments(per_device_train_batch_size=4, optim="adafactor", **default_args)
```

ç»“åˆå…¶ä»–æ–¹æ³•ï¼ˆæ¢¯åº¦ç§¯ç´¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼‰ï¼Œå¯ä»¥åœ¨ä¿æŒååé‡çš„åŒæ—¶å®ç°æœ€å¤š3å€çš„æ”¹è¿›ï¼ç„¶è€Œï¼Œæ­£å¦‚å‰é¢æåˆ°çš„ï¼ŒAdafactorçš„æ”¶æ•›æ€§å¯èƒ½æ¯”Adamå·®ã€‚

### 8ä½Adam

ä¸Adafactorä¸åŒï¼Œ8ä½Adamä¿ç•™å®Œæ•´çŠ¶æ€å¹¶å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚é‡åŒ–æ„å‘³ç€ä»¥è¾ƒä½çš„ç²¾åº¦å­˜å‚¨çŠ¶æ€ï¼Œå¹¶ä»…åœ¨ä¼˜åŒ–æ—¶è¿›è¡Œè§£é‡åŒ–ã€‚è¿™ç±»ä¼¼äºæ··åˆç²¾åº¦è®­ç»ƒçš„æ€è·¯ã€‚

è¦ä½¿ç”¨`adamw_bnb_8bit`ï¼Œä½ åªéœ€è¦åœ¨[`TrainingArguments`]ä¸­è®¾ç½®`optim="adamw_bnb_8bit"`ï¼š

```py
training_args = TrainingArguments(per_device_train_batch_size=4, optim="adamw_bnb_8bit", **default_args)
```

ç„¶è€Œï¼Œä¸ºäº†ç¤ºèŒƒç›®çš„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ç¬¬ä¸‰æ–¹å®ç°çš„8ä½ä¼˜åŒ–å™¨ï¼Œçœ‹çœ‹å¦‚ä½•é›†æˆè¯¥ä¼˜åŒ–å™¨ã€‚

é¦–å…ˆï¼ŒæŒ‰ç…§GitHub [repo](https://github.com/TimDettmers/bitsandbytes)ä¸­çš„å®‰è£…æŒ‡å—å®‰è£…å®ç°8ä½Adamä¼˜åŒ–å™¨çš„`bitsandbytes`åº“ã€‚

æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚è¿™æ¶‰åŠä¸¤ä¸ªæ­¥éª¤ï¼š
* é¦–å…ˆï¼Œå°†æ¨¡å‹çš„å‚æ•°åˆ†ç»„ä¸ºä¸¤ç»„-ä¸€ç»„åº”ç”¨æƒé‡è¡°å‡ï¼Œå¦ä¸€ç»„ä¸åº”ç”¨æƒé‡è¡°å‡ã€‚é€šå¸¸ï¼Œåå·®å’Œå±‚è§„èŒƒåŒ–å‚æ•°ä¸ä¼šåº”ç”¨æƒé‡è¡°å‡ã€‚
* ç„¶åè¿›è¡Œä¸€äº›å‚æ•°å¤„ç†ï¼Œä»¥ä½¿ç”¨å…ˆå‰ä½¿ç”¨çš„AdamWä¼˜åŒ–å™¨ç›¸åŒçš„å‚æ•°ã€‚

```py
import bitsandbytes as bnb
from torch import nn
from transformers.trainer_pt_utils import get_parameter_names

training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)

decay_parameters = get_parameter_names(model, [nn.LayerNorm])
decay_parameters = [name for name in decay_parameters if "bias" not in name]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in model.named_parameters() if n in decay_parameters],
        "weight_decay": training_args.weight_decay,
    },
    {
        "params": [p for n, p in model.named_parameters() if n not in decay_parameters],
        "weight_decay": 0.0,
    },
]

optimizer_kwargs = {
    "betas": (training_args.adam_beta1, training_args.adam_beta2),
    "eps": training_args.adam_epsilon,
}
optimizer_kwargs["lr"] = training_args.learning_rate
adam_bnb_optim = bnb.optim.Adam8bit(
    optimizer_grouped_parameters,
    betas=(training_args.adam_beta1, training_args.adam_beta2),
    eps=training_args.adam_epsilon,
    lr=training_args.learning_rate,
)
```

æœ€åï¼Œå°†è‡ªå®šä¹‰ä¼˜åŒ–å™¨ä½œä¸ºå‚æ•°ä¼ é€’ç»™`Trainer`ï¼š

```py
trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))
```

ç»“åˆå…¶ä»–æ–¹æ³•ï¼ˆæ¢¯åº¦ç§¯ç´¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼‰ï¼Œä½ å¯ä»¥æœŸæœ›è·å¾—çº¦3å€çš„å†…å­˜æ”¹è¿›ï¼Œç”šè‡³æ¯”ä½¿ç”¨Adafactoræ—¶å…·æœ‰ç¨é«˜çš„ååé‡ã€‚

### multi_tensor

pytorch-nightlyå¼•å…¥äº†`torch.optim._multi_tensor`ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«å¤§é‡å°ç‰¹å¾å¼ é‡çš„ä¼˜åŒ–å™¨é€Ÿåº¦ã€‚å®ƒæœ€ç»ˆå°†æˆä¸ºé»˜è®¤é€‰é¡¹ï¼Œä½†å¦‚æœä½ æƒ³æå‰å°è¯•å®ƒï¼Œè¯·æŸ¥çœ‹æ­¤GitHub [issue](https://github.com/huggingface/transformers/issues/9965)ã€‚

## æ•°æ®é¢„åŠ è½½

å®ç°å‡ºè‰²çš„è®­ç»ƒé€Ÿåº¦çš„ä¸€ä¸ªé‡è¦è¦æ±‚æ˜¯èƒ½å¤Ÿä»¥GPUèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é€Ÿåº¦é¦ˆé€æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ“ä½œéƒ½åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼Œå¯èƒ½æ— æ³•å¿«é€Ÿä»ç£ç›˜è¯»å–æ•°æ®ï¼Œä»è€Œé€ æˆç“¶é¢ˆï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä¸é«˜ã€‚é€šè¿‡é…ç½®ä»¥ä¸‹å‚æ•°æ¥å‡å°‘ç“¶é¢ˆï¼š

- `DataLoader(pin_memory=True, ...)` - ç¡®ä¿æ•°æ®é¢„åŠ è½½åˆ°CPUä¸Šçš„å›ºå®šå†…å­˜ä¸­ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä»CPUåˆ°GPUå†…å­˜çš„ä¼ è¾“é€Ÿåº¦å¤§å¤§æé«˜ã€‚
- `DataLoader(num_workers=4, ...)` - æ´¾ç”Ÿå¤šä¸ªå·¥ä½œçº¿ç¨‹ä»¥æ›´å¿«åœ°é¢„åŠ è½½æ•°æ®ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè§‚å¯ŸGPUåˆ©ç”¨ç‡ç»Ÿè®¡æ•°æ®ï¼›å¦‚æœç¦»100ï¼…æœ‰ä¸€å®šå·®è·ï¼Œè¯·å°è¯•å¢åŠ å·¥ä½œçº¿ç¨‹æ•°ã€‚å½“ç„¶ï¼Œé—®é¢˜å¯èƒ½ä¸åœ¨è¿™é‡Œï¼Œå› æ­¤å¢åŠ å·¥ä½œçº¿ç¨‹æ•°ä¸ä¸€å®šä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚

ä½¿ç”¨[`Trainer`]æ—¶ï¼Œå¯¹åº”çš„[`TrainingArguments`]å‚æ•°æ˜¯ï¼š`dataloader_pin_memory`ï¼ˆé»˜è®¤ä¸º`True`ï¼‰å’Œ`dataloader_num_workers`ï¼ˆé»˜è®¤ä¸º`0`ï¼‰ã€‚

## DeepSpeed ZeRO

DeepSpeedæ˜¯ä¸€ä¸ªä¸ğŸ¤—Transformerså’ŒğŸ¤—Accelerateé›†æˆçš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—åŠŸèƒ½å’Œä¼˜åŒ–ï¼Œæ—¨åœ¨æ”¹è¿›å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

å¦‚æœä½ çš„æ¨¡å‹é€‚åˆäºå•ä¸ªGPUå¹¶ä¸”æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥æ”¾ç½®è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼Œåˆ™ä¸éœ€è¦ä½¿ç”¨DeepSpeedï¼Œå› ä¸ºå®ƒåªä¼šä½¿äº‹æƒ…å˜æ…¢ã€‚ç„¶è€Œï¼Œå¦‚æœæ¨¡å‹æ— æ³•é€‚åº”å•ä¸ªGPUï¼Œæˆ–è€…æ— æ³•æ”¾ç½®è¾ƒå°çš„æ‰¹æ¬¡ï¼Œåˆ™å¯ä»¥åˆ©ç”¨DeepSpeedçš„ZeRO + CPU Offloadæˆ–NVMe Offloadæ¥å¤„ç†æ›´å¤§çš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦å•ç‹¬[å®‰è£…åº“](main_classes/deepspeed#installation)ï¼Œç„¶åéµå¾ªä¸€ä¸ªé…ç½®æ–‡ä»¶å¹¶å¯åŠ¨DeepSpeedçš„æŒ‡å—ï¼š

* å¯¹äºDeepSpeedä¸[`Trainer`]çš„å®Œæ•´æŒ‡å—ï¼Œè¯·æŸ¥é˜…[ç›¸åº”çš„æ–‡æ¡£](main_classes/deepspeed) ï¼Œç‰¹åˆ«æ˜¯[å•ä¸ªGPUçš„éƒ¨ç½²éƒ¨åˆ†](main_classes/deepspeed#deployment-with-one-gpu)ã€‚è¦åœ¨ç¬”è®°æœ¬ä¸­ä½¿ç”¨DeepSpeedï¼Œéœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ï¼›è¯·æŸ¥é˜…[å¯¹åº”æŒ‡å—](main_classes/deepspeed#deployment-in-notebooks)ã€‚
* å¦‚æœä½ æ›´å–œæ¬¢ä½¿ç”¨ğŸ¤—Accelerateï¼Œè¯·å‚è€ƒ[ğŸ¤—Accelerate DeepSpeedæŒ‡å—](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)ã€‚

## ä½¿ç”¨torch.compile

PyTorch 2.0å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–è¯‘å‡½æ•°ï¼Œå®ƒä¸éœ€è¦å¯¹ç°æœ‰çš„PyTorchä»£ç è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œä½†å¯ä»¥é€šè¿‡æ·»åŠ ä¸€è¡Œä»£ç æ¥ä¼˜åŒ–ä½ çš„ä»£ç ï¼š`model = torch.compile(model)`ã€‚

å¦‚æœä½¿ç”¨[`Trainer`]ï¼Œä½ åªéœ€è¦åœ¨[`TrainingArguments`]ä¸­ä¼ é€’`torch_compile`é€‰é¡¹ï¼š

```python
training_args = TrainingArguments(torch_compile=True, **default_args)
```

`torch.compile`ä½¿ç”¨Pythonçš„å¸§è¯„ä¼°APIæ¥è‡ªåŠ¨ä»ç°æœ‰çš„PyTorchç¨‹åºä¸­åˆ›å»ºå›¾å½¢ã€‚åœ¨æ•è·å›¾å½¢ä¹‹åï¼Œå¯ä»¥éƒ¨ç½²ä¸åŒçš„åç«¯å°†å›¾å½¢é™åˆ°ä¼˜åŒ–å¼•æ“ã€‚ä½ å¯ä»¥åœ¨[PyTorchæ–‡æ¡£](https://pytorch.org/get-started/pytorch-2.0/)ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†æµ‹è¯•ã€‚

`torch.compile`å…·æœ‰ä¸æ–­å¢é•¿çš„åç«¯åˆ—è¡¨ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`torchdynamo.list_backends()`æ‰¾åˆ°ã€‚æ¯ä¸ªåç«¯éƒ½æœ‰å…¶å¯é€‰çš„ä¾èµ–é¡¹ã€‚

é€šè¿‡åœ¨[`TrainingArguments`]ä¸­æŒ‡å®šè¦ä½¿ç”¨çš„åç«¯çš„æ–¹å¼é€‰æ‹©è¦ä½¿ç”¨çš„åç«¯ã€‚æœ€å¸¸ç”¨çš„å‡ ä¸ªåç«¯æ˜¯ï¼š

**è°ƒè¯•åç«¯**ï¼š
* `dynamo.optimize("eager")` - ä½¿ç”¨PyTorchè¿è¡Œæå–çš„GraphModuleï¼Œè¿™å¯¹äºè°ƒè¯•TorchDynamoé—®é¢˜éå¸¸æœ‰ç”¨ã€‚
* `dynamo.optimize("aot_eager")` - ä½¿ç”¨AotAutogradè€Œæ²¡æœ‰ç¼–è¯‘å™¨çš„AotAutograd'sæå–çš„å‰å‘å’Œåå‘å›¾çš„PyTorch eagerè¿è¡Œã€‚è¿™å¯¹äºè°ƒè¯•éå¸¸æœ‰ç”¨ï¼Œä¸å¤ªå¯èƒ½å¸¦æ¥é€Ÿåº¦æå‡ã€‚

**è®­ç»ƒå’Œæ¨æ–­åç«¯**ï¼š
* `dynamo.optimize("inductor")` - ä½¿ç”¨å…·æœ‰AotAutogradå’Œcudagraphsçš„TorchInductoråç«¯ï¼Œé€šè¿‡åˆ©ç”¨codegened Tritonå†…æ ¸å¹³è¡¡åœ°è®­ç»ƒæ¯ä¸ªä¸“å®¶çš„é—¨æ§å‡½æ•°æ¥è¿›è¡Œè®­ç»ƒã€‚[äº†è§£æ›´å¤š](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)
* `dynamo.optimize("nvfuser")` - ä½¿ç”¨TorchScriptçš„nvFuserã€‚[äº†è§£æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)
* `dynamo.optimize("aot_nvfuser")` - ä½¿ç”¨AotAutogradçš„nvFuserã€‚[äº†è§£æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)
* `dynamo.optimize("aot_cudagraphs")` - ä½¿ç”¨AotAutogradçš„cudagraphsã€‚[äº†è§£æ›´å¤š](https://github.com/pytorch/torchdynamo/pull/757)

**ä»…æ¨æ–­åç«¯**ï¼š
* `dynamo.optimize("ofi")` - ä½¿ç”¨Torchscriptçš„optimize_for_inferenceã€‚[äº†è§£æ›´å¤š](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)
* `dynamo.optimize("fx2trt")` - ä½¿ç”¨Nvidia TensorRTè¿›è¡Œæ¨æ–­ä¼˜åŒ–ã€‚[äº†è§£æ›´å¤š](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)
* `dynamo.optimize("onnxrt")` - ä½¿ç”¨ONNXRTè¿›è¡ŒCPU/GPUä¸Šçš„æ¨æ–­ã€‚[äº†è§£æ›´å¤š](https://onnxruntime.ai/)
* `dynamo.optimize("ipex")` - ä½¿ç”¨IPEXè¿›è¡ŒCPUä¸Šçš„æ¨æ–­ã€‚[äº†è§£æ›´å¤š](https://github.com/intel/intel-extension-for-pytorch)

è¦ä½¿ç”¨`torch.compile`ä¸ğŸ¤—Transformersçš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹æœ¬æ–‡æ¡£ä¸­å…³äºä½¿ç”¨æœ€æ–°çš„PyTorch 2.0åŠŸèƒ½[Fine-tuning a BERT model for Text Classification using the newest PyTorch 2.0 features]çš„[åšå®¢æ–‡ç« ](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)ã€‚

## ä½¿ç”¨ğŸ¤—Accelerate

é€šè¿‡[ğŸ¤—Accelerate](https://huggingface.co/docs/accelerate/index)ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸Šæ–¹æ³•ï¼Œå¹¶å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ï¼Œå®è´¨ä¸Šå¯ä»¥ä½¿ç”¨çº¯ç²¹çš„PyTorchç¼–å†™å¾ªç¯ï¼Œåªéœ€è¿›è¡Œä¸€äº›ç»†å¾®çš„ä¿®æ”¹ã€‚

å‡è®¾ä½ å·²ç»å°†[`TrainingArguments`]ä¸­çš„æ–¹æ³•ç»„åˆå¦‚ä¸‹ï¼š

```py
training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)
```

ä½¿ç”¨ğŸ¤—Accelerateçš„å®Œæ•´ç¤ºä¾‹è®­ç»ƒå¾ªç¯åªæœ‰å‡ è¡Œä»£ç ï¼š

```py
from accelerate import Accelerator
from torch.utils.data.dataloader import DataLoader

dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

if training_args.gradient_checkpointing:
    model.gradient_checkpointing_enable()

accelerator = Accelerator(fp16=training_args.fp16)
model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

model.train()
for step, batch in enumerate(dataloader, start=1):
    loss = model(**batch).loss
    loss = loss / training_args.gradient_accumulation_steps
    accelerator.backward(loss)
    if step % training_args.gradient_accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åŒ…è£…åœ¨[`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨æ¨¡å‹çš„[`~PreTrainedModel.gradient_checkpointing_enable`]æ–¹æ³•æ¥å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚åœ¨åˆå§‹åŒ–[`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator)æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶ä¸”å®ƒå°†åœ¨[`prepare`]è°ƒç”¨ä¸­ä¸ºæˆ‘ä»¬å¤„ç†ã€‚åœ¨[`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare)è°ƒç”¨æœŸé—´ï¼Œæ•°æ®åŠ è½½å™¨ä¹Ÿå°†åœ¨ä½¿ç”¨å¤šä¸ªGPUæ—¶åˆ†å¸ƒåœ¨å·¥ä½œè¿›ç¨‹ä¸­ã€‚æˆ‘ä»¬ä»ä¹‹å‰ç¤ºä¾‹ä¸­ä½¿ç”¨ç›¸åŒçš„[8ä½ä¼˜åŒ–å™¨](#8-bit-adam)ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸»è¦çš„è®­ç»ƒå¾ªç¯ã€‚è¯·æ³¨æ„ï¼Œ`backward`è°ƒç”¨æ˜¯ç”±ğŸ¤—Accelerateå¤„ç†çš„ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°æ¢¯åº¦ç´¯ç§¯çš„å·¥ä½œåŸç†ï¼šæˆ‘ä»¬å°†æŸå¤±å½’ä¸€åŒ–ï¼Œå› æ­¤åœ¨ç´¯ç§¯ç»“æŸæ—¶å¾—åˆ°å¹³å‡å€¼ï¼Œå¹¶ä¸”ä¸€æ—¦æˆ‘ä»¬è¿›è¡Œè¶³å¤Ÿçš„æ­¥éª¤ï¼Œå°±è¿›è¡Œä¼˜åŒ–ã€‚

åœ¨ğŸ¤—Accelerateä¸­ï¼Œé€šè¿‡å°‘é‡çš„ä»£ç å³å¯å®ç°è¿™äº›ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶ä¸”å…·æœ‰æ›´çµæ´»çš„è®­ç»ƒå¾ªç¯ã€‚è¦äº†è§£æ‰€æœ‰åŠŸèƒ½çš„å®Œæ•´æ–‡æ¡£ï¼Œè¯·æŸ¥çœ‹[Accelerateæ–‡æ¡£](https://huggingface.co/docs/accelerate/index)ã€‚

## é«˜æ•ˆçš„è½¯ä»¶é¢„æ„å»º

PyTorchçš„[pipå’Œcondaæ„å»º](https://pytorch.org/get-started/locally/#start-locally)å·²ç»é¢„å…ˆæ„å»ºäº†cuda toolkitï¼Œè¿™è¶³ä»¥è¿è¡ŒPyTorchï¼Œä½†å¦‚æœä½ éœ€è¦æ„å»ºcudaæ‰©å±•ï¼Œåˆ™ä¸è¶³å¤Ÿã€‚

æœ‰æ—¶å€™å¯èƒ½éœ€è¦é¢å¤–çš„åŠªåŠ›æ¥é¢„å…ˆæ„å»ºæŸäº›ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸é¢„å…ˆç¼–è¯‘çš„åº“ï¼ˆå¦‚`apex`ï¼‰ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„åŠªåŠ›ã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°å¦‚ä½•åœ¨ç³»ç»ŸèŒƒå›´å†…å®‰è£…æ­£ç¡®çš„cuda toolkitå¯èƒ½å¾ˆå¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æƒ…å†µï¼ŒPyTorchå’ŒNVIDIAå‘å¸ƒäº†æ–°ç‰ˆæœ¬çš„NGC Dockerå®¹å™¨ï¼Œå…¶ä¸­å·²ç»é¢„å…ˆæ„å»ºäº†æ‰€æœ‰å†…å®¹ã€‚ä½ åªéœ€å°†ç¨‹åºå®‰è£…åœ¨å…¶ä¸­ï¼Œå®ƒå°±å¯ä»¥ç›´æ¥è¿è¡Œã€‚

å¦‚æœä½ æƒ³è°ƒæ•´pytorchæºä»£ç å’Œ/æˆ–è¿›è¡Œæ–°çš„å®šåˆ¶æ„å»ºï¼Œè¿™ç§æ–¹æ³•ä¹Ÿå¾ˆæœ‰ç”¨ã€‚
è¦æ‰¾åˆ°æ‰€éœ€çš„Dockeræ˜ åƒç‰ˆæœ¬ï¼Œè¯·æŸ¥çœ‹[PyTorchå‘å¸ƒè¯´æ˜](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/)ï¼Œé€‰æ‹©æœ€æ–°çš„æœˆåº¦ç‰ˆæœ¬ä¹‹ä¸€ã€‚è¿›å…¥æ‰€éœ€ç‰ˆæœ¬çš„å‘å¸ƒè¯´æ˜ï¼Œæ£€æŸ¥ç¯å¢ƒç»„ä»¶æ˜¯å¦ç¬¦åˆä½ çš„éœ€æ±‚ï¼ˆåŒ…æ‹¬NVIDIAé©±åŠ¨ç¨‹åºè¦æ±‚ï¼ï¼‰ï¼Œç„¶ååœ¨è¯¥æ–‡æ¡£çš„é¡¶éƒ¨è½¬åˆ°ç›¸åº”çš„NGCé¡µé¢ã€‚å¦‚æœå› æŸç§åŸå› è¿·å¤±æ–¹å‘ï¼Œè¯·æŸ¥çœ‹[æ‰€æœ‰PyTorch NGCå›¾åƒçš„ç´¢å¼•](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch)ã€‚

æ¥ä¸‹æ¥ï¼Œè¯·æŒ‰ç…§ä¸‹è½½å’Œéƒ¨ç½²Dockeræ˜ åƒçš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚

## ä¸“å®¶ç»„åˆ

ä¸€äº›æœ€è¿‘çš„è®ºæ–‡æŠ¥é“ï¼Œç»“åˆä¸“å®¶ç»„åˆï¼ˆMixture of Expertsï¼ŒMoEï¼‰æŠ€æœ¯å°†Transformeræ¨¡å‹ä¸­çš„è®­ç»ƒé€Ÿåº¦æé«˜äº†4-5å€ï¼Œå¹¶å®ç°äº†æ›´å¿«çš„æ¨æ–­ã€‚

ç”±äºå‘ç°æ›´å¤šçš„å‚æ•°å¯ä»¥å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼Œæ­¤æŠ€æœ¯å…è®¸å°†å‚æ•°æ•°é‡æé«˜ä¸€ä¸ªæ•°é‡çº§ï¼Œè€Œä¸å¢åŠ è®­ç»ƒæˆæœ¬ã€‚

åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ¯ä¸ªFFNå±‚è¢«ä¸€ä¸ªMoEå±‚å–ä»£ï¼Œè¯¥MoEå±‚ç”±è®¸å¤šä¸“å®¶ç»„æˆï¼Œå…·æœ‰æ ¹æ®è¾“å…¥æ ‡è®°åœ¨åºåˆ—ä¸­çš„ä½ç½®å¹³è¡¡è®­ç»ƒçš„é—¨æ§å‡½æ•°ã€‚

![MoE Transformer 2x block](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png)

ï¼ˆæ¥æºï¼š[GLAM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)ï¼‰

å¯ä»¥åœ¨ä¸‹é¢åˆ—å‡ºçš„è®ºæ–‡ä¸­æ‰¾åˆ°è¯¦ç»†çš„ä¿¡æ¯å’Œæ¯”è¾ƒè¡¨æ ¼ï¼š

- ["Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"](https://arxiv.org/abs/2101.03961) by Ben-Zaken et al.
- ["GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"](https://arxiv.org/abs/2006.16668) by Fedus et al.
- ["MixNMatch: Training a Convolutional Neural Network with a Switchable Mixture of Experts"](https://arxiv.org/abs/2002.03598) by Aghajanyan et al.
- ["Scalable Mixture Models for Deep Learning"](https://arxiv.org/abs/2010.09161) by Chen et al.

ä»¥ä¸Šæ˜¯ç¿»è¯‘ç»“æœï¼Œä»…ä¾›å‚è€ƒã€‚

è¿™ç§æ–¹æ³•çš„ä¸»è¦ç¼ºç‚¹æ˜¯éœ€è¦å¤§é‡çš„GPUå†…å­˜ï¼Œå‡ ä¹æ¯”å…¶å¯†é›†ç­‰æ•ˆæ¨¡å‹å¤šä¸€ä¸ªæ•°é‡çº§ã€‚æœ‰å¤šç§è’¸é¦å’Œæ–¹æ³•è¢«æå‡ºæ¥è§£å†³è¿™ç§æ›´é«˜çš„å†…å­˜éœ€æ±‚ã€‚

ç„¶è€Œï¼Œå­˜åœ¨ç›´æ¥çš„æƒè¡¡ï¼Œä½ å¯ä»¥ä½¿ç”¨å°‘é‡å…·æœ‰2-3å€è¾ƒå°åŸºç¡€æ¨¡å‹çš„ä¸“å®¶ï¼Œè€Œä¸æ˜¯æ•°åæˆ–æ•°ç™¾ä¸ªä¸“å®¶ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ª5å€è¾ƒå°çš„æ¨¡å‹ï¼Œé€‚åº¦æé«˜è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶é€‚åº¦æé«˜å†…å­˜éœ€æ±‚ã€‚

å¤§å¤šæ•°ç›¸å…³è®ºæ–‡å’Œå®ç°éƒ½æ˜¯åŸºäºTensorflow/TPUsçš„ï¼š

- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
- [GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)

å¯¹äºPyTorchï¼ŒDeepSpeedä¹Ÿæ„å»ºäº†ä¸€ä¸ªï¼š[DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://arxiv.org/abs/2201.05596)ï¼Œ[Mixture of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - åšå®¢æ–‡ç« ï¼š[1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)ï¼Œ[2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/)ä»¥åŠå…·æœ‰å¤§å‹åŸºäºtransformerçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„ç‰¹å®šéƒ¨ç½²ï¼š[åšå®¢æ–‡ç« ](https://www.deepspeed.ai/news/2021/12/09/deepspeed-moe-nlg.html)ï¼Œ[Megatron-Deepspeedåˆ†æ”¯](Thttps://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training)ã€‚

## ä½¿ç”¨PyTorchåŸç”Ÿæ³¨æ„åŠ›å’ŒFlash Attention

PyTorch 2.0å‘å¸ƒäº†ä¸€ä¸ªåŸç”Ÿçš„[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA)ï¼Œ
å®ƒå…è®¸ä½¿ç”¨èåˆçš„GPUå†…æ ¸ï¼Œä¾‹å¦‚[å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›](https://arxiv.org/abs/2112.05682)å’Œ[é—ªå­˜æ³¨æ„åŠ›](https://arxiv.org/abs/2205.14135)ã€‚

åœ¨å®‰è£…äº†[`optimum`](https://github.com/huggingface/optimum)è½¯ä»¶åŒ…ä¹‹åï¼Œå¯ä»¥æ›¿æ¢ç›¸å…³çš„å†…éƒ¨æ¨¡å—ä»¥ä½¿ç”¨PyTorchçš„åŸç”Ÿæ³¨æ„åŠ›ï¼š

```python
model = model.to_bettertransformer()
```

ä¸€æ—¦è½¬æ¢å®Œæˆï¼Œå¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®­ç»ƒæ¨¡å‹ã€‚

<Tip warning={true}>

å¦‚æœæ²¡æœ‰æä¾›`attention_mask`ï¼ŒPyTorchåŸç”Ÿçš„`scaled_dot_product_attention`è¿ç®—ç¬¦åªèƒ½è°ƒåº¦åˆ°Flash Attentionã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼ŒBetterTransformeré›†æˆä¼š**æ”¾å¼ƒå¯¹æ©ç çš„æ”¯æŒï¼Œå¹¶ä¸”åªèƒ½ç”¨äºä¸éœ€è¦æ‰¹é‡è®­ç»ƒçš„å¡«å……æ©ç çš„è®­ç»ƒ**ã€‚ä¾‹å¦‚ï¼Œè¿™é€‚ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡æˆ–å› æœè¯­è¨€å»ºæ¨¡ã€‚BetterTransformerä¸é€‚åˆåœ¨éœ€è¦å¡«å……æ©ç çš„ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ã€‚

</Tip>

é˜…è¯»è¿™ç¯‡[åšæ–‡](https://pytorch.org/blog/out-of-the-box-acceleration/)ï¼Œäº†è§£æ›´å¤šå…³äºSDPAçš„åŠ é€Ÿå’ŒèŠ‚çœå†…å­˜çš„ä¿¡æ¯ã€‚