<!--ç‰ˆæƒæ‰€æœ‰2023 HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›å¦‚æœä¸ç¬¦åˆè®¸å¯è¯ï¼Œæ‚¨ä¸èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€æä¾›çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–éšå«çš„ä¿è¯æˆ–æ¡ä»¶ã€‚æœ‰å…³è®¸å¯çš„ç‰¹å®šè¯­è¨€å’Œé™åˆ¶ï¼Œè¯·å‚é˜…è®¸å¯ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä»¥Markdownæ ¼å¼ç¼–å†™ï¼Œä½†åŒ…å«ç”¨äºæˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äºMDXçš„è¯­æ³•ï¼‰ï¼Œè¿™å¯èƒ½åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚-->

# è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

[[open-in-colab]]

<Youtube id = "TksaY_FDgnk" />

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå°†ä¸€ç³»åˆ—éŸ³é¢‘è¾“å…¥æ˜ å°„åˆ°æ–‡æœ¬è¾“å‡ºã€‚è™šæ‹ŸåŠ©æ‰‹ï¼ˆå¦‚Siriå’ŒAlexaï¼‰ä½¿ç”¨ASRæ¨¡å‹æ¥å¸®åŠ©ç”¨æˆ·æ—¥å¸¸ä½¿ç”¨ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–æœ‰ç”¨çš„ç”¨æˆ·ç•Œé¢åº”ç”¨ï¼Œä¾‹å¦‚å®æ—¶å­—å¹•å’Œä¼šè®®è®°å½•ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ä¸Šå¾®è°ƒ[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)ï¼Œå°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>
æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡æ”¯æŒä»¥ä¸‹æ¨¡å‹æ¶æ„:

[Data2VecAudio](../model_doc/data2vec-audio)ï¼Œ[Hubert](../model_doc/hubert)ï¼Œ[M-CTC-T](../model_doc/mctct)ï¼Œ[SEW](../model_doc/sew)ï¼Œ[SEW-D](../model_doc/sew-d)ï¼Œ[UniSpeech](../model_doc/unispeech)ï¼Œ[UniSpeechSat](../model_doc/unispeech-sat)ï¼Œ[Wav2Vec2](../model_doc/wav2vec2)ï¼Œ[Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer)ï¼Œ[WavLM](../model_doc/wavlm)

<!--The tip is automatically generated by `make fix-copies`, do not fill manually!-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š

```bash
pip install transformers datasets evaluate jiwer
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºå…±äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½MInDS-14æ•°æ®é›†

é¦–å…ˆåŠ è½½ğŸ¤— Datasetsåº“ä¸­[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ªå®éªŒå’Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸çš„æœºä¼šï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

ä½¿ç”¨[`~Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†çš„â€œtrainâ€åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

æ¥ä¸‹æ¥æŸ¥çœ‹æ•°æ®é›†ï¼š

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

è™½ç„¶æ•°æ®é›†åŒ…å«äº†å¾ˆå¤šæœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¦‚â€œlang_idâ€å’Œâ€œenglish_transcriptionâ€ï¼Œä½†æ‚¨åœ¨æœ¬æŒ‡å—ä¸­å°†é‡ç‚¹å…³æ³¨â€œaudioâ€å’Œâ€œtranscriptionâ€ã€‚ä½¿ç”¨[`~datasets.Dataset.remove_columns`]æ–¹æ³•åˆ é™¤å…¶ä»–åˆ—ï¼š

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

å†æ¬¡æŸ¥çœ‹ç¤ºä¾‹ï¼š

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

æœ‰ä¸¤ä¸ªå­—æ®µï¼š

- `audio`ï¼šä¸€ç»´`array`å½¢å¼çš„è¯­éŸ³ä¿¡å·ï¼Œå¿…é¡»è°ƒç”¨å®ƒæ¥åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
- `transcription`ï¼šç›®æ ‡æ–‡æœ¬ã€‚

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ªWav2Vec2å¤„ç†å™¨æ¥å¤„ç†éŸ³é¢‘ä¿¡å·ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸º8000kHzï¼ˆæ‚¨å¯ä»¥åœ¨å…¶[æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/PolyAI/minds14)ä¸­æ‰¾åˆ°è¿™äº›ä¿¡æ¯ï¼‰ï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦å°†æ•°æ®é›†é‡æ–°é‡‡æ ·ä¸º16000kHzä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹ï¼š

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œåœ¨ä¸Šé¢çš„`transcription`ä¸­ï¼Œæ–‡æœ¬åŒ…å«ä¸€ç³»åˆ—å¤§å†™å’Œå°å†™å­—ç¬¦ã€‚Wav2Vec2 tokenizeråªæ˜¯åœ¨å¤§å†™å­—ç¬¦ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥æ‚¨éœ€è¦ç¡®ä¿æ–‡æœ¬ä¸tokenizerçš„è¯æ±‡è¡¨åŒ¹é…ï¼š

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

ç°åœ¨åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°ï¼š

1. è°ƒç”¨`audio`åˆ—æ¥åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
2. ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå–`input_values`å¹¶ä½¿ç”¨processorå¯¹`transcription`åˆ—è¿›è¡Œæ ‡è®°ã€‚

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasets [`~datasets.Dataset.map`]å‡½æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡å¢åŠ `num_proc`å‚æ•°æ¥åŠ é€Ÿ`map`ã€‚ä½¿ç”¨[`~datasets.Dataset.remove_columns`]æ–¹æ³•åˆ é™¤æ‚¨ä¸éœ€è¦çš„åˆ—ï¼š

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ğŸ¤—Transformersæ²¡æœ‰é€‚ç”¨äºASRçš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æ‚¨éœ€è¦é€‚åº”[`DataCollatorWithPadding`]æ¥åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚å®ƒè¿˜å°†åŠ¨æ€åœ°å°†æ–‡æœ¬å’Œæ ‡ç­¾å¡«å……åˆ°å…¶æ‰¹æ¬¡ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„é•¿åº¦ï¼‰ï¼Œä»¥ç¡®ä¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚å°½ç®¡å¯ä»¥é€šè¿‡è®¾ç½®`padding=True`åœ¨`tokenizer`å‡½æ•°ä¸­å¡«å……æ–‡æœ¬ï¼Œä½†åŠ¨æ€å¡«å……æ›´é«˜æ•ˆã€‚

ä¸å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œæ­¤ç‰¹å®šæ•°æ®æ•´ç†å™¨éœ€è¦å¯¹`input_values`å’Œ`labels`åº”ç”¨ä¸åŒçš„å¡«å……æ–¹æ³•ï¼š

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         # split inputs and labels since they have to be of different lengths and need
...         # different padding methods
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # replace padding with -100 to ignore loss correctly
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

ç°åœ¨å®ä¾‹åŒ–`DataCollatorForCTCWithPadding`ï¼š

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½[å•è¯é”™è¯¯ç‡](https://huggingface.co/spaces/evaluate-metric/wer)ï¼ˆWERï¼‰åº¦é‡ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡ï¼‰ï¼š

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™[`~evaluate.EvaluationModule.compute`]è®¡ç®—WERï¼š

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

ç°åœ¨ï¼Œæ‚¨çš„`compute_metrics`å‡½æ•°å·²å‡†å¤‡å¥½äº†ï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶å°†è¿”å›åˆ°å®ƒã€‚

## è®­ç»ƒ

<frameworkcontent>
<pt>
<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„](../training.md#train-with-pytorch-trainer)çš„åŸºæœ¬æ•™ç¨‹ï¼

</Tip>

ç°åœ¨ï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒäº†ï¼ä½¿ç”¨[`AutoModelForCTC`]åŠ è½½Wav2Vec2æ¨¡å‹ã€‚ä½¿ç”¨`ctc_loss_reduction`å‚æ•°æŒ‡å®šè¦åº”ç”¨çš„ç¼©å‡ã€‚é€šå¸¸æœ€å¥½ä½¿ç”¨å¹³å‡è€Œä¸æ˜¯é»˜è®¤çš„æ±‚å’Œï¼š

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
... )
```

ç°åœ¨åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨[`TrainingArguments`]ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯`output_dir`ï¼Œå®ƒæŒ‡å®šè¦ä¿å­˜æ‚¨çš„æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†è¯¥æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•åˆ°HuggingFaceä»¥ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶ï¼Œ[`Trainer`]å°†è¯„ä¼°WERå¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[`Trainer`]ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€æ ‡è®°å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
3. è°ƒç”¨[`~Trainer.train`]æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     evaluation_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=processor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

å®Œæˆè®­ç»ƒåï¼Œä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€‚

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<Tip>

æœ‰å…³å¦‚ä½•å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·å‚é˜…æœ¬åšå®¢[æ–‡ç« ](https://huggingface.co/blog/fine-tune-wav2vec2-english)æ¥äº†è§£è‹±è¯­ASRï¼Œå’Œè¿™ä¸ª[å¸–å­](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)æ¥äº†è§£å¤šè¯­è¨€ASRã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œæ—¢ç„¶æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œç°åœ¨å¯ä»¥å°†å…¶ç”¨äºæ¨æ–­ï¼

åŠ è½½è¦åœ¨å…¶ä¸Šè¿è¡Œæ¨æ–­çš„éŸ³é¢‘æ–‡ä»¶ã€‚è®°å¾—å°†éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡é‡æ–°é‡‡æ ·ä»¥åŒ¹é…æ¨¡å‹çš„é‡‡æ ·ç‡ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

å°è¯•åœ¨æ¨æ–­ä¸­ä½¿ç”¨å¾®è°ƒçš„æ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[`pipeline`]ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«`pipeline`ï¼Œå°†éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

<Tip>

è¿™æ¬¡è½¬å½•è¿˜å¯ä»¥ï¼Œä½†å¯èƒ½è¿˜æœ‰è¿›ä¸€æ­¥æ”¹è¿›çš„ç©ºé—´ï¼å°è¯•åœ¨æ›´å¤šçš„ç¤ºä¾‹ä¸Šå¾®è°ƒæ‚¨çš„æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼

</Tip>

å¦‚æœéœ€è¦ï¼Œæ‚¨è¿˜å¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š

<frameworkcontent>
<pt>
åŠ è½½å¤„ç†å™¨ä»¥å¤„ç†éŸ³é¢‘æ–‡ä»¶å’Œè½¬å½•ï¼Œå¹¶å°†`input`è¿”å›ä¸ºPyTorchå¼ é‡ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›logitsï¼š

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

ä½¿ç”¨æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹`input_ids`ï¼Œå¹¶ä½¿ç”¨å¤„ç†å™¨å°†é¢„æµ‹çš„`input_ids`è§£ç å›æ–‡æœ¬ï¼š

```py
>>> import torch

>>> pred_ids = torch.argmax(logits, dim=-1)
>>> pred_str = processor.batch_decode(pred_ids)

>>> pred_str
['I would like to set up a joint account with my partner.']
```
</pt>
</frameworkcontent>

```
```python
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)
transcription
['æˆ‘æƒ³å’Œæˆ‘çš„ä¼´ä¾£ä¸€èµ·è®¾ç«‹å…±åŒè´¦æˆ·']
```