<!--ç‰ˆæƒ2023å¹´HuggingFaceå›¢é˜Ÿï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ï¼Œç‰ˆæœ¬2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰è¿›è¡Œè®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ã€‚

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶æ ¹æ®è®¸å¯è¯çš„â€œåŸæ ·â€åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·æŸ¥é˜…è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸‹å…è´£å£°æ˜å’Œé™åˆ¶ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶é‡‡ç”¨Markdownæ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚-->

# å›¾åƒæ ‡æ³¨

[[open-in-colab]]

å›¾åƒæ ‡æ³¨æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„æ ‡é¢˜çš„ä»»åŠ¡ã€‚å®ƒåœ¨ç°å®ä¸–ç•Œçš„å¸¸è§åº”ç”¨åŒ…æ‹¬å¸®åŠ©è§†éšœäººå£«é€šè¿‡ä¸åŒæƒ…å¢ƒè¿›è¡Œå¯¼èˆªã€‚å› æ­¤ï¼Œå›¾åƒæ ‡æ³¨é€šè¿‡æè¿°å›¾åƒæ¥æé«˜äººä»¬çš„å†…å®¹å¯è®¿é—®æ€§ã€‚

æœ¬æŒ‡å—å°†ä»‹ç»ä»¥ä¸‹å†…å®¹ï¼š

* å¾®è°ƒå›¾åƒæ ‡æ³¨æ¨¡å‹ã€‚
* ä½¿ç”¨å·²å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate -q
pip install jiwer -q
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å¯ä»¥ä¸ç¤¾åŒºä¸Šä¼ å¹¶åˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

## åŠ è½½PokÃ©mon BLIPæ ‡é¢˜æ•°æ®é›†

ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“åŠ è½½ä¸€ä¸ªç”±{å›¾åƒ-æ ‡é¢˜}å¯¹ç»„æˆçš„æ•°æ®é›†ã€‚è¦åœ¨PyTorchä¸­åˆ›å»ºè‡ªå·±çš„å›¾åƒæ ‡æ³¨æ•°æ®é›†ï¼Œå¯ä»¥å‚è€ƒ[æ­¤ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)ã€‚

```python
from datasets import load_dataset

ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```
```bash
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

æ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ`image`å’Œ`text`ã€‚

<Tip>

è®¸å¤šå›¾åƒæ ‡æ³¨æ•°æ®é›†åŒ…å«å›¾åƒçš„å¤šä¸ªæ ‡é¢˜ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä¸€ç§å¸¸è§çš„ç­–ç•¥æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»å¯ç”¨çš„æ ‡é¢˜ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ ‡é¢˜ã€‚

</Tip>

ä½¿ç”¨[~datasets.Dataset.train_test_split]æ–¹æ³•å°†æ•°æ®é›†çš„è®­ç»ƒé›†åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

è®©æˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬ã€‚

```python
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np

def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png" alt="Sample training images"/>
</div>

## é¢„å¤„ç†æ•°æ®é›†

ç”±äºæ•°æ®é›†æœ‰ä¸¤ç§æ¨¡æ€ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œé¢„å¤„ç†æµæ°´çº¿å°†é¢„å¤„ç†å›¾åƒå’Œæ ‡é¢˜ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼ŒåŠ è½½ä¸æ‚¨å³å°†å¾®è°ƒçš„æ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ç±»ã€‚

```python
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

å¤„ç†å™¨å°†å†…éƒ¨é¢„å¤„ç†å›¾åƒï¼ˆåŒ…æ‹¬è°ƒæ•´å¤§å°å’Œåƒç´ ç¼©æ”¾ï¼‰å¹¶å¯¹æ ‡é¢˜è¿›è¡Œåˆ†è¯ã€‚

```python
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs

train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

å‡†å¤‡å¥½æ•°æ®é›†åï¼Œæ‚¨ç°åœ¨å¯ä»¥ä¸ºå¾®è°ƒè®¾ç½®æ¨¡å‹äº†ã€‚

## åŠ è½½åŸºç¡€æ¨¡å‹

å°†["microsoft/git-base"](https://huggingface.co/microsoft/git-base)åŠ è½½åˆ°[`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)å¯¹è±¡ä¸­ã€‚

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

## è¯„ä¼°

å›¾åƒæ ‡æ³¨æ¨¡å‹é€šå¸¸ä½¿ç”¨[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge)æˆ–[Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer)è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºæœ¬æŒ‡å—ï¼Œæ‚¨å°†ä½¿ç”¨Word Error Rate (WER)ã€‚

æˆ‘ä»¬ä½¿ç”¨ğŸ¤— Evaluateåº“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å…³äºWERçš„æ½œåœ¨é™åˆ¶å’Œå…¶ä»–å†…å®¹ï¼Œè¯·å‚è€ƒ[æ­¤æŒ‡å—](https://huggingface.co/spaces/evaluate-metric/wer)ã€‚

```python
from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## è®­ç»ƒï¼

ç°åœ¨ï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹å¾®è°ƒæ¨¡å‹äº†ã€‚æ‚¨å°†ä½¿ç”¨ğŸ¤—[`Trainer`]å®Œæˆæ­¤æ“ä½œã€‚

é¦–å…ˆï¼Œä½¿ç”¨[`TrainingArguments`]å®šä¹‰è®­ç»ƒå‚æ•°ã€‚

```python
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

ç„¶åå°†å®ƒä»¬ä¸æ•°æ®é›†å’Œæ¨¡å‹ä¸€èµ·ä¼ é€’ç»™ğŸ¤— Trainerã€‚

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

è¦å¼€å§‹è®­ç»ƒï¼Œåªéœ€åœ¨[`Trainer`]å¯¹è±¡ä¸Šè°ƒç”¨[`~Trainer.train`]å³å¯ã€‚

```python
trainer.train()
```

æ‚¨ä¼šçœ‹åˆ°éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè®­ç»ƒæŸå¤±å¹³ç¨³ä¸‹é™ã€‚

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[`~Trainer.push_to_hub`]æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```python
trainer.push_to_hub()
```

## æ¨ç†

ä»`test_ds`ä¸­å–ä¸€å¼ æ ·æœ¬å›¾åƒæ¥æµ‹è¯•æ¨¡å‹ã€‚

```python
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png" alt="Test image"/>
</div>

ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

è°ƒç”¨[`generate`]å¹¶è§£ç é¢„æµ‹ç»“æœã€‚

```python
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

```bash
a drawing of a pink and blue pokemon
```

çœ‹èµ·æ¥å¾®è°ƒæ¨¡å‹ç”Ÿæˆäº†ä¸€ä¸ªå¾ˆå¥½çš„æ ‡é¢˜ï¼