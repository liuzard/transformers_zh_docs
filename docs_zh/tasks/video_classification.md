版权2023 HuggingFace团队。保留所有权利。

根据Apache许可证2.0（"许可证"），在遵循许可证的情况下，你不得使用此文件。你可以在http://www.apache.org/licenses/LICENSE-2.0获取许可证的副本。

除非适用法律要求或书面同意，按"原样"分发的软件均不带任何担保或条件，无论是明示的还是暗示的。请参阅许可证以获得许可证下限制和特殊语言的具体条款。

⚠️请注意，此文件使用Markdown格式，但包含特定的语法用于我们的文档构建器（类似于MDX），这在你的Markdown查看器中可能无法正确呈现。

视频分类

[[open-in-colab]]

视频分类是将标签或类别分配给整个视频的任务。预计每个视频只有一个类别。视频分类模型将视频作为输入，并返回有关视频所属类别的预测。这些模型可用于对视频的分类。视频分类的真实应用是行为/活动识别，这对健身应用非常有用。对视力受损的人特别有帮助，尤其是在他们通勤时。

本指南将展示以下内容：

1. 在[UCF101](https://www.crcv.ucf.edu/data/UCF101.php)数据集的子集上对[VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)进行微调。
2. 使用微调的模型进行推理。

<Tip>
本教程中所示的任务受到以下模型架构的支持：

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[TimeSformer](../model_doc/timesformer), [VideoMAE](../model_doc/videomae), [ViViT](../model_doc/vivit)

<!--End of the generated tip-->

</Tip>

开始之前，请确保你已安装所有必要的库：

```bash
pip install -q pytorchvideo transformers evaluate
```

你将使用[PyTorchVideo](https://pytorchvideo.org/)（称为`pytorchvideo`）来处理和准备视频。

我们鼓励你登录你的Hugging Face帐户，这样你就可以上传和与社区分享模型。在提示时输入你的token登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

加载UCF101数据集

首先加载[UCF-101数据集](https://www.crcv.ucf.edu/data/UCF101.php)的一个子集。在对整个数据集进行更长时间的训练之前，这将为你提供一个机会来进行实验并确保一切正常运行。

```py
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

下载完子集后，你需要解压缩存档：

```py 
>>> import tarfile

>>> with tarfile.open(file_path) as t:
...      t.extractall(".")
```

在高层次上，数据集的组织方式如下：

```bash
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```

（排序）视频路径如下所示：

```bash
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
```

你将注意到，同一组/场景中的视频剪辑属于同一组，其中组在视频文件路径中以`g`表示。例如，`v_ApplyEyeMakeup_g07_c04.avi`和`v_ApplyEyeMakeup_g07_c06.avi`。

对于验证和评估集，你不希望具有来自同一组/场景的视频剪辑，以防止[数据泄漏](https://www.kaggle.com/code/alexisbcook/data-leakage)。该教程中使用的子集就考虑到了这一点。

接下来，你将提取数据集中的标签集。同时，创建两个在初始化模型时会有帮助的字典：

* `label2id`：将类名映射到整数。
* `id2label`：将整数映射到类名。

```py 
>>> class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f"Unique classes: {list(label2id.keys())}.")

# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].
```

有10个唯一类别。对于每个类别，训练集有30个视频。

加载要微调的模型

从预训练检查点和其相关联的图像处理器实例化视频分类模型。模型的编码器带有预训练参数，并且分类头是随机初始化的。在编写数据集的预处理流水线时，图像处理器将非常有用。

```py 
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # 此参数表示你打算对已经微调过的检查点进行微调
... )
```

在模型加载过程中，你可能会注意到以下警告：

```bash
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

警告告诉我们，我们丢弃了一些权重（例如`classifier`层的权重和偏差），并且随机初始化了另一些权重（新的`classifier`层的权重和偏差）。这在这种情况下是正常的，因为我们添加了一个新的头部，我们没有预训练权重，因此库警告我们在使用它进行推断之前应先微调此模型，而这正是我们要做的。

**注意**，[此检查点](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)在此任务的性能上更好，因为此检查点通过微调获得，微调时与此任务的域有很大的重叠。你可以查看[此检查点](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)，该检查点通过微调了`MCG-NJU/videomae-base-finetuned-kinetics`检查点而获得。  

准备训练数据集

要预处理视频，你将利用[PyTorchVideo库](https://pytorchvideo.org/)。首先导入所需的依赖项。 

```py 
>>> import pytorchvideo.data

>>> from pytorchvideo.transforms import (
...     ApplyTransformToKey,
...     Normalize,
...     RandomShortSideScale,
...     RemoveKey,
...     ShortSideScale,
...     UniformTemporalSubsample,
... )

>>> from torchvision.transforms import (
...     Compose,
...     Lambda,
...     RandomCrop,
...     RandomHorizontalFlip,
...     Resize,
... )
```

对于训练数据集，使用统一的时间子采样、像素归一化、随机裁剪和随机水平翻转的组合作为转换。对于验证和评估数据集，保持相同的变换链，除了随机裁剪和水平翻转。要了解有关这些变换的详细信息，请查看[PyTorchVideo的官方文档](https://pytorchvideo.org)。

使用与预训练模型关联的`image_processor`来获取以下信息：

* 图像均值和标准差，用于对视频帧像素进行归一化。
* 视频帧将被调整为的空间分辨率。

首先定义一些常量。

```py
>>> mean = image_processor.image_mean
>>> std = image_processor.image_std
>>> if "shortest_edge" in image_processor.size:
...     height = width = image_processor.size["shortest_edge"]
>>> else:
...     height = image_processor.size["height"]
...     width = image_processor.size["width"]
>>> resize_to = (height, width)

>>> num_frames_to_sample = model.config.num_frames
>>> sample_rate = 4
>>> fps = 30
>>> clip_duration = num_frames_to_sample * sample_rate / fps
```

现在，定义数据集特定的转换和数据集。

首先是训练集： 

```py 
>>> train_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     RandomShortSideScale(min_size=256, max_size=320),
...                     RandomCrop(resize_to),
...                     RandomHorizontalFlip(p=0.5),
...                 ]
...             ),
...         ),
...     ]
... )

>>> train_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "train"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
...     decode_audio=False,
...     transform=train_transform,
... )
```

相同的工作流程也适用于验证和评估集： 

```py 
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "val"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )

>>> test_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "test"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
```

**注意**：以上数据集流水线取自[PyTorchVideo官方示例](https://pytorchvideo.org/docs/tutorial_classification#dataset)。我们使用[`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)函数，因为它专为UCF-101数据集定制。在幕后，它返回一个[`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)对象。`LabeledVideoDataset`类是PyTorchVideo数据集中与视频相关的基类。因此，如果要使用PyTorchVideo不支持的自定义数据集，可以相应地扩展`LabeledVideoDataset`类。请参阅`data` API [文档](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)以了解更多信息。另外，如果你的数据集遵循类似的结构（如上所示），那么使用`pytorchvideo.data.Ucf101()`应该没有问题。

你可以访问`num_videos`参数以了解数据集中的视频数量。

```py
>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
# (300, 30, 75)
```

为了更好地进行调试，可视化预处理的视频

```py 
>>> import imageio
>>> import numpy as np
>>> from IPython.display import Image

>>> def unnormalize_img(img):
...     """Un-normalizes the image pixels."""
...     img = (img * std) + mean
...     img = (img * 255).astype("uint8")
...     return img.clip(0, 255)

>>> def create_gif(video_tensor, filename="sample.gif"):
...     """Prepares a GIF from a video tensor.
...     
...     The video tensor is expected to have the following shape:
...     (num_frames, num_channels, height, width).
...     """
...     frames = []
...     for video_frame in video_tensor:
...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
...         frames.append(frame_unnormalized)
...     kargs = {"duration": 0.25}
...     imageio.mimsave(filename, frames, "GIF", **kargs)
...     return filename

>>> def display_gif(video_tensor, gif_name="sample.gif"):
...     """Prepares and displays a GIF from a video tensor."""
...     video_tensor = video_tensor.permute(1, 0, 2, 3)
...     gif_filename = create_gif(video_tensor, gif_name)
...     return Image(filename=gif_filename)

>>> sample_video = next(iter(train_dataset))
>>> video_tensor = sample_video["video"]
>>> display_gif(video_tensor)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif" alt="Person playing basketball"/>
</div>

训练模型

使用 🤗 Transformers 中的 [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) 对模型进行训练。要实例化一个 `Trainer`，需要定义训练配置和评估指标。其中最重要的是 [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)，它是一个包含所有属性以配置训练的类。它需要一个输出文件夹名称，用于保存模型的检查点。它还有助于同步 🤗 Hub 上模型存储库中的所有信息。

大多数训练参数都是显而易见的，但这里有一个非常重要的参数 `remove_unused_columns=False`。它会丢弃模型的调用函数未使用的任何特征。默认情况下它是 `True`，因为通常丢弃未使用的特征列是理想的，这样可以更容易地将输入解包到模型的调用函数中。但是，在这种情况下，你需要未使用的特征（特别是 'video'）来创建 `pixel_values`（这是我们的模型在其输入中期望的一个必需键）。

下面代码定义了训练的参数：

```py 
>>> from transformers import TrainingArguments, Trainer

>>> model_name = model_ckpt.split("/")[-1]
>>> new_model_name = f"{model_name}-finetuned-ucf101-subset"
>>> num_epochs = 4

>>> args = TrainingArguments(
...     new_model_name,
...     remove_unused_columns=False,
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=batch_size,
...     per_device_eval_batch_size=batch_size,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
... )
```

`pytorchvideo.data.Ucf101()` 返回的数据集没有实现 `__len__` 方法。因此，在实例化 `TrainingArguments` 时，我们必须定义 `max_steps`。

下一步，你需要定义一个用于根据预测结果计算度量标准的函数，该函数将使用你将要加载的 `metric`。你唯一需要做的预处理就是获取预测 logits 的 argmax：

```py
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

**关于评估的说明**：

在 [VideoMAE 论文](https://arxiv.org/abs/2203.12602) 中，作者使用以下评估策略。他们对测试视频中的几个片段进行评估，并对这些片段应用不同的裁剪，并报告聚合分数。然而，在简洁和简洁的兴趣下，该教程中没有考虑这一点。

还要定义一个 `collate_fn`，它将用于将示例批处理在一起。每个批处理包含 `pixel_values` 和 `labels` 这两个键。

```py 
>>> def collate_fn(examples):
...     # permute to (num_frames, num_channels, height, width)
...     pixel_values = torch.stack(
...         [example["video"].permute(1, 0, 2, 3) for example in examples]
...     )
...     labels = torch.tensor([example["label"] for example in examples])
...     return {"pixel_values": pixel_values, "labels": labels}
```

然后，将所有这些和数据集一起传递给 `Trainer`：

```py 
>>> trainer = Trainer(
...     model,
...     args,
...     train_dataset=train_dataset,
...     eval_dataset=val_dataset,
...     tokenizer=image_processor,
...     compute_metrics=compute_metrics,
...     data_collator=collate_fn,
... )
```

你可能想知道为什么在预处理数据时已经将 `image_processor` 作为 tokenizer 传递了。这只是为了确保图像处理器配置文件（存储为 JSON）也会上传到 Hub 上的仓库中。

现在通过调用 `train` 方法来微调我们的模型：

```py 
>>> train_results = trainer.train()
```

训练完成后，使用 [`~transformers.Trainer.push_to_hub`] 方法将你的模型共享到 Hub 中，这样每个人都可以使用你的模型：

```py
>>> trainer.push_to_hub()
```

## 推理

很棒，现在你已经对模型进行了微调，可以用它进行推理了！

加载一个用于推理的视频：

```py 
>>> sample_test_video = next(iter(test_dataset))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif" alt="Teams playing basketball"/>
</div>

在推理中尝试使用微调后的模型最简单的方法是在 [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline) 中使用它。使用你的模型实例化一个视频分类的 `pipeline`，并将视频传递给它：

```py
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},
 {'score': 0.017777055501937866, 'label': 'BabyCrawling'},
 {'score': 0.01663011871278286, 'label': 'BalanceBeam'},
 {'score': 0.009560945443809032, 'label': 'BandMarching'},
 {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]
```

你也可以手动复制 `pipeline` 的结果。

```py
>>> def run_inference(model, video):
...     # (num_frames, num_channels, height, width)
...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)
...     inputs = {
...         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
...         "labels": torch.tensor(
...             [sample_test_video["label"]]
...         ),  # 如果没有标签可用，可以跳过此项。
...     }

...     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
...     inputs = {k: v.to(device) for k, v in inputs.items()}
...     model = model.to(device)

...     # 前向传递
...     with torch.no_grad():
...         outputs = model(**inputs)
...         logits = outputs.logits

...     return logits
```

现在，将输入传递给模型并返回 `logits`：

```
>>> logits = run_inference(trained_model, sample_test_video["video"])
```

解码 `logits`，我们得到： 

```py 
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```