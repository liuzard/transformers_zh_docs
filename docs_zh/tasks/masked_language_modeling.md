<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# 掩码语言建模

[[在Google Colab中打开]]

<Youtube id="mqElG5QJWUg"/>

掩码语言建模是预测序列中掩码标记的任务，模型可以双向关注标记。这意味着模型可以完全访问左右两侧的标记。掩码语言建模适用于需要对整个序列具有良好上下文理解的任务。BERT就是掩码语言模型的一个例子。

本指南将向你展示如何：

1. 在ELI5数据集的[r/askscience](https://www.reddit.com/r/askscience/)子集上微调[DistilRoBERTa](https://huggingface.co/distilroberta-base)。
2. 使用微调后的模型进行推理。

<Tip>
你可以按照本指南的步骤微调其他掩码语言模型架构。
选择以下架构之一：

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [CamemBERT](../model_doc/camembert), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ESM](../model_doc/esm), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MRA](../model_doc/mra), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha), [Nyströmformer](../model_doc/nystromformer), [Perceiver](../model_doc/perceiver), [QDQBert](../model_doc/qdqbert), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [TAPAS](../model_doc/tapas), [Wav2Vec2](../model_doc/wav2vec2), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)

<!--End of the generated tip-->

</Tip>

在开始之前，请确保已安装所有必要的库：

```bash
pip install transformers datasets evaluate
```

我们鼓励你登录Hugging Face帐户，以便你可以上传和与社区共享你的模型。在提示时，输入你的token以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 加载ELI5数据集

首先，从🤗 Datasets库中加载ELI5数据集中r/askscience子集的一个较小子集。这将给你一个机会进行实验，并确保一切正常，然后再花更多时间在完整数据集上进行训练。

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

将数据集的`train_asks`拆分为训练集和测试集，使用[`~datasets.Dataset.train_test_split`]方法：

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

然后查看一个示例：
```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
 ```

虽然这看起来可能有点多，但你实际上只对`text`字段感兴趣。关于语言建模任务的酷炫之处在于你不需要标签（也称为无监督任务），因为下一个词就是标签。

## 预处理

<Youtube id="8PmhEIXhBvI"/>

对于掩码语言建模，下一步是加载一个DistilRoBERTa分词器来处理`text`子字段：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")
```

你会注意到上面的示例中，`text`字段实际上是嵌套在`answers`内的。这意味着你需要使用[`flatten`]方法从其嵌套结构中提取`text`子字段：

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

现在，每个子字段都是一个单独的列，如`answers`前缀所示，而`text`字段现在是一个列表。不需要将每个句子分别标记化，而是将列表转换为字符串，以便可以联合标记化它们。

下面是第一个预处理函数，该函数将每个示例的字符串列表连接起来，并对结果进行标记化：

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

使用[`~datasets.Dataset.map`]方法将此预处理函数应用于整个数据集。你可以通过设置`batched=True`以一次处理数据集的多个元素，并通过设置`num_proc`来增加进程数，以加快`map`函数的速度。删除你不需要的任何列：

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

该数据集包含标记序列，但其中一些序列比模型的最大输入长度要长。

现在，使用第二个预处理函数：
- 连接所有序列
- 根据`block_size`将连接后的序列拆分为较短的块，其长度应小于最大输入长度，并且足够小，适应你的GPU RAM。

```py
>>> block_size = 128

>>> def group_texts(examples):
...     # 合并所有文本。
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # 我们舍弃掉不足一个block_size的部分，我们可以在这里添加填充，而不是舍弃，前提是模型支持填充，你可以根据需要自定义此部分。
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # 按block_size划分。
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     return result
```

在整个数据集上应用`group_texts`函数：

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

现在，使用[`DataCollatorForLanguageModeling`]创建一个示例批次。在整个批次的数据校对期间，*动态填充*句子到批次中的最长长度比在整个数据集上填充到最大长度更高效。

<frameworkcontent>
<pt>

将序列结束标记用作填充标记，并指定`mlm_probability`，以便在每次迭代数据时随机掩码标记：

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
</pt>
<tf>

将序列结束标记用作填充标记，并指定`mlm_probability`，以便在每次迭代数据时随机掩码标记：

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```
</tf>
</frameworkcontent>

## 训练

<frameworkcontent>
<pt>
<Tip>

如果你对使用[`Trainer`]微调模型不太熟悉，请查看[此处](../training.md#train-with-pytorch-trainer)的基本教程。

</Tip>

你现在可以开始训练模型了！使用[`AutoModelForMaskedLM`]加载DistilRoBERTa：
```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilroberta-base")
```

只剩下三个步骤：

1. 在[`TrainingArguments`]中定义你的训练超参数。仅需要`output_dir`参数，指定保存模型的位置。
2. 将训练参数与模型、数据集和数据校验器一起传递给[`Trainer`]。
3. 调用[`~Trainer.train`]进行模型微调。

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

当训练完成后，使用[`~transformers.Trainer.evaluate`]方法评估模型并获得模型的困惑度：

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
将你的模型共享到Hub上，使用[`~transformers.Trainer.push_to_hub`]方法，这样每个人都可以使用你的模型：

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

如果你对使用Keras微调模型不太熟悉，请查看[此处](../training.md#train-a-tensorflow-model-with-keras)的基本教程。

</Tip>
要在TensorFlow中微调模型，请首先设置优化器函数、学习率调度程序和一些训练超参数：

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

然后可以使用[`TFAutoModelForMaskedLM`]加载DistilRoBERTa：

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("distilroberta-base")
```

使用[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]将数据集转换为`tf.data.Dataset`格式：

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

使用[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)为模型配置训练。请注意，Transformers模型都有默认的与任务相关的损失函数，因此除非你想要指定一个，否则不需要指定：

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # 没有损失参数！
```

可以通过在[`~transformers.PushToHubCallback`]中指定要推送模型和分词器的位置来实现：

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_mlm_model",
...     tokenizer=tokenizer,
... )
```
</tf>
</frameworkcontent>

最后，你可以开始训练模型！使用你的训练和验证数据集、epochs的数量以及你的回调函数来调用[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)方法来微调模型：

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

训练完成后，你的模型会自动上传到Hub，这样每个人都可以使用它！
</tf>
</frameworkcontent>

<Tip>

如果你想要了解更深入的关于如何对模型进行掩码语言建模微调的示例，请查看相应的
[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
或[TensorFlow笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)。

</Tip>

## 推断

太好了，现在你已经对模型进行了微调，可以用它进行推断了！

想出一些你希望模型填补空白的文本，并使用特殊的`<mask>`标记表示空白处：

```py
>>> text = "The Milky Way is a <mask> galaxy."
```

尝试使用微调后的模型进行推断的最简单方式是将其用于[`pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.Pipeline)中。实例化一个填充掩码的`pipeline`，将模型和文本传递给它。如果需要，你可以使用`top_k`参数指定要返回的预测数量：

```py
>>> from transformers import pipeline

>>> mask_filler = pipeline("fill-mask", "stevhliu/my_awesome_eli5_mlm_model")
>>> mask_filler(text, top_k=3)
[{'score': 0.5150994658470154,
  'token': 21300,
  'token_str': ' spiral',
  'sequence': 'The Milky Way is a spiral galaxy.'},
 {'score': 0.07087188959121704,
  'token': 2232,
  'token_str': ' massive',
  'sequence': 'The Milky Way is a massive galaxy.'},
 {'score': 0.06434620916843414,
  'token': 650,
  'token_str': ' small',
  'sequence': 'The Milky Way is a small galaxy.'}]
```

<frameworkcontent>
<pt>
将文本进行标记化，并返回PyTorch张量作为`input_ids`。你还需要指定`<mask>`标记的位置：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="pt")
>>> mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
```

将输入传递给模型，并返回掩码标记的`logits`：

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

然后返回具有最高概率的三个掩码标记并将其打印出来：

```py
>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</pt>
<tf>
将文本进行标记化，并返回TensorFlow张量作为`input_ids`。你还需要指定`<mask>`标记的位置：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="tf")
>>> mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

将输入传递给模型，并返回掩码标记的`logits`：

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

然后返回具有最高概率的三个掩码标记并将其打印出来：

```py
>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</tf>
</frameworkcontent>