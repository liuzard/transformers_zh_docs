```py
# Step 1: Define training hyperparameters
learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[100, 300, 500, 1000, 3000],
    values=[1e-4, 5e-5, 3e-5, 1e-5, 1e-6, 1e-7])


# Step 2: Instantiate a pretrained model
model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)


# Step 3: Convert the ğŸ¤— Dataset to a `tf.data.Dataset`
train_dataset = train_ds.to_tf_dataset(with_transform=train_transforms)


# Step 4: Compile your model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer)


# Step 5: Add callbacks
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="checkpoint.h5",
    save_weights_only=True,
    save_best_only=True,
    monitor="val_loss",
    mode="min",
)


# Step 6: Use `fit()` to run training
model.fit(
    train_dataset,
    validation_data=test_ds,
    epochs=50,
    callbacks=[checkpoint_callback],
)


# Step 7: Save the model to Hub
model.save_pretrained("segformer-b0-scene-parse-150-tf")
```
</tf>
</frameworkcontent>

è¿™æ˜¯å…³äºè¯­ä¹‰åˆ†å‰²çš„æŒ‡å—ï¼Œå®ƒå°†æ¯ä¸ªåƒç´ åˆ†é…ç»™ä¸€ä¸ªæ ‡ç­¾æˆ–ç±»åˆ«ã€‚è¯­ä¹‰åˆ†å‰²æœ‰å‡ ç§ç±»å‹ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œä¸åŒºåˆ†ç›¸åŒå¯¹è±¡çš„å”¯ä¸€å®ä¾‹ã€‚ä¸¤ä¸ªå¯¹è±¡è¢«èµ‹äºˆç›¸åŒçš„æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œcarâ€è€Œä¸æ˜¯â€œcar-1â€å’Œâ€œcar-2â€ï¼‰ã€‚å®é™…åº”ç”¨åŒ…æ‹¬åŸ¹è®­è‡ªåŠ¨é©¾é©¶æ±½è½¦è¯†åˆ«è¡Œäººå’Œé‡è¦äº¤é€šä¿¡æ¯ï¼Œè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„ç»†èƒå’Œå¼‚å¸¸ï¼Œä»¥åŠç›‘æµ‹å«æ˜Ÿå›¾åƒä¸­çš„ç¯å¢ƒå˜åŒ–ã€‚

æ­¤æŒ‡å—å°†å±•ç¤ºå¦‚ä½•è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š

1. åœ¨SceneParse150æ•°æ®é›†ä¸Šå¾®è°ƒSegFormerã€‚
2. ä½¿ç”¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install -q datasets transformers evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å°†æ¨¡å‹ä¸Šä¼ å’Œå…±äº«ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½SceneParse150æ•°æ®é›†

é¦–å…ˆä»ğŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½SceneParse150æ•°æ®é›†çš„è¾ƒå°å­é›†ã€‚åœ¨ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒä¹‹å‰ï¼Œè¿™å°†ä¸ºæ‚¨æä¾›å®éªŒå’Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸çš„æœºä¼šã€‚

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†çš„`train`æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`ï¼šåœºæ™¯çš„PILå›¾åƒã€‚
- `annotation`ï¼šåˆ†å‰²åœ°å›¾çš„PILå›¾åƒï¼Œä¹Ÿæ˜¯æ¨¡å‹çš„ç›®æ ‡ã€‚
- `scene_category`ï¼šæè¿°å›¾åƒåœºæ™¯çš„ç±»åˆ«IDï¼Œå¦‚â€œkitchenâ€æˆ–â€œofficeâ€ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨åªéœ€è¦`image`å’Œ`annotation`ï¼Œå®ƒä»¬éƒ½æ˜¯PILå›¾åƒã€‚

æ‚¨è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†æ ‡ç­¾idæ˜ å°„åˆ°æ ‡ç­¾ç±»åˆ«ï¼Œè¿™åœ¨è®¾ç½®æ¨¡å‹æ—¶å°†å¾ˆæœ‰ç”¨ã€‚ä»Hubä¸‹è½½æ˜ å°„å¹¶åˆ›å»º`id2label`å’Œ`label2id`å­—å…¸ï¼š

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½SegFormerå›¾åƒå¤„ç†å™¨ï¼Œä»¥å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä¾›æ¨¡å‹ä½¿ç”¨ã€‚ä¸€äº›æ•°æ®é›†ï¼ˆä¾‹å¦‚æ­¤æ•°æ®é›†ï¼‰ä½¿ç”¨é›¶ç´¢å¼•ä½œä¸ºèƒŒæ™¯ç±»åˆ«ã€‚ä½†æ˜¯ï¼Œå®é™…ä¸ŠèƒŒæ™¯ç±»åˆ«ä¸åŒ…æ‹¬åœ¨è¿™150ä¸ªç±»åˆ«ä¸­ï¼Œå› æ­¤æ‚¨éœ€è¦è®¾ç½®`reduce_labels=True`ï¼Œå°†æ‰€æœ‰æ ‡ç­¾å‡1ã€‚é›¶ç´¢å¼•ç”±`255`æ›¿æ¢ï¼Œå› æ­¤SegFormerçš„æŸå¤±å‡½æ•°ä¼šå¿½ç•¥å®ƒï¼š

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

<frameworkcontent>
<pt>

é€šå¸¸åœ¨å›¾åƒæ•°æ®é›†ä¸Šåº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥ä½¿æ¨¡å‹å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨[`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)å‡½æ•°ï¼Œéšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒåº“ã€‚

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ç°åœ¨åˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œä»¥å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º`pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º`labels`ã€‚å¯¹äºè®­ç»ƒé›†ï¼Œåœ¨å°†å›¾åƒæä¾›ç»™å›¾åƒå¤„ç†å™¨ä¹‹å‰ï¼Œåº”ç”¨`jitter`ã€‚å¯¹äºæµ‹è¯•é›†ï¼Œå›¾åƒå¤„ç†å™¨å¯¹`images`è¿›è¡Œè£å‰ªå’Œå½’ä¸€åŒ–ï¼Œåªè£å‰ª`labels`ï¼Œå› ä¸ºåœ¨æµ‹è¯•æœŸé—´ä¸åº”ç”¨æ•°æ®å¢å¼ºã€‚

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ä½¿ç”¨ğŸ¤—æ•°æ®é›†[`~datasets.Dataset.set_transform`]å‡½æ•°åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨`jitter`ã€‚å˜æ¢æ˜¯å³æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

é€šå¸¸ï¼Œåœ¨å›¾åƒæ•°æ®é›†ä¸Šåº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹å¯¹è¿‡æ‹Ÿåˆçš„é²æ£’æ€§ã€‚
åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒåº“ã€‚
è¯·å®šä¹‰ä¸¤ä¸ªä¸åŒçš„è½¬æ¢å‡½æ•°ï¼š
- åŒ…å«å›¾åƒå¢å¼ºçš„è®­ç»ƒæ•°æ®è½¬æ¢
- ä»…è½¬ç½®å›¾åƒçš„éªŒè¯æ•°æ®è½¬æ¢ï¼Œå› ä¸ºğŸ¤— Transformersä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹éœ€è¦ä»¥é€šé“ä¼˜å…ˆçš„å¸ƒå±€ï¼ˆchannels-first layoutï¼‰

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œä»¥å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šçš„æ‰¹æ¬¡ä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›å‡½æ•°åº”ç”¨å›¾åƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„`image_processor`å°†å›¾åƒè½¬æ¢ä¸º`pixel_values`å’Œæ³¨é‡Šè½¬æ¢ä¸º`labels`ã€‚`ImageProcessor`è¿˜è´Ÿè´£è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–å›¾åƒã€‚

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ä½¿ç”¨ğŸ¤—æ•°æ®é›†[`~datasets.Dataset.set_transform`]å‡½æ•°åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†è½¬æ¢ã€‚å˜æ¢æ˜¯å³æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡æŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½[mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)ï¼ˆIoUï¼‰åº¦é‡æŒ‡æ ‡ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æŒ‡æ ‡ï¼‰ï¼š

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥[`~evaluate.EvaluationModule.compute`]åº¦é‡æŒ‡æ ‡ã€‚é¦–å…ˆéœ€è¦å°†é¢„æµ‹è½¬æ¢ä¸ºlogitsï¼Œç„¶åå°†å…¶é‡å¡‘ä¸ºä¸æ ‡ç­¾å¤§å°ç›¸åŒ¹é…ï¼Œç„¶åæ‰èƒ½è°ƒç”¨[`~evaluate.EvaluationModule.compute`]ï¼š

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

ç°åœ¨æ‚¨çš„`compute_metrics`å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œè¯·åœ¨è®¾ç½®è®­ç»ƒæ—¶è¿”å›ã€‚

## è®­ç»ƒ
<frameworkcontent>
<pt>
<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨[`Trainer`]è¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·å…ˆæŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training.md#finetune-with-trainer)ï¼

</Tip>

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨[`AutoModelForSemanticSegmentation`]åŠ è½½SegFormerï¼Œå¹¶å°†æ¨¡å‹ä¸æ ‡ç­¾idå’Œæ ‡ç­¾ç±»åˆ«ä¹‹é—´çš„æ˜ å°„ä¼ é€’ç»™æ¨¡å‹ï¼š

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨[`TrainingArguments`]ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™å°†åˆ é™¤`image`åˆ—ã€‚æ²¡æœ‰`image`åˆ—ï¼Œæ‚¨æ— æ³•åˆ›å»º`pixel_values`ã€‚å°†`remove_unused_columns=False`è®¾ç½®ä¸ºé˜²æ­¢æ­¤è¡Œä¸ºï¼ä»…å…¶ä»–å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆéœ€è¦ç™»å½•Hugging Faceä»¥ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[`Trainer`]å°†è¯„ä¼°IoUåº¦é‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€tokenizerã€æ•°æ®æ”¶é›†å™¨å’Œ`compute_metrics`å‡½æ•°ä¼ é€’ç»™[`Trainer`]ã€‚
3. è°ƒç”¨[`~Trainer.train`]å¼€å§‹å¾®è°ƒæ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

å®Œæˆè®­ç»ƒåï¼Œè¯·ä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

å¦‚æœæ‚¨ç†Ÿæ‚‰ä½¿ç”¨Kerasè¿›è¡Œå¾®è°ƒæ¨¡å‹ï¼Œè¯·å…ˆå‚é˜…[åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼

</Tip>

è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
1. å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è®¡åˆ’ã€‚
2. å®ä¾‹åŒ–é¢„è®­ç»ƒæ¨¡å‹ã€‚
3. å°†ğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`ã€‚
4. ç¼–è¯‘æ¨¡å‹ã€‚
5. æ·»åŠ å›è°ƒå‡½æ•°æ¥è®¡ç®—æŒ‡æ ‡å’Œä¸Šä¼ æ¨¡å‹åˆ°ğŸ¤— Hubã€‚
6. ä½¿ç”¨`fit()`æ–¹æ³•æ¥è¿è¡Œè®­ç»ƒã€‚

```py
# æ­¥éª¤1ï¼šå®šä¹‰è®­ç»ƒè¶…å‚æ•°
learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[100, 300, 500, 1000, 3000],
    values=[1e-4, 5e-5, 3e-5, 1e-5, 1e-6, 1e-7])


# æ­¥éª¤2ï¼šå®ä¾‹åŒ–é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)


# æ­¥éª¤3ï¼šå°†ğŸ¤— Datasetè½¬æ¢ä¸º`tf.data.Dataset`
train_dataset = train_ds.to_tf_dataset(with_transform=train_transforms)


# æ­¥éª¤4ï¼šç¼–è¯‘æ¨¡å‹
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer)


# æ­¥éª¤5ï¼šæ·»åŠ å›è°ƒå‡½æ•°
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="checkpoint.h5",
    save_weights_only=True,
    save_best_only=True,
    monitor="val_loss",
    mode="min",
)


# æ­¥éª¤6ï¼šä½¿ç”¨`fit()`æ–¹æ³•è¿›è¡Œè®­ç»ƒ
model.fit(
    train_dataset,
    validation_data=test_ds,
    epochs=50,
    callbacks=[checkpoint_callback],
)


# æ­¥éª¤7ï¼šå°†æ¨¡å‹ä¿å­˜åˆ°Hub
model.save_pretrained("segformer-b0-scene-parse-150-tf")
```
</tf>
</frameworkcontent>

```markdown
### æ¨ç†

å¥½çš„ï¼Œæ—¢ç„¶æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼

åŠ è½½ç”¨äºæ¨ç†çš„å›¾åƒï¼š

```py
image = ds[0]["image"]
image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>
</div>

<frameworkcontent>
<pt>
å°è¯•ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨[`pipeline`]ã€‚ä½¿ç”¨æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªå›¾åƒåˆ†å‰²çš„`pipeline`ï¼Œç„¶åå°†å›¾åƒä¼ é€’ç»™å®ƒï¼š

```py
from transformers import pipeline

segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```

å¦‚æœéœ€è¦ï¼Œæ‚¨è¿˜å¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœã€‚ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒï¼Œå¹¶å°†`pixel_values`æ”¾åœ¨GPUä¸Šï¼š

```py
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # å¦‚æœæœ‰å¯ç”¨çš„GPUï¼Œåˆ™ä½¿ç”¨GPUï¼Œå¦åˆ™ä½¿ç”¨CPU
encoding = image_processor(image, return_tensors="pt")
pixel_values = encoding.pixel_values.to(device)
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`:

```py
outputs = model(pixel_values=pixel_values)
logits = outputs.logits.cpu()
```

æ¥ä¸‹æ¥ï¼Œå°†`logits`è°ƒæ•´åˆ°åŸå§‹å›¾åƒçš„å¤§å°ï¼š

```py
upsampled_logits = nn.functional.interpolate(
    logits,
    size=image.size[::-1],
    mode="bilinear",
    align_corners=False,
)

pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
åŠ è½½å›¾åƒå¤„ç†å™¨ä»¥é¢„å¤„ç†å›¾åƒå¹¶ä»¥TensorFlowå¼ é‡çš„å½¢å¼è¿”å›è¾“å…¥ï¼š

```py
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
inputs = image_processor(image, return_tensors="tf")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`:

```py
from transformers import TFAutoModelForSemanticSegmentation

model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
logits = model(**inputs).logits
```

æ¥ä¸‹æ¥ï¼Œå°†`logits`è°ƒæ•´åˆ°åŸå§‹å›¾åƒçš„å¤§å°ï¼Œå¹¶å¯¹ç±»ç»´åº¦åº”ç”¨`argmax`å‡½æ•°ï¼š
```py
logits = tf.transpose(logits, [0, 2, 3, 1])

upsampled_logits = tf.image.resize(
    logits,
    # ç”±äº`image.size`è¿”å›å®½åº¦å’Œé«˜åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢ å€’äº†`image`çš„å½¢çŠ¶ã€‚
    image.size[::-1],
)

pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

è¦å¯è§†åŒ–ç»“æœï¼ŒåŠ è½½[æ•°æ®é›†é¢œè‰²è°ƒè‰²æ¿](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ä½œä¸º`ade_palette()`å°†æ¯ä¸ªç±»åˆ«æ˜ å°„åˆ°RGBå€¼ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥å°†å›¾åƒå’Œé¢„æµ‹çš„åˆ†å‰²å›¾ç»„åˆåœ¨ä¸€èµ·å¹¶ç»˜åˆ¶å‡ºæ¥ï¼š

```py
import matplotlib.pyplot as plt
import numpy as np

color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
palette = np.array(ade_palette())
for label, color in enumerate(palette):
    color_seg[pred_seg == label, :] = color
color_seg = color_seg[..., ::-1]  # è½¬æ¢ä¸ºBGR

img = np.array(image) * 0.5 + color_seg * 0.5  # å°†å›¾åƒä¸åˆ†å‰²å›¾é‡å æ˜¾ç¤º
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>
```