<!--ç‰ˆæƒæ‰€æœ‰2022 The HuggingFace Teamã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apache License, Version 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯; åœ¨éµå®ˆè®¸å¯è¯çš„æƒ…å†µä¸‹ï¼Œä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢çš„ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢çº¦å®šï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å‘çš„åŸºç¡€ä¸Šï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºæ‹…ä¿ã€‚è¯¦ç»†äº†è§£æƒé™å’Œé™åˆ¶ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä¸ºMarkdownæ–‡ä»¶ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œè¿™å¯èƒ½æ— æ³•åœ¨MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ¸²æŸ“ã€‚

-->

# Tokenåˆ†ç±»

[[åœ¨colabä¸­æ‰“å¼€]]

<Youtube id="wVHdVlPScxA"/>

Tokenåˆ†ç±»ä¸ºå¥å­ä¸­çš„æ¯ä¸ªæ ‡è®°åˆ†é…ä¸€ä¸ªæ ‡ç­¾ã€‚æœ€å¸¸è§çš„Tokenåˆ†ç±»ä»»åŠ¡ä¹‹ä¸€æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚NERæ—¨åœ¨ä¸ºå¥å­ä¸­çš„æ¯ä¸ªå®ä½“ï¼ˆå¦‚äººã€ä½ç½®æˆ–ç»„ç»‡ï¼‰æ‰¾åˆ°ä¸€ä¸ªæ ‡ç­¾ã€‚

æœ¬æŒ‡å—å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š

1. ä½¿ç”¨[DistilBERT](https://huggingface.co/distilbert-base-uncased)å¯¹[WNUT 17](https://huggingface.co/datasets/wnut_17)æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥æ£€æµ‹æ–°çš„å®ä½“ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip>
æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š

<!--æ­¤æç¤ºç”± `make fix-copies` è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™!-->

[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [BioGpt](../model_doc/biogpt), [BLOOM](../model_doc/bloom), [BROS](../model_doc/bros), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m), [ESM](../model_doc/esm), [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [MarkupLM](../model_doc/markuplm), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt), [MRA](../model_doc/mra), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer), [QDQBert](../model_doc/qdqbert), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)

<!--è‡ªåŠ¨ç”Ÿæˆçš„æç¤ºç»“æŸ-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate seqeval
```

æˆ‘ä»¬å»ºè®®ä½ ç™»å½•åˆ°ä½ çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·ä½ å¯ä»¥ä¸Šä¼ å’Œå…±äº«ä½ çš„æ¨¡å‹ç»™ç¤¾åŒºã€‚æç¤ºè¾“å…¥ä½ çš„tokenä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½WNUT 17æ•°æ®é›†

é¦–å…ˆä»ğŸ¤—Datasetsåº“ä¸­åŠ è½½WNUT 17æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

ç„¶åçœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

`ner_tags`ä¸­çš„æ¯ä¸ªæ•°å­—è¡¨ç¤ºä¸€ä¸ªå®ä½“ã€‚å°†æ•°å­—è½¬æ¢ä¸ºå…¶æ ‡ç­¾åç§°ä»¥äº†è§£å®ä½“æ˜¯ä»€ä¹ˆï¼š

```py
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

`ner_tags`ä¸­çš„æ¯ä¸ªæ ‡è®°å‰ç¼€å­—æ¯è¡¨ç¤ºå®ä½“çš„tokenä½ç½®ï¼š

- `B-`è¡¨ç¤ºå®ä½“çš„å¼€å§‹ã€‚
- `I-`è¡¨ç¤ºtokenåŒ…å«åœ¨åŒä¸€ä¸ªå®ä½“ä¸­ï¼ˆä¾‹å¦‚ï¼Œ`State`tokenæ˜¯`Empire State Building`å®ä½“çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
- `0`è¡¨ç¤ºè¯¥tokenä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚

## é¢„å¤„ç†

<Youtube id="iY2AZYdZAr0"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½DistilBERTåˆ†è¯å™¨ä»¥é¢„å¤„ç†`tokens`å­—æ®µï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

æ­£å¦‚ä½ åœ¨ä¸Šé¢çš„ç¤ºä¾‹`tokens`å­—æ®µä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œå®ƒçœ‹èµ·æ¥åƒå·²ç»è¿›è¡Œäº†æ ‡è®°åŒ–çš„è¾“å…¥ã€‚ä½†æ˜¯å®é™…ä¸Šè¾“å…¥å°šæœªæ ‡è®°åŒ–ï¼Œä½ éœ€è¦è®¾ç½®`is_split_into_words=True`å°†å•è¯æ ‡è®°åŒ–ä¸ºå­å•è¯ã€‚ä¾‹å¦‚ï¼š

```py
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

ä½†æ˜¯ï¼Œè¿™ä¼šæ·»åŠ ä¸€äº›ç‰¹æ®Šæ ‡è®°`[CLS]`å’Œ`[SEP]`ï¼Œè€Œå­è¯æ ‡è®°ä¼šå¯¼è‡´è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ç°åœ¨ï¼Œä¸€ä¸ªå¯¹åº”äºå•ä¸ªæ ‡ç­¾çš„å•ä¸ªå•è¯å¯èƒ½è¢«åˆ†å‰²ä¸ºä¸¤ä¸ªå­è¯ã€‚ä½ éœ€è¦é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯¹é½æ ‡è®°å’Œæ ‡ç­¾ï¼š

1. ä½¿ç”¨[`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids)æ–¹æ³•å°†æ‰€æœ‰æ ‡è®°æ˜ å°„åˆ°ç›¸åº”çš„å•è¯ã€‚
2. å°†ç‰¹æ®Šæ ‡è®°`[CLS]`å’Œ`[SEP]`çš„æ ‡ç­¾è®¾ç½®ä¸º`-100`ï¼Œè¿™æ ·å®ƒä»¬å°†è¢«å¿½ç•¥æ‰ç”¨äºPyTorchæŸå¤±å‡½æ•°çš„è®¡ç®—ï¼ˆè¯·å‚è§[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)ï¼‰ã€‚
3. ä»…ä¸ºç»™å®šå•è¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è¿›è¡Œæ ‡è®°ã€‚å°†åŒä¸€å•è¯çš„å…¶ä»–å­è¯åˆ†é…ä¸º`-100`ã€‚

ä»¥ä¸‹æ˜¯ä½ å¯ä»¥åˆ›å»ºä»¥å¯¹é½æ ‡è®°å’Œæ ‡ç­¾çš„å‡½æ•°ï¼Œå¹¶æˆªæ–­åºåˆ—ä¸ºä¸è¶…è¿‡DistilBERTçš„æœ€å¤§è¾“å…¥é•¿åº¦çš„æ–¹æ³•ï¼š

```py
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # å°†æ ‡è®°æ˜ å°„åˆ°å®ƒä»¬å¯¹åº”çš„å•è¯ã€‚
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # å°†ç‰¹æ®Šæ ‡è®°è®¾ç½®ä¸º-100ã€‚
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # ä»…å¯¹ç»™å®šå•è¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°è¿›è¡Œæ ‡è®°ã€‚
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

è¦å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨ğŸ¤—Datasets [`~datasets.Dataset.map`]å‡½æ•°ã€‚é€šè¿‡è®¾ç½®`batched=True`å¯ä»¥åŠ é€Ÿ`map`å‡½æ•°ï¼Œä»¥ä¾¿ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼š

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

ç°åœ¨ä½¿ç”¨[`DataCollatorWithPadding`]åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚åœ¨æ•´ç†æœŸé—´å°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```
</tf>
</frameworkcontent>

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«åº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ å¯ä»¥ä½¿ç”¨ğŸ¤—[è¯„ä¼°](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæœ¬ä»»åŠ¡ï¼Œè¯·åŠ è½½[seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval)æ¡†æ¶ï¼ˆè¯·å‚é˜…ğŸ¤—Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æ ‡å‡†çš„æ›´å¤šä¿¡æ¯ï¼‰ã€‚Seqevalå®é™…ä¸Šäº§ç”Ÿäº†å‡ ä¸ªåˆ†æ•°ï¼šç²¾ç¡®åº¦ï¼ˆprecisionï¼‰ã€å¬å›ç‡ï¼ˆrecallï¼‰ã€F1å’Œå‡†ç¡®åº¦ï¼ˆaccuracyï¼‰ã€‚

```py
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

é¦–å…ˆè·å–NERæ ‡ç­¾ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†ä½ çš„çœŸå®é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¼ é€’ç»™[`~evaluate.EvaluationModule.compute`]ä»¥è®¡ç®—åˆ†æ•°ï¼š

```py
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

ç°åœ¨ä½ çš„`compute_metrics`å‡½æ•°å·²ç»å‡†å¤‡å¥½ï¼Œå½“è®¾ç½®è®­ç»ƒæ—¶å°†ä¼šè¿”å›å®ƒã€‚

## è®­ç»ƒ

åœ¨å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œè¯·åˆ›å»ºä¸€ä¸ªé¢„æœŸçš„IDåˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠIDåˆ°æ ‡ç­¾çš„æ˜ å°„`id2label`å’Œ`label2id`ï¼š

```py
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

<frameworkcontent>
<pt>
<Tip>

å¦‚æœä½ å¯¹ä½¿ç”¨[`Trainer`]å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„](../training.md#train-with-pytorch-trainer)çš„åŸºæœ¬æ•™ç¨‹ï¼

</Tip>

ç°åœ¨ï¼Œä½ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨[`AutoModelForTokenClassification`]åŠ è½½DistilBERTï¼ŒåŒæ—¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š

```py
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

æ­¤æ—¶ï¼Œä»…å‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨[`TrainingArguments`]ä¸­å®šä¹‰ä½ çš„è®­ç»ƒè¶…å‚æ•°ã€‚`output_dir`æ˜¯å”¯ä¸€éœ€è¦çš„å‚æ•°ï¼Œå®ƒæŒ‡å®šè¦ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚ä½ å¯ä»¥è®¾ç½®`push_to_hub=True`å°†æ¨¡å‹æ¨é€åˆ°Hubï¼ˆä¸Šä¼ æ¨¡å‹éœ€è¦ç™»å½•åˆ°Hugging Faceï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[`Trainer`]å°†è¯„ä¼°seqevalåˆ†æ•°å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ä¸€èµ·ä¼ é€’ç»™[`Trainer`]ã€‚
3. è°ƒç”¨[`~Trainer.train`]ä»¥å¾®è°ƒä½ çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†ä½ çš„æ¨¡å‹åˆ†äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

å¦‚æœä½ å¯¹ä½¿ç”¨Kerasè¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„](../training.md#train-a-tensorflow-model-with-keras)çš„åŸºæœ¬æ•™ç¨‹ï¼

</Tip>
è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¸€ä¸ªä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=2e-5,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=0.01,
...     num_warmup_steps=0,
... )
```

ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨[`TFAutoModelForTokenClassification`]åŠ è½½DistilBERTï¼ŒåŒæ—¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained(
...     "distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

ä½¿ç”¨[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_wnut["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

```md
å°†æ¨¡å‹é…ç½®ä¸ºä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)è¿›è¡Œè®­ç»ƒã€‚æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éä½ æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦å†æŒ‡å®šäº†ï¼š

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # æ²¡æœ‰æŸå¤±å‚æ•°ï¼
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œè¿˜æœ‰æœ€åä¸¤ä»¶äº‹è¦åšï¼Œå³ä»é¢„æµ‹ä¸­è®¡ç®—seqevalåˆ†æ•°ï¼Œå¹¶æä¾›å°†æ¨¡å‹ä¸Šä¼ åˆ°Hubçš„æ–¹æ³•ã€‚è¿™ä¸¤ä»¶äº‹éƒ½æ˜¯é€šè¿‡ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)æ¥å®Œæˆçš„ã€‚

å°†ä½ çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[`~transformers.KerasMetricCallback`]ï¼š

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

åœ¨[`~transformers.PushToHubCallback`]ä¸­æŒ‡å®šå°†æ¨¡å‹å’Œåˆ†è¯å¤„ç†å™¨ä¸Šä¼ åˆ°å“ªï¼š

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_wnut_model",
...     tokenizer=tokenizer,
... )
```

ç„¶åå°†å›è°ƒæ†ç»‘åœ¨ä¸€èµ·ï¼š

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œä½ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€è®­ç»ƒè½®æ•°ä»¥åŠå›è°ƒå‡½æ•°æ¥è°ƒç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½ çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
</tf>
</frameworkcontent>

<Tip>

è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºtokenåˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)æˆ–[TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ã€‚

</Tip>

## æ¨ç†

å¤ªæ£’äº†ï¼Œç°åœ¨ä½ å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼

é€‰æ‹©ä¸€äº›ä½ æƒ³è¦è¿›è¡Œæ¨ç†çš„æ–‡æœ¬ï¼š

```py
>>> text = "The Golden State Warriors are an American professional basketball team based in San Francisco."
```

å°è¯•ä½¿ç”¨[`pipeline`]ä¸­çš„æ¨¡å‹è¿›è¡Œæ¨ç†æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ä½¿ç”¨NERå®ä¾‹åŒ–ä¸€ä¸ª`pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> classifier = pipeline("ner", model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
[{'entity': 'B-location',
  'score': 0.42658573,
  'index': 2,
  'word': 'golden',
  'start': 4,
  'end': 10},
 {'entity': 'I-location',
  'score': 0.35856336,
  'index': 3,
  'word': 'state',
  'start': 11,
  'end': 16},
 {'entity': 'B-group',
  'score': 0.3064001,
  'index': 4,
  'word': 'warriors',
  'start': 17,
  'end': 25},
 {'entity': 'B-location',
  'score': 0.65523505,
  'index': 13,
  'word': 'san',
  'start': 80,
  'end': 83},
 {'entity': 'B-location',
  'score': 0.4668663,
  'index': 14,
  'word': 'francisco',
  'start': 84,
  'end': 93}]
```

å¦‚æœéœ€è¦ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š

<frameworkcontent>
<pt>
å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è¿”å›PyTorchå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼š

```py
>>> predictions = torch.argmax(logits, dim=2)
>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]
>>> predicted_token_class
['O',
 'O',
 'B-location',
 'I-location',
 'B-group',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'B-location',
 'B-location',
 'O',
 'O']
```
</pt>
<tf>
å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è¿”å›TensorFlowå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> logits = model(**inputs).logits
```

è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼š

```py
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)
>>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
>>> predicted_token_class
['O',
 'O',
 'B-location',
 'I-location',
 'B-group',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'B-location',
 'B-location',
 'O',
 'O']
```
</tf>
</frameworkcontent>