<!--ç‰ˆæƒæ‰€æœ‰2022å¹´çš„HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apache License Version 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œé™¤éä½ éµå®ˆè®¸å¯è¯è§„å®šï¼Œå¦åˆ™ä½ ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬

httpï¼š//www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å¸ƒï¼Œæ²¡æœ‰ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºä¿è¯ã€‚æœ‰å…³è®¸å¯ä¸‹é™åˆ¶çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä¸ºMarkdownæ ¼å¼ï¼Œä½†åŒ…å«ç‰¹å®šè¯­æ³•ï¼Œç”¨äºæˆ‘ä»¬çš„doc-builderï¼ˆç±»ä¼¼äºMDXï¼‰ï¼Œåœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®å‘ˆç°ã€‚

-->

# é—®ç­”

[[open-in-colab]]

<Youtube id="ajPx5LwJD-I"/>

é—®ç­”ä»»åŠ¡æ ¹æ®é—®é¢˜å¾—åˆ°ä¸€ä¸ªç­”æ¡ˆã€‚å¦‚æœä½ æ›¾ç»å‘Alexaã€Siriæˆ–Googleç­‰è™šæ‹ŸåŠ©æ‰‹è¯¢é—®å¤©æ°”æƒ…å†µï¼Œé‚£ä¹ˆä½ ä¹‹å‰ä½¿ç”¨è¿‡é—®ç­”æ¨¡å‹ã€‚å¸¸è§çš„é—®ç­”ä»»åŠ¡æœ‰ä¸¤ç§ç±»å‹ï¼š

- æŠ½å–å¼ï¼šä»ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­æŠ½å–ç­”æ¡ˆã€‚
- ç”Ÿæˆå¼ï¼šä»ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä¸ªæ­£ç¡®å›ç­”é—®é¢˜çš„ç­”æ¡ˆã€‚

æœ¬æŒ‡å—å°†ä»‹ç»å¦‚ä½•ï¼š

1. ä½¿ç”¨[SQuAD](https://huggingface.co/datasets/squad)æ•°æ®é›†å¯¹[DistilBERT](https://huggingface.co/distilbert-base-uncased)è¿›è¡Œå¾®è°ƒï¼Œç”¨äºæŠ½å–å¼é—®ç­”ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip>
æœ¬æ•™ç¨‹ä¸­å±•ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š

<!--æ­¤æç¤ºç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆï¼Œå‹¿è‡ªè¡Œå¡«å†™ï¼-->


[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m), [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [OpenAI GPT-2](../model_doc/gpt2), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj), [I-BERT](../model_doc/ibert), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3), [LED](../model_doc/led), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [LXMERT](../model_doc/lxmert), [MarkupLM](../model_doc/markuplm), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt), [MRA](../model_doc/mra), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer), [OPT](../model_doc/opt), [QDQBert](../model_doc/qdqbert), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [Splinter](../model_doc/splinter), [SqueezeBERT](../model_doc/squeezebert), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)


<!--End of the generated tip-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„åº“ï¼š

```bash
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±ä½ ç™»å½•åˆ°ä½ çš„Hugging Faceè´¦å·ï¼Œè¿™æ ·ä½ å°±å¯ä»¥å°†ä½ çš„æ¨¡å‹ä¸Šä¼ å¹¶å…±äº«ç»™ç¤¾åŒºç”¨æˆ·ã€‚å½“æç¤ºæ—¶ï¼Œè¯·è¾“å…¥ä½ çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ SQuAD æ•°æ®é›†

é¦–å…ˆï¼Œé€šè¿‡ğŸ¤— Datasetsåº“åŠ è½½SQuADæ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†ç»™ä½ ä¸€ä¸ªæœºä¼šåœ¨ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒä¹‹å‰è¿›è¡Œå®éªŒå’Œç¡®ä¿ä¸€åˆ‡å·¥ä½œæ­£å¸¸ã€‚

```py
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†çš„â€œtrainâ€æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> squad = squad.train_test_split(test_size=0.2)
```

ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
>>> squad["train"][0]
{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},
 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
 'id': '5733be284776f41900661182',
 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
 'title': 'University_of_Notre_Dame'
}
```

è¿™é‡Œæœ‰å‡ ä¸ªé‡è¦çš„å­—æ®µï¼š

- `answers`ï¼šç­”æ¡ˆæ ‡è®°çš„èµ·å§‹ä½ç½®å’Œç­”æ¡ˆæ–‡æœ¬ã€‚
- `context`ï¼šæ¨¡å‹éœ€è¦ä»ä¸­æå–ç­”æ¡ˆçš„èƒŒæ™¯ä¿¡æ¯ã€‚
- `question`ï¼šæ¨¡å‹åº”è¯¥å›ç­”çš„é—®é¢˜ã€‚

## é¢„å¤„ç†

<Youtube id="qgaM0weJHpA"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½DistilBERT tokenizerä»¥å¤„ç†`question`å’Œ`context`å­—æ®µï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

è¿˜æœ‰ä¸€äº›ç‰¹å®šäºé—®ç­”ä»»åŠ¡çš„é¢„å¤„ç†æ­¥éª¤éœ€è¦æ³¨æ„ï¼š

1. æ•°æ®é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹å¯èƒ½å…·æœ‰éå¸¸é•¿çš„`context`ï¼Œè¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚ä¸ºäº†å¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œåªæˆªæ–­`context`ï¼Œå°†`truncation`è®¾ç½®ä¸º"only_second"ã€‚
2. æ¥ä¸‹æ¥ï¼Œé€šè¿‡è®¾ç½®`return_offsets_mapping=True`ï¼Œå°†å›ç­”çš„å¼€å§‹å’Œç»“æŸä½ç½®æ˜ å°„åˆ°åŸå§‹çš„`context`ã€‚
3. æœ‰äº†æ˜ å°„åï¼Œå¯ä»¥æ‰¾åˆ°ç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸæ ‡è®°ã€‚ä½¿ç”¨[`~tokenizers.Encoding.sequence_ids`]æ–¹æ³•æ‰¾å‡ºå“ªéƒ¨åˆ†åç§»å¯¹åº”äº`question`ï¼Œå“ªéƒ¨åˆ†å¯¹åº”äº`context`ã€‚

ä¸‹é¢æ˜¯å¦‚ä½•åˆ›å»ºå‡½æ•°æ¥æˆªæ–­å’Œæ˜ å°„`answer`çš„å¼€å§‹å’Œç»“æŸæ ‡è®°åˆ°`context`çš„æ–¹æ³•ï¼š

```py
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding=True,
...         max_length=(64, 384),  # æ‰©å±•è¾“å…¥ä»¥é€‚åº”æ–°çš„è¾“å…¥token
...         stride=128  # æµ‹è¯•æ—¶æŒ‰ç…§128 stride
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„å¼€å§‹å’Œç»“æŸä½ç½®
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # å¦‚æœç­”æ¡ˆæ²¡æœ‰å®Œå…¨åœ¨ä¸Šä¸‹æ–‡å†…ï¼Œåˆ™æ ‡è®°ä¸º(0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # å¦åˆ™æ˜¯å¼€å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[`~datasets.Dataset.map`]å‡½æ•°å³å¯ã€‚ä½ å¯ä»¥é€šè¿‡å°†`batched=True`è®¾ç½®ä¸ºä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ æ¥åŠ å¿«`map`å‡½æ•°çš„é€Ÿåº¦ã€‚åˆ é™¤ä½ ä¸éœ€è¦çš„ä»»ä½•åˆ—ï¼š

```py
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

ç„¶åä½¿ç”¨[`DefaultDataCollator`]åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚ä¸ğŸ¤— Transformersä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ[`DefaultDataCollator`]ä¸ä¼šåº”ç”¨ä»»ä½•é¢å¤–çš„é¢„å¤„ç†ï¼Œä¾‹å¦‚å¡«å……ã€‚

<frameworkcontent>
<pt>
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```
</pt>
<tf>
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```
</tf>
</frameworkcontent>

## è®­ç»ƒ

<frameworkcontent>
<pt>
<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·å‚é˜…[æ­¤å¤„](../training.md#train-with-pytorch-trainer)çš„åŸºç¡€æ•™ç¨‹ï¼

</Tip>

ç°åœ¨ä½ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨[`AutoModelForQuestionAnswering`]åŠ è½½DistilBERTï¼š

```py
>>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

>>> model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```

åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨[`TrainingArguments`]ä¸­å®šä¹‰ä½ çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ¨¡å‹æ¨é€åˆ°Hubï¼ˆä½ éœ€è¦ç™»å½•Hugging Faceæ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€tokenizerå’Œæ•°æ®æ•´ç†å™¨ä¸€èµ·ä¼ é€’ç»™[`Trainer`]ã€‚
3. è°ƒç”¨[`~Trainer.train`]è¿›è¡Œå¾®è°ƒæ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_qa_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_squad["train"],
...     eval_dataset=tokenized_squad["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†æ¨¡å‹åˆ†äº«ç»™Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·å‚é˜…[æ­¤å¤„](../training.md#train-a-tensorflow-model-with-keras)çš„åŸºç¡€æ•™ç¨‹ï¼

</Tip>
è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_epochs = 2
>>> total_train_steps = (len(tokenized_squad["train"]) // batch_size) * num_epochs
>>> optimizer, schedule = create_optimizer(
...     init_lr=2e-5,
...     num_warmup_steps=0,
...     num_train_steps=total_train_steps,
... )
```

ç„¶åä½¿ç”¨[`TFAutoModelForQuestionAnswering`]åŠ è½½DistilBERTï¼š

```py
>>> from transformers import TFAutoModelForQuestionAnswering

>>> model = TFAutoModelForQuestionAnswering("distilbert-base-uncased")
```

ä½¿ç”¨[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_squad["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_squad["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡å‹ï¼š

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œä½ è¿˜éœ€è¦æä¾›ä¸€ç§å°†æ¨¡å‹æ¨é€åˆ°Hubçš„æ–¹æ³•ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨[`~transformers.PushToHubCallback`]ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œtokenizerçš„ä½ç½®æ¥å®Œæˆï¼š

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_qa_model",
...     tokenizer=tokenizer,
... )
```

æœ€åï¼Œä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼è°ƒç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ä¸è®­ç»ƒé›†ã€éªŒè¯é›†çš„æ ·æœ¬æ•°é‡ã€å›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=[callback])
```
è®­ç»ƒå®Œæˆåï¼Œä½ çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
</tf>
</frameworkcontent>

<Tip>

è¦äº†è§£å¦‚ä½•å¯¹é—®ç­”æ¨¡å‹è¿›è¡Œè¯„ä¼°å¹¶äº†è§£å…¶æ€§èƒ½ï¼Œè¯·å‚é˜…ğŸ¤— Hugging Faceè¯¾ç¨‹ä¸­çš„[é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)ç« èŠ‚ã€‚

</Tip>

## æ¨ç†

å¤ªå¥½äº†ï¼Œä½ å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œç°åœ¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼

æå‡ºä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä½ å¸Œæœ›æ¨¡å‹é¢„æµ‹çš„ä¸Šä¸‹æ–‡ï¼š

åœ¨ä½¿ç”¨ä½ çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†æ—¶ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨[`pipeline`]ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨ä½ çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªé—®é¢˜å›ç­”çš„`pipeline`ï¼Œå¹¶å°†ä½ çš„æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> question_answerer = pipeline("question-answering", model="my_awesome_qa_model")
>>> question_answerer(question=question, context=context)
{'score': 0.2058267742395401,
 'start': 10,
 'end': 95,
 'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}
```

å¦‚æœä½ æ„¿æ„ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š

<frameworkcontent>
<pt>
å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›PyTorchå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
>>> inputs = tokenizer(question, context, return_tensors="pt")
```

å°†ä½ çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> import torch
>>> from transformers import AutoModelForQuestionAnswering

>>> model = AutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
>>> with torch.no_grad():
...     outputs = model(**inputs)
```

ä»æ¨¡å‹è¾“å‡ºä¸­è·å–å¼€å§‹å’Œç»“æŸä½ç½®çš„æœ€é«˜æ¦‚ç‡ï¼š

```py
>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()
```

å°†é¢„æµ‹çš„æ ‡è®°è§£ç ä¸ºç­”æ¡ˆï¼š

```py
>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
'176 billion parameters and can generate text in 46 languages natural languages and 13'
```
</pt>
<tf>
å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›TensorFlowå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
>>> inputs = tokenizer(question, text, return_tensors="tf")
```

å°†ä½ çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> from transformers import TFAutoModelForQuestionAnswering

>>> model = TFAutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
>>> outputs = model(**inputs)
```

ä»æ¨¡å‹è¾“å‡ºä¸­è·å–å¼€å§‹å’Œç»“æŸä½ç½®çš„æœ€é«˜æ¦‚ç‡ï¼š

```py
>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])
```

å°†é¢„æµ‹çš„æ ‡è®°è§£ç ä¸ºç­”æ¡ˆï¼š

```py
>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
'176 billion parameters and can generate text in 46 languages natural languages and 13'
```
</tf>
</frameworkcontent>