## å› æœè¯­è¨€å»ºæ¨¡

[[open-in-colab]]

è¯­è¨€å»ºæ¨¡æœ‰ä¸¤ç§ç±»å‹ï¼Œå› æœå’Œé®è”½ã€‚æœ¬æŒ‡å—ä»‹ç»å› æœè¯­è¨€å»ºæ¨¡ã€‚å› æœè¯­è¨€æ¨¡å‹ç»å¸¸ç”¨äºæ–‡æœ¬ç”Ÿæˆã€‚ä½ å¯ä»¥å°†è¿™äº›æ¨¡å‹ç”¨äºåˆ›æ„åº”ç”¨ï¼Œä¾‹å¦‚é€‰æ‹©ä½ è‡ªå·±çš„æ–‡å­—å†’é™©æˆ–æ™ºèƒ½ç¼–ç åŠ©æ‰‹ï¼ˆå¦‚Copilotæˆ–CodeParrotï¼‰ã€‚

<Youtube id="Vpjb1lu0MDk"/>

å› æœè¯­è¨€å»ºæ¨¡é¢„æµ‹æ ‡è®°åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”æ¨¡å‹åªèƒ½å…³æ³¨å·¦ä¾§çš„æ ‡è®°ã€‚è¿™æ„å‘³ç€æ¨¡å‹æ— æ³•çœ‹åˆ°æœªæ¥çš„æ ‡è®°ã€‚GPT-2æ˜¯å› æœè¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªä¾‹å­ã€‚

æœ¬æŒ‡å—å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨[ELI5](https://huggingface.co/datasets/eli5)æ•°æ®é›†çš„[r/askscience](https://www.reddit.com/r/askscience/)å­é›†ä¸Šå¾®è°ƒ[DistilGPT2](https://huggingface.co/distilgpt2)æ¨¡å‹ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip>
ä½ å¯ä»¥æŒ‰ç…§æœ¬æŒ‡å—ä¸­çš„ç›¸åŒæ­¥éª¤å¾®è°ƒå…¶ä»–ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„æ¶æ„ã€‚
é€‰æ‹©ä»¥ä¸‹æ¶æ„ä¹‹ä¸€ï¼š

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->
[BART](../model_doc/bart), [BERT](../model_doc/bert), [Bert Generation](../model_doc/bert-generation), [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [BioGpt](../model_doc/biogpt), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CodeLlama](../model_doc/code_llama), [CodeGen](../model_doc/codegen), [CPM-Ant](../model_doc/cpmant), [CTRL](../model_doc/ctrl), [Data2VecText](../model_doc/data2vec-text), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [Falcon](../model_doc/falcon), [GIT](../model_doc/git), [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT NeoX Japanese](../model_doc/gpt_neox_japanese), [GPT-J](../model_doc/gptj), [LLaMA](../model_doc/llama), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MPT](../model_doc/mpt), [MusicGen](../model_doc/musicgen), [MVP](../model_doc/mvp), [OpenLlama](../model_doc/open-llama), [OpenAI GPT](../model_doc/openai-gpt), [OPT](../model_doc/opt), [Pegasus](../model_doc/pegasus), [Persimmon](../model_doc/persimmon), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [QDQBert](../model_doc/qdqbert), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [RWKV](../model_doc/rwkv), [Speech2Text2](../model_doc/speech_to_text_2), [Transformer-XL](../model_doc/transfo-xl), [TrOCR](../model_doc/trocr), [XGLM](../model_doc/xglm), [XLM](../model_doc/xlm), [XLM-ProphetNet](../model_doc/xlm-prophetnet), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod)



<!--End of the generated tip-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate
```

æˆ‘ä»¬å»ºè®®ä½ ç™»å½•Hugging Faceå¸æˆ·ï¼Œè¿™æ ·ä½ å°±å¯ä»¥å°†æ¨¡å‹ä¸Šä¼ å¹¶ä¸ç¤¾åŒºå…±äº«ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥ä½ çš„tokenè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ELI5æ•°æ®é›†

é¦–å…ˆï¼Œä»ğŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½ELI5æ•°æ®é›†çš„è¾ƒå°å­é›†r/askscienceå­é›†ã€‚è¿™æ ·å¯ä»¥è®©ä½ æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œæ›´å¤šæ—¶é—´çš„è®­ç»ƒä¹‹å‰ï¼Œä¸€åˆ‡æ­£å¸¸ã€‚

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†çš„â€œtrain_asksâ€æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

ç„¶åçœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

è™½ç„¶è¿™çœ‹èµ·æ¥å¾ˆå¤šï¼Œä½†ä½ å®é™…ä¸Šåªå¯¹â€œtextâ€å­—æ®µæ„Ÿå…´è¶£ã€‚è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æœ‰è¶£ä¹‹å¤„åœ¨äºä½ ä¸éœ€è¦æ ‡ç­¾ï¼ˆä¹Ÿç§°ä¸ºæ— ç›‘ç£ä»»åŠ¡ï¼‰ï¼Œå› ä¸ºä¸‹ä¸€ä¸ªå•è¯å°±æ˜¯æ ‡ç­¾ã€‚

## é¢„å¤„ç†

<Youtube id="ma1TrR7gE7I"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½DistilGPT2æ ‡è®°å™¨ä»¥å¤„ç†â€œtextâ€å­å­—æ®µï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
```

ä½ å°†ä»ä¸Šé¢çš„ç¤ºä¾‹ä¸­æ³¨æ„åˆ°ï¼Œâ€œtextâ€å­—æ®µå®é™…ä¸Šæ˜¯åµŒå¥—åœ¨â€œanswersâ€å†…éƒ¨çš„ã€‚è¿™æ„å‘³ç€ä½ éœ€è¦ä½¿ç”¨[`~datasets.Dataset.flatten`]æ–¹æ³•ä»å…¶åµŒå¥—ç»“æ„ä¸­æå–â€œtextâ€å­å­—æ®µï¼š

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

ç°åœ¨ï¼Œæ¯ä¸ªå­å­—æ®µéƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ—ï¼Œå¦‚â€œanswersâ€å‰ç¼€æ‰€ç¤ºï¼Œâ€œtextâ€å­—æ®µç°åœ¨æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚ä¸æ˜¯åˆ†åˆ«å¯¹æ¯ä¸ªå¥å­è¿›è¡Œæ ‡è®°åŒ–ï¼Œè€Œæ˜¯å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿å¯ä»¥è”åˆå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚

ä¸‹é¢æ˜¯ç”¨äºè¿æ¥ç¤ºä¾‹ä¸­çš„å­—ç¬¦ä¸²åˆ—è¡¨å¹¶å¯¹ç»“æœè¿›è¡Œæ ‡è®°åŒ–çš„ç¬¬ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼š

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

ä½¿ç”¨ğŸ¤—æ•°æ®é›†[`~datasets.Dataset.map`]æ–¹æ³•å°†è¯¥é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡å°†`batched=True`è®¾ç½®ä¸ºåŒæ—¶å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¹¶ä½¿ç”¨`num_proc`å¢åŠ è¿›ç¨‹çš„æ•°é‡ï¼Œå¯ä»¥åŠ é€Ÿ`map`å‡½æ•°çš„å¤„ç†é€Ÿåº¦ã€‚åˆ é™¤ä¸éœ€è¦çš„åˆ—ï¼š

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

è¯¥æ•°æ®é›†åŒ…å«æ ‡è®°åºåˆ—ï¼Œä½†å…¶ä¸­ä¸€äº›åºåˆ—é•¿åº¦è¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

ç°åœ¨ï¼Œå¯ä»¥ä½¿ç”¨ç¬¬äºŒä¸ªé¢„å¤„ç†å‡½æ•°æ¥
- è¿æ¥æ‰€æœ‰åºåˆ—
- å°†è¿æ¥çš„åºåˆ—æ‹†åˆ†ä¸ºé•¿åº¦ç”±`block_size`å®šä¹‰çš„è¾ƒçŸ­çš„å—ï¼Œå…¶é•¿åº¦åº”å°äºæœ€å¤§è¾“å…¥é•¿åº¦ï¼Œå¹¶ä¸”è¶³å¤ŸçŸ­ä»¥é€‚åº”GPU RAMã€‚

```py
>>> block_size = 128


>>> def group_texts(examples):
...     # è¿æ¥æ‰€æœ‰æ–‡æœ¬ã€‚
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # æˆ‘ä»¬ä¸¢å¼ƒå‰©ä½™çš„å°å—ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ å¡«å……è€Œä¸æ˜¯ä¸¢å¼ƒçš„éƒ¨åˆ†ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰æ­¤éƒ¨åˆ†ã€‚
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # æŒ‰block_sizeè¿›è¡Œæ‹†åˆ†ã€‚
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨`group_texts`å‡½æ•°ï¼š

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

ç°åœ¨ï¼Œä½¿ç”¨[`DataCollatorForLanguageModeling`]åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ä½¿ç”¨åŠ¨æ€å¡«å……æ¨¡å‹åœ¨ä¸€æ‰¹ä¸­æœ€é•¿é•¿åº¦çš„å¥å­æ›´æœ‰æ•ˆï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

<frameworkcontent>
<pt>
ä½¿ç”¨ç»ˆæ­¢åºåˆ—tokenä½œä¸ºå¡«å……tokenï¼Œå¹¶è®¾ç½®`mlm=False`ã€‚è¿™å°†ä½¿ç”¨å³ç§»ä¸€ä¸ªå…ƒç´ çš„æ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼š

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

</pt>
<tf>
ä½¿ç”¨ç»ˆæ­¢åºåˆ—tokenä½œä¸ºå¡«å……tokenï¼Œå¹¶è®¾ç½®`mlm=False`ã€‚è¿™å°†ä½¿ç”¨å³ç§»ä¸€ä¸ªå…ƒç´ çš„æ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼š

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

</tf>
</frameworkcontent>


## è®­ç»ƒ

<frameworkcontent>
<pt>
<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åŸºæœ¬æ•™ç¨‹](../training.md#train-with-pytorch-trainer)ï¼

</Tip>

ä½ ç°åœ¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨[`AutoModelForCausalLM`]åŠ è½½DistilGPT2æ¨¡å‹ï¼š

```py
>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

ç°åœ¨åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. ä½¿ç”¨[`TrainingArguments`]å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯`output_dir`ï¼Œç”¨äºæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆä½ éœ€è¦ç™»å½•Hugging Faceä»¥ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ä¸€èµ·ä¼ é€’ç»™[`Trainer`]ã€‚
3. è°ƒç”¨[`~Trainer.train`]æ¥å¾®è°ƒæ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_clm-model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[`~transformers.Trainer.evaluate`]æ–¹æ³•è¯„ä¼°æ¨¡å‹å¹¶è·å¾—å›°æƒ‘åº¦ï¼š

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 49.61
```

ç„¶åå¯ä»¥ä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åŸºæœ¬æ•™ç¨‹](../training.md#train-a-tensorflow-model-with-keras)ï¼

</Tip>
è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åå¯ä»¥ä½¿ç”¨[`TFAutoModelForCausalLM`]åŠ è½½DistilGPT2æ¨¡å‹ï¼š

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")
```

ä½¿ç”¨[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformeræ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œæ‰€ä»¥é™¤éä½ æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦ç‰¹åˆ«æŒ‡å®šï¼š

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # æ²¡æœ‰æŸå¤±å‚æ•°ï¼
```


è¿™å¯ä»¥é€šè¿‡ åœ¨[`~transformers.PushToHubCallback`]ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä½ç½® æ¥å®ç°ï¼š

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_clm-model",
...     tokenizer=tokenizer,
... )
```

æœ€åï¼Œä½ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨`fit`æ–¹æ³•å¹¶ä¼ å…¥è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€è®­ç»ƒè½®æ•°ï¼Œä»¥åŠå›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

è®­ç»ƒå®Œæˆåï¼Œä½ çš„æ¨¡å‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
</tf>
</frameworkcontent>

<Tip>

æ›´è¯¦ç»†çš„å…³äºå¦‚ä½•ä¸ºå› æœè¯­è¨€å»ºæ¨¡å¾®è°ƒæ¨¡å‹çš„ç¤ºä¾‹ï¼Œè¯·å‚è€ƒç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) æˆ–[TensorFlowç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨ä½ å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

æ„é€ ä¸€ä¸ªä½ æƒ³è¦ç”Ÿæˆæ–‡æœ¬çš„è¾“å…¥æç¤ºï¼š

```py
>>> prompt = "Somatic hypermutation allows the immune system to"
```

å°è¯•ä½¿ç”¨[`pipeline`] ä¸­çš„æ¨¡å‹è¿›è¡Œæ¨æ–­æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ä½¿ç”¨ä½ çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„ `pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> generator = pipeline("text-generation", model="my_awesome_eli5_clm-model")
>>> generator(prompt)
[{'generated_text': "Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\n\n\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks."}]
```

<frameworkcontent>
<pt>
å°†æ–‡æœ¬åˆ†è¯å¤„ç†å¹¶å°†`input_ids`è¿”å›ä¸ºPyTorchå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

ä½¿ç”¨[`~transformers.generation_utils.GenerationMixin.generate`]æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies.md)é¡µé¢ã€‚

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

å°†ç”Ÿæˆçš„token idè§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system"]
```
</pt>
<tf>
å°†æ–‡æœ¬åˆ†è¯å¤„ç†å¹¶å°†`input_ids`è¿”å›ä¸ºTensorFlowå¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="tf").input_ids
```

ä½¿ç”¨[`~transformers.generation_tf_utils.TFGenerationMixin.generate`]æ–¹æ³•åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹[æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies.md)é¡µé¢ã€‚

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

å°†ç”Ÿæˆçš„token idè§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']
```
</tf>
</frameworkcontent>