
<!--ç‰ˆæƒ2022 HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ®Apacheè®¸å¯è¯ç¬¬2ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯æ‚¨åªèƒ½åœ¨ç¬¦åˆè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶åŸºäºâ€œåŸæ ·â€ BASIS ï¼ŒWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤æ–‡ä»¶ä»¥Markdownçš„æ ¼å¼ç¼–å†™ï¼Œä½†åŒ…å«äº†ç‰¹å®šçš„æ ‡è®°è¯­æ³•ä»¥ä¾›doc-builderï¼ˆç±»ä¼¼äºMDXï¼‰ä½¿ç”¨ï¼Œè¿™äº›è¯­æ³•å¯èƒ½åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ— æ³•æ­£ç¡®æ¸²æŸ“ã€‚-->

# æ‘˜è¦

[[åœ¨Colabä¸­æŸ¥çœ‹ä»£ç ]]


<Youtube id="yHnr5Dk2zCI"/>

æ‘˜è¦ç”Ÿæˆä¸€ä¸ªè¾ƒçŸ­çš„æ–‡æ¡£æˆ–æ–‡ç« ï¼Œè¯¥æ‘˜è¦æ•æ‰åˆ°æ‰€æœ‰é‡è¦ä¿¡æ¯ã€‚ä¸ç¿»è¯‘ä¸€æ ·ï¼Œæ‘˜è¦æ˜¯å¯ä»¥è¢«å®šä¹‰æˆåºåˆ—-åºåˆ—ä»»åŠ¡çš„å¦ä¸€ä¸ªä¾‹å­ã€‚æ‘˜è¦å¯ä»¥æ˜¯ï¼š

- æŠ½å–å¼çš„ï¼šä»æ–‡æ¡£ä¸­æŠ½å–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
- åˆ›é€ æ€§çš„ï¼šç”Ÿæˆæ–°çš„æ–‡æœ¬ï¼ŒæŠ“ä½æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

æœ¬æŒ‡å—å°†å‘Šè¯‰æ‚¨å¦‚ä½•ï¼š

1. å¯¹åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†ä¸Šçš„[T5](https://huggingface.co/t5-small)è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›è¡Œåˆ›é€ æ€§æ‘˜è¦ã€‚
2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚


>æ­¤æ•™ç¨‹ä¸­æ˜¾ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
>[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“æ–‡ä»¶ï¼š

```bash
pip install transformers datasets evaluate rouge_score
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸ç¤¾åŒºä¸Šä¼ å’Œå…±äº«æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½BillSumæ•°æ®é›†

é¦–å…ˆä»ğŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½è¾ƒå°çš„åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†çš„BillSumæ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> billsum = billsum.train_test_split(test_size=0.2)
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> billsum["train"][0]
{'summary': 'Existing law authorizes state agencies to enter into contracts for the acquisition of goods or services upon approval by the Department of General Services. Existing law sets forth various requirements and prohibitions for those contracts, including, but not limited to, a prohibition on entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between spouses and domestic partners or same-sex and different-sex couples in the provision of benefits. Existing law provides that a contract entered into in violation of those requirements and prohibitions is void and authorizes the state or any person acting on behalf of the state to bring a civil action seeking a determination that a contract is in violation and therefore void. Under existing law, a willful violation of those requirements and prohibitions is a misdemeanor.\nThis bill would also prohibit a state agency from entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between employees on the basis of gender identity in the provision of benefits, as specified. By expanding the scope of a crime, this bill would impose a state-mandated local program.\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\nThis bill would provide that no reimbursement is required by this act for a specified reason.',

## ç¿»è¯‘ private_upload/default/2023-11-07-19-45-48/summarization.md.part-1.md

'text': 'åŠ åˆ©ç¦å°¼äºšå·çš„äººæ°‘ä»¥å¦‚ä¸‹æ–¹å¼è¡Œäº‹ï¼š\n\n\nç¬¬1èŠ‚\nç¬¬10295.35èŠ‚è¢«æ·»åŠ åˆ°ã€Šå…¬å…±åˆåŒæ³•å…¸ã€‹ä¸­ï¼Œå¦‚ä¸‹æ‰€è¯»ï¼š\n10295.35ã€‚\nï¼ˆaï¼‰ï¼ˆ1ï¼‰ä¸é¡¾ä»»ä½•å…¶ä»–æ³•å¾‹çš„è§„å®šï¼Œå›½å®¶æœºæ„ä¸å¾—ä»¥ä¸€ç™¾ä¸‡å…ƒï¼ˆ$100000ï¼‰æˆ–æ›´å¤šçš„é‡‘é¢ä¸æ‰¿åŒ…å•†è®¢ç«‹ä»»ä½•è´­ä¹°è´§ç‰©æˆ–æœåŠ¡çš„åˆåŒï¼Œè¯¥æ‰¿åŒ…å•†åœ¨æä¾›ç¦åˆ©æ–¹é¢å¯¹é›‡å‘˜æ ¹æ®é›‡å‘˜çš„æˆ–ä¾èµ–çš„å®é™…æˆ–è¢«è§†ä¸ºçš„æ€§åˆ«èº«ä»½è¿›è¡Œæ­§è§†ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå°†é›‡å‘˜æˆ–ä¾èµ–äººå‘˜çš„èº«ä»½å®šä¸ºè·¨æ€§åˆ«ã€‚\nï¼ˆ2ï¼‰ä¸ºäº†æœ¬èŠ‚çš„ç›®çš„ï¼Œâ€œåˆåŒâ€åŒ…æ‹¬æ¯å¹´æ¯ä¸ªæ‰¿åŒ…å•†ç´¯è®¡ä¸€ç™¾ä¸‡å…ƒï¼ˆ$100000ï¼‰æˆ–æ›´å¤šçš„é‡‘é¢çš„åˆåŒã€‚\nï¼ˆ3ï¼‰ä¸ºäº†æœ¬èŠ‚çš„ç›®çš„ï¼Œå¦‚æœè®¡åˆ’ä¸ç¬¦åˆå«ç”Ÿå’Œå®‰å…¨æ³•å…¸ç¬¬1365.5èŠ‚å’Œä¿é™©æ³•å…¸ç¬¬10140èŠ‚çš„è§„å®šï¼Œåˆ™é›‡å‘˜çš„å¥åº·è®¡åˆ’æ˜¯æ­§è§†æ€§çš„ã€‚\nï¼ˆ4ï¼‰æœ¬èŠ‚çš„è¦æ±‚ä»…é€‚ç”¨äºå‘ç”Ÿåœ¨æ‰¿åŒ…å•†ç»è¥çš„ä»¥ä¸‹æƒ…å†µï¼š\nï¼ˆAï¼‰å›½å†…ã€‚\nï¼ˆBï¼‰åœ¨å›½å†…ä»¥å¤–çš„æˆ¿åœ°äº§ä¸Šï¼Œå¦‚æœè¯¥æˆ¿åœ°äº§å½’å›½æœ‰ï¼Œæˆ–è€…å¦‚æœå›½å®¶æœ‰æƒå ç”¨è¯¥æˆ¿åœ°äº§ï¼Œå¹¶ä¸”å¦‚æœæ‰¿åŒ…å•†åœ¨è¯¥ä½ç½®çš„å­˜åœ¨ä¸å›½å®¶çš„åˆåŒæœ‰å…³ã€‚\nï¼ˆCï¼‰åœ¨ç¾å›½å…¶ä»–åœ°æ–¹ï¼Œæ­£åœ¨è¿›è¡Œä¸å›½å®¶åˆåŒæœ‰å…³çš„å·¥ä½œã€‚\nï¼ˆbï¼‰æ‰¿åŒ…å•†åº”æŒ‰ç°è¡Œæ³•å¾‹æˆ–æ‰¿åŒ…å•†çš„ä¿é™©æä¾›è€…çš„è¦æ±‚ï¼Œæœ€å¤§ç¨‹åº¦åœ°ä¿å¯†é›‡å‘˜æˆ–æ±‚èŒäººå‘˜å¯¹å°±ä¸šç¦åˆ©çš„è¯·æ±‚æˆ–å¯¹èµ„æ ¼è¯æ˜æäº¤çš„ä»»ä½•æ–‡ä»¶ã€‚\nï¼ˆcï¼‰åœ¨å›½å®¶æœºæ„ç¡®å®šçš„æ‰€æœ‰åˆç†æªæ–½éƒ½å·²é‡‡å–çš„æƒ…å†µä¸‹ï¼Œæœ¬èŠ‚çš„è¦æ±‚å¯ä»¥åœ¨ä»¥ä¸‹ä»»ä½•æƒ…å†µä¸‹è±å…ï¼š\nï¼ˆ1ï¼‰åªæœ‰ä¸€ä¸ªå‡†å¤‡ä¸å›½å®¶æœºæ„è®¢ç«‹ç‰¹å®šåˆåŒçš„æ½œåœ¨æ‰¿åŒ…å•†ã€‚\nï¼ˆ2ï¼‰åˆåŒæ˜¯ä¸ºäº†åº”å¯¹å›½å®¶æœºæ„è®¤å®šçš„å±æœºï¼Œè¯¥å±æœºå±åŠå…¬å…±å¥åº·ã€ç¦åˆ©æˆ–å®‰å…¨ï¼Œæˆ–è€…è¯¥åˆåŒæ˜¯ä¸ºäº†æä¾›åŸºæœ¬æœåŠ¡è€Œå¿…è¦çš„ï¼Œä¸”æ²¡æœ‰ç¬¦åˆæœ¬èŠ‚è¦æ±‚çš„å®ä½“èƒ½å¤Ÿç«‹å³æä¾›å±æœºå“åº”ã€‚\nï¼ˆ3ï¼‰æœ¬èŠ‚çš„è¦æ±‚è¿åæˆ–ä¸æˆæƒé€‚ç”¨äºæœ¬èŠ‚çš„ä»»ä½•æˆæƒã€æ´¥è´´æˆ–åè®®çš„æ¡æ¬¾æˆ–æ¡ä»¶ä¸ä¸€è‡´ï¼Œå¦‚æœä»£ç†æœºæ„å·²ç»å°½åŠ›æ”¹å˜ä»»ä½•æˆæƒã€æ´¥è´´æˆ–åè®®çš„æ¡æ¬¾æˆ–æ¡ä»¶ä»¥æˆæƒæœ¬èŠ‚çš„é€‚ç”¨ã€‚\nï¼ˆ4ï¼‰è¯¥æ‰¿åŒ…å•†æä¾›æ‰¹å‘æˆ–å¤§å®—æ°´ã€ç”µåŠ›æˆ–å¤©ç„¶æ°”ï¼Œä»¥åŠä¸ä¹‹æœ‰å…³çš„è¾“é€æˆ–ä¼ è¾“ï¼Œæˆ–é™„å±æœåŠ¡ï¼Œä½œä¸ºæ ¹æ®è‰¯å¥½çš„å…¬ç”¨å®è·µç¡®ä¿å¯é æœåŠ¡æ‰€éœ€çš„ï¼Œå¦‚æœæ— æ³•é€šè¿‡æ ‡å‡†ç«äº‰æ‹›æ ‡ç¨‹åºå®ç°å¯¹åŒæ ·éœ€è´­ä¹°çš„ï¼Œåˆ™è¯¥æ‰¿åŒ…å•†ä¸æä¾›ç›´æ¥é›¶å”®æœåŠ¡ç»™æœ€ç»ˆç”¨æˆ·ã€‚\nï¼ˆdï¼‰ï¼ˆ1ï¼‰å¦‚æœæ‰¿åŒ…å•†åœ¨æä¾›ç¦åˆ©æ—¶æ”¯ä»˜è·å¾—ç¦åˆ©æ‰€å‘ç”Ÿçš„å®é™…è´¹ç”¨ï¼Œåˆ™è¯¥æ‰¿åŒ…å•†åœ¨æä¾›ç¦åˆ©æ–¹é¢ä¸è¢«è§†ä¸ºæ­§è§†ã€‚\nï¼ˆ2ï¼‰å¦‚æœæ‰¿åŒ…å•†æ— æ³•æä¾›æŸé¡¹ç¦åˆ©ï¼Œå°½ç®¡é‡‡å–åˆç†æªæ–½ä½¿å…¶èƒ½å¤Ÿæä¾›ï¼Œè¯¥æ‰¿åŒ…å•†åœ¨æä¾›ç¦åˆ©æ–¹é¢ä¸è¢«è§†ä¸ºæ­§è§†ã€‚\nï¼ˆeï¼‰ï¼ˆ1ï¼‰æœ¬ç« é€‚ç”¨çš„æ¯ä»½åˆåŒåº”åŒ…å«ä¸€ä¸ªå£°æ˜ï¼Œæ‰¿åŒ…å•†åœ¨è¯¥å£°æ˜ä¸­è¯æ˜æ‰¿åŒ…å•†ç¬¦åˆæœ¬èŠ‚çš„è§„å®šã€‚\nï¼ˆ2ï¼‰è¯¥éƒ¨é—¨æˆ–å…¶ä»–æ‰¿åŒ…æœºæ„åº”æ ¹æ®å…¶ç°æœ‰çš„æ‰§æ³•æƒåŠ›æ¥æ‰§è¡Œæœ¬èŠ‚ã€‚\nï¼ˆ3ï¼‰ï¼ˆAï¼‰å¦‚æœæ‰¿åŒ…å•†è™šå‡è¯æ˜å…¶ç¬¦åˆæœ¬èŠ‚çš„è§„å®šï¼Œé‚£ä¹ˆä¸è¯¥æ‰¿åŒ…å•†çš„åˆåŒåº”å—åˆ°ã€Šç°è¡Œæ³•å…¸ã€‹ç¬¬10420èŠ‚å¼€å§‹çš„ç¬¬9ç¯‡çš„çº¦æŸï¼Œé™¤éåœ¨éƒ¨é—¨æˆ–å…¶ä»–æ‰¿åŒ…æœºæ„æŒ‡å®šçš„æ—¶é—´å†…ï¼Œè¯¥æ‰¿åŒ…å•†å‘è¯¥éƒ¨é—¨æˆ–æœºæ„æä¾›ç¬¦åˆæˆ–æ­£åœ¨ç¬¦åˆæœ¬èŠ‚çš„è¯æ˜ã€‚\nï¼ˆBï¼‰å°†ã€Šç°è¡Œæ³•å…¸ã€‹ç¬¬10420èŠ‚å¼€å§‹çš„ç¬¬9ç¯‡çš„è¡¥æ•‘æªæ–½æˆ–å¤„ç½šé€‚ç”¨äºæœ¬ç« é€‚ç”¨çš„åˆåŒä¸ä¼šæ’æ–¥éƒ¨é—¨æˆ–å…¶ä»–æ‰¿åŒ…æœºæ„åœ¨å…¶ç°æœ‰çš„æ‰§æ³•æƒåŠ›ä¸‹çš„ä»»ä½•ç°æœ‰è¡¥æ•‘æªæ–½ã€‚\n(f)æœ¬èŠ‚çš„ä»»ä½•è§„å®šéƒ½ä¸æ„å‘³ç€è°ƒæ•´ä»»ä½•åœ°æ–¹è¡Œæ”¿åŒºçš„æ‰¿åŒ…æƒ¯ä¾‹ã€‚\nï¼ˆgï¼‰æœ¬èŠ‚åº”è§£é‡Šä¸ºä¸ä¸é€‚ç”¨çš„è”é‚¦æ³•å¾‹ã€è§„åˆ™æˆ–æ³•è§„å†²çªã€‚å¦‚æœæœ‰æ®å¯ä¿¡çš„æ³•é™¢æˆ–å…·æœ‰ç®¡è¾–æƒçš„æœºæ„è®¤ä¸ºè”é‚¦æ³•å¾‹ã€è§„åˆ™æˆ–æ³•è§„ä½¿æœ¬ä»£ç çš„ä»»ä½•æ¡æ¬¾ã€å¥å­ã€æ®µè½æˆ–èŠ‚æ— æ•ˆï¼Œæˆ–è€…ä½¿å…¶é€‚ç”¨äºä»»ä½•äººæˆ–æƒ…å†µæ— æ•ˆï¼Œé‚£ä¹ˆå·æ„å›¾æ˜¯æ³•é™¢æˆ–æœºæ„æ’¤é”€è¯¥æ¡æ¬¾ã€å¥å­ã€æ®µè½æˆ–èŠ‚ï¼Œä»¥ä½¿æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ç»§ç»­æœ‰æ•ˆã€‚\nç¬¬2èŠ‚\nã€Šå…¬å…±åˆåŒæ³•å…¸ã€‹ç¬¬10295.35èŠ‚ä¸å¾—è§£é‡Šä¸ºåœ¨ç¬¬44REC#çš„97å·ä¸­åˆ›å»ºä»»ä½•æ–°çš„æ‰§æ³•æƒé™æˆ–è´£ä»»çš„éƒ¨é—¨æ€»åŠ¡éƒ¨æˆ–ä»»ä½•å…¶ä»–æ‰¿åŒ…æœºæ„ã€‚\n\n\nç¬¬ä¸‰èŠ‚\næ ¹æ®åŠ åˆ©ç¦å°¼äºšå·å®ªæ³•ç¬¬13 XXBæ¡ç¬¬6èŠ‚ï¼Œè¯¥æ³•å¾‹ä¸éœ€è¦æ ¹æ®ç¬¬93CHAR#7Q3.9å·ç¬¬17556æ¡å¯¹ä»»ä½•åœ°æ–¹æœºæ„æˆ–å­¦åŒºè¿›è¡Œè¡¥å¿ï¼Œå› ä¸ºè¯¥æ³•å¾‹å¯èƒ½ç”±äºåˆ›å»ºæ–°çš„çŠ¯ç½ªæˆ–å‰¯ç›¸ä¹‹å¤–çš„çŠ¯ç½ªã€å‰¯ç›¸çš„æ¶‰åŠï¼Œç¬¬93CHAR#7Q3.9å·ç¬¬17556æ¡çš„ç½šé‡‘å˜åŒ–ï¼Œæˆ–è€…æ ¹æ®åŠ åˆ©ç¦å°¼äºšå·å®ªæ³•ç¬¬13Xå­—æ¡ç¬¬6èŠ‚å¯¹çŠ¯ç½ªçš„å®šä¹‰è¿›è¡Œæ›´æ”¹è€Œäº§ç”Ÿçš„è´¹ç”¨ã€‚',
 'title': 'æ·»åŠ ç¬¬10295.35èŠ‚è‡³å…¬å…±åˆåŒæ³•å…¸ï¼Œæœ‰å…³å…¬å…±åˆåŒã€‚'}

## ç¿»è¯‘ private_upload/default/2023-11-07-19-45-48/summarization.md.part-2.md

è¦åº”ç”¨é¢„å¤„ç†å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ğŸ¤— Datasets [`~datasets.Dataset.map`] æ–¹æ³•ã€‚é€šè¿‡è®¾ç½® `batched=True` æ¥åŠ é€Ÿ `map` å‡½æ•°ï¼Œä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†ä¸­çš„å¤šä¸ªå…ƒç´ ï¼š

```py
>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorForSeq2Seq`] åˆ›å»ºä¸€ä¸ªæ‰¹æ¬¡çš„ç¤ºä¾‹ã€‚åœ¨æ±‡ç¼–æœŸé—´ï¼Œé€šè¿‡åŠ¨æ€å¡«å……å¥å­åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯å¡«å……æ•´ä¸ªæ•°æ®é›†åˆ°æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥æé«˜æ•ˆç‡ã€‚

**1ã€pytorchä»£ç **

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

**2ã€tensorflow ä»£ç **

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```


## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·»åŠ åº¦é‡æ–¹æ³•é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥é€šè¿‡ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) åº¦é‡ï¼ˆå‚é˜…ğŸ¤— è¯„ä¼°åº“çš„[å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡ï¼‰ï¼š

```py
>>> import evaluate

>>> rouge = evaluate.load("rouge")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— ROUGE åº¦é‡ï¼š

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
...     result["gen_len"] = np.mean(prediction_lens)

...     return {k: round(v, 4) for k, v in result.items()}
```

ç°åœ¨æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶å°†è¿”å›åˆ°å®ƒã€‚

## è®­ç»ƒ

**1ã€pytorch ä»£ç **


>å¦‚æœæ‚¨å¯¹ä½¿ç”¨ [`Trainer`] å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)ï¼


ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ç›®å‰åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`Seq2SeqTrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€æ‰€éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° ROUGE åº¦é‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ±‡é›†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Seq2SeqTrainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_billsum_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=4,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_billsum["train"],
...     eval_dataset=tokenized_billsum["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ¨¡å‹å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

**2ã€tensorflowä»£ç **



>å¦‚æœæ‚¨å¯¹ä½¿ç”¨ Keras å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-a-tensorflow-model-with-keras)ï¼


è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åï¼Œä½¿ç”¨ [`TFAutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ä½¿ç”¨ [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] å°†æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_billsum["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_billsum["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) é…ç½®ç”¨äºè®­ç»ƒçš„æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä»»åŠ¡ç›¸å…³æŸå¤±å‡½æ•°ï¼Œæ‰€ä»¥æ‚¨ä¸éœ€è¦æŒ‡å®šä¸€ä¸ªï¼Œé™¤éæ‚¨æƒ³è¦ä½¿ç”¨è‡ªå®šä¹‰çš„ï¼š

```py
>>> import tensorflow as tf

## ç¿»è¯‘ private_upload/default/2023-11-07-19-45-48/summarization.md.part-3.md

```markdown
>>> model.compile(optimizer=optimizer)  # æ²¡æœ‰æŸå¤±å‚æ•°ï¼
```

å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæœ€åè¦åšçš„ä¸¤ä»¶äº‹æƒ…æ˜¯ä»é¢„æµ‹ä¸­è®¡ç®— ROUGE åˆ†æ•°ï¼Œå¹¶æä¾›ä¸€ç§å°†æ¨¡å‹æ¨é€åˆ° Hub çš„æ–¹æ³•ã€‚è¿™ä¸¤ä¸ªå¯ä»¥é€šè¿‡ä½¿ç”¨ [Keras å›è°ƒ](../main_classes/keras_callbacks) æ¥å®Œæˆã€‚

å°†ä½ çš„ `compute_metrics` å‡½æ•°ä¼ é€’ç»™ [`~transformers.KerasMetricCallback`]ï¼š

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

æŒ‡å®šå°†ä½ çš„æ¨¡å‹å’Œ tokenizer æ¨é€åˆ°çš„ä½ç½®ï¼Œç”¨ [`~transformers.PushToHubCallback`]ï¼š

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_billsum_model",
...     tokenizer=tokenizer,
... )
```

ç„¶åå°†ä½ çš„å›è°ƒå‡½æ•°æ†ç»‘åœ¨ä¸€èµ·ï¼š

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œä½ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼é€šè¿‡è°ƒç”¨ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ï¼Œä¼ é€’ä½ çš„è®­ç»ƒé›†ã€éªŒè¯é›†ã€è®­ç»ƒè½®æ•°å’Œå›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

è®­ç»ƒå®Œæˆåï¼Œä½ çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ° Hubï¼Œå¤§å®¶éƒ½å¯ä»¥ä½¿ç”¨ï¼



>æ›´è¯¦ç»†çš„æ‘˜è¦å¾®è°ƒæ¨¡å‹ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
æˆ– [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)ã€‚

## æ¨ç†

å¾ˆå¥½ï¼Œç°åœ¨ä½ å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒåšæ¨ç†äº†ï¼

å‡†å¤‡ä¸€äº›ä½ æƒ³è¦è¿›è¡Œæ‘˜è¦çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œä½ éœ€è¦æ ¹æ®ä½ æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥åŠ ä¸Šå‰ç¼€ã€‚å¯¹äºæ‘˜è¦ï¼Œä½ åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºä¸ºè¾“å…¥åŠ ä¸Šå‰ç¼€ï¼š

```py
>>> text = "summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes."
```

å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨ [`pipeline`] ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨ä½ çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªæ‘˜è¦çš„ `pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> summarizer = pipeline("summarization", model="stevhliu/my_awesome_billsum_model")
>>> summarizer(text)
[{"summary_text": "The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country."}]
```

å¦‚æœä½ æ„¿æ„ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š

**1ã€pytorchä»£ç **

å°†æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå°† `input_ids` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_billsum_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ä½¿ç”¨ [`~transformers.generation_utils.GenerationMixin.generate`] æ–¹æ³•è¿›è¡Œæ‘˜è¦ã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œç”¨äºæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ–‡æœ¬ç”Ÿæˆ](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("stevhliu/my_awesome_billsum_model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)
```

å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'
```

**2ã€tensorflowä»£ç **

å°†æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå°† `input_ids` è¿”å›ä¸º TensorFlow å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_billsum_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

ä½¿ç”¨ [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] æ–¹æ³•è¿›è¡Œæ‘˜è¦ã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œç”¨äºæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ–‡æœ¬ç”Ÿæˆ](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("stevhliu/my_awesome_billsum_model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)
```

å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'
```


