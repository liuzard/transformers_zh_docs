<!--
# 版权2022 HuggingFace团队。保留所有权利。

根据Apache许可证第2版（“许可证”），你除非符合许可证的要求，否则无权使用此文件。
你可以在以下位置获取许可证的副本

http://www.apache.org/licenses/LICENSE-2.0

除非适用法律要求或书面同意，软件在"AS IS"基础上分发，不附带任何明示或暗示的保证或条件。请参阅许可证以了解许可证下的特定语言以及限制。

⚠️ 请注意，此文件是Markdown格式，但包含用于我们的文档构建器（类似于MDX）的特定语法，可能无法在你的Markdown查看器中正确呈现。

-->

# 目标检测

[[open-in-colab]]

目标检测是计算机视觉的任务之一，用于在图像中检测实例（例如人、建筑物或汽车）。目标检测模型接收图像作为输入，并输出检测到的对象的边界框的坐标和关联标签。一张图像可以包含多个对象，每个对象都有自己的边界框和标签（例如可以同时包含汽车和建筑物），每个对象可以出现在图像的不同部分（例如图像中可能有几辆汽车）。这个任务在自动驾驶中常用于检测像行人、道路标志和交通灯等物体。其他应用包括图像中对象计数、图像搜索等。

在本指南中，你将学习如何：

 1. 对[DETR](https://huggingface.co/docs/transformers/model_doc/detr)进行微调，DETR是一个将卷积骨干网络与编码器-解码器Transformer相结合的模型，使用[CPPE-5](https://huggingface.co/datasets/cppe-5)数据集进行微调。
 2. 使用已经微调过的模型进行推断。

<Tip>
本教程中的任务支持以下模型架构：

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[Conditional DETR](../model_doc/conditional_detr), [Deformable DETR](../model_doc/deformable_detr), [DETA](../model_doc/deta), [DETR](../model_doc/detr), [Table Transformer](../model_doc/table-transformer), [YOLOS](../model_doc/yolos)

<!--End of the generated tip-->

</Tip>

在开始之前，请确保已安装所有必要的库：

```bash
pip install -q datasets transformers evaluate timm albumentations
```

你将使用🤗Datasets 来从 Hugging Face Hub 加载数据集，🤗Transformers 用于训练模型，`albumentations` 用于增强数据。`timm` 目前需要用于加载 DETR 模型的卷积骨干网络。

我们鼓励你与社区分享你的模型。登录到你的 Hugging Face 帐户并将其上传到 Hub。
在提示时输入你的token以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

加载 CPPE-5 数据集

[CPPE-5 数据集](https://huggingface.co/datasets/cppe-5) 包含在 COVID-19 疫情背景下识别医疗个人防护装备（PPE）的图像。

首先加载数据集：

```py
>>> from datasets import load_dataset

>>> cppe5 = load_dataset("cppe-5")
>>> cppe5
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 29
    })
})
```

你会发现这个数据集已经包含了一个包含 1000 张图像的训练集和一个包含 29 张图像的测试集。

为了熟悉数据，查看一下示例的样子。

```py
>>> cppe5["train"][0]
{'image_id': 15,
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,
 'width': 943,
 'height': 663,
 'objects': {'id': [114, 115, 116, 117],
  'area': [3796, 1596, 152768, 81002],
  'bbox': [[302.0, 109.0, 73.0, 52.0],
   [810.0, 100.0, 57.0, 28.0],
   [160.0, 31.0, 248.0, 616.0],
   [741.0, 68.0, 202.0, 401.0]],
  'category': [4, 4, 0, 0]}}
```

数据集中的示例包括以下字段：
- `image_id`：示例图像的ID
- `image`：包含图像的 `PIL.Image.Image` 对象
- `width`：图像的宽度
- `height`：图像的高度
- `objects`：包含图像中对象的边界框元数据的字典：
  - `id`：注释的 ID
  - `area`：边界框的面积
  - `bbox`：对象的边界框（[COCO 格式](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco)）
  - `category`：对象的类别，可能的值包括 `Coverall (0)`、`Face_Shield (1)`、`Gloves (2)`、`Goggles (3)` 和 `Mask (4)`

你可能会注意到 `bbox` 字段遵循 COCO 格式，这是 DETR 模型希望的格式。
然而，`objects` 中字段的分组与 DETR 要求的注释格式不同。你需要在使用此数据进行训练之前应用一些预处理转换。

为了更好地了解数据，可视化数据集中的一个示例。

```py
>>> import numpy as np
>>> import os
>>> from PIL import Image, ImageDraw

>>> image = cppe5["train"][0]["image"]
>>> annotations = cppe5["train"][0]["objects"]
>>> draw = ImageDraw.Draw(image)

>>> categories = cppe5["train"].features["objects"].feature["category"].names

>>> id2label = {index: x for index, x in enumerate(categories, start=0)}
>>> label2id = {v: k for k, v in id2label.items()}

>>> for i in range(len(annotations["id"])):
...     box = annotations["bbox"][i - 1]
...     class_idx = annotations["category"][i - 1]
...     x, y, w, h = tuple(box)
...     draw.rectangle((x, y, x + w, y + h), outline="red", width=1)
...     draw.text((x, y), id2label[class_idx], fill="white")

>>> image
```

<div class="flex justify-center">
    <img src="https://i.imgur.com/TdaqPJO.png" alt="CPPE-5 Image Example"/>
</div>

要可视化带有关联标签的边界框，你可以从数据集的元数据中获取标签，特别是 `category` 字段。
你还需要创建映射将标签 ID 映射到标签类别（`id2label`），以及将标签类别映射到 ID 的映射（`label2id`）。
如果你在 Hugging Face Hub 上分享模型，包括这些映射将使你的模型可以被其他人重复使用。

在了解数据的过程中，要侧重检查潜在问题。对象检测数据集中的一个常见问题是边界框“拉伸”到图像边缘以外。这样的“行为”边界框可能会在训练时引发错误，并且应该在此阶段予以解决。在此数据集中有几个示例存在这个问题。为了简化本指南，我们从数据中删除这些图像。

```py
>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)
```

对数据进行预处理

要微调模型，你必须预处理计划使用的数据，以精确地匹配预训练模型所使用的方法。
[`AutoImageProcessor`] 负责处理图像数据，创建 `pixel_values`、`pixel_mask` 和可以用于训练 DETR 模型的 `labels`。图像处理器具有一些你不必担心的属性：

- `image_mean = [0.485, 0.456, 0.406]`
- `image_std = [0.229, 0.224, 0.225]`

这些是用于规范化模型在预训练期间的图像的平均值和标准差。当进行推断或微调预训练图像模型时，这些值非常重要。

从与预处理使用的检查点相同的检查点实例化图像处理器。

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "facebook/detr-resnet-50"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

在将图像传递给 `image_processor` 之前，应用两种预处理转换对数据集进行预处理：
- 对图像进行增强
- 重新格式化注释以满足 DETR 的预期

首先，为了确保模型不会过度拟合训练数据，可以使用任何数据增强库对图像进行增强。这里我们使用 [Albumentations](https://albumentations.ai/docs/)...
该库确保转换影响图像并相应地更新边界框。🤗Datasets库的文档中有一个详细的 [关于如何为对象检测增强图像的指南](https://huggingface.co/docs/datasets/object_detection)，它使用完全相同的数据集作为示例。在这里应用相同的方法，将每个图像调整为 (480, 480) 的大小，水平翻转图像，并增加亮度：

```py
>>> import albumentations
>>> import numpy as np
>>> import torch

>>> transform = albumentations.Compose(
...     [
...         albumentations.Resize(480, 480),
...         albumentations.HorizontalFlip(p=1.0),
...         albumentations.RandomBrightnessContrast(p=1.0),
...     ],
...     bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
... )
```

`image_processor` 期望注释的格式为：`{'image_id': int, 'annotations': List[Dict]}`，其中每个字典都是 COCO 对象注释。让我们添加一个函数，以重新格式化单个示例的注释：

```py
>>> def formatted_anns(image_id, category, area, bbox):
...     annotations = []
...     for i in range(0, len(category)):
...         new_ann = {
...             "image_id": image_id,
...             "category_id": category[i],
...             "isCrowd": 0,
...             "area": area[i],
...             "bbox": list(bbox[i]),
...         }
...         annotations.append(new_ann)

...     return annotations
```

现在，你可以将图像和注释转换组合起来以在一批示例上使用：

```py
>>> # transforming a batch
>>> def transform_aug_ann(examples):
...     image_ids = examples["image_id"]
...     images, bboxes, area, categories = [], [], [], []
...     for image, objects in zip(examples["image"], examples["objects"]):
...         image = np.array(image.convert("RGB"))[:, :, ::-1]
...         out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

...         area.append(objects["area"])
...         images.append(out["image"])
...         bboxes.append(out["bboxes"])
...         categories.append(out["category"])

...     targets = [
...         {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
...     ]

...     return image_processor(images=images, annotations=targets, return_tensors="pt")
```

使用 🤗Datasets 的 [`~datasets.Dataset.with_transform`] 方法将此预处理函数应用于整个数据集。此方法在加载数据集元素时动态应用转换。

此时，你可以检查经过转换的数据集中的示例的样子了。你应该会看到带有 `pixel_values` 的张量，带有 `pixel_mask` 的张量和 `labels`。

```py
>>> cppe5["train"] = cppe5["train"].with_transform(transform_aug_ann)
>>> cppe5["train"][15]
{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],
          ...,
          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],

         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],

         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],
          ...,
          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}
```

你已成功增强了个别图像并准备了它们的注释。然而，预处理还没有完成。在最后一步中，创建一个自定义的 `collate_fn` 来将图像批处理在一起。
将图像（现在是 `pixel_values`）填充到一批中最大的图像，创建相应的 `pixel_mask` 来指示哪些像素是真实的（1）和哪些是填充的（0）。

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

训练 DEER 模型
在之前的部分中，你已经完成了大部分繁重的工作，现在可以开始训练模型了！即使在调整大小之后，此数据集中的图像仍然相当大。这意味着微调此模型将需要至少一个 GPU。

训练包括以下步骤：
1. 使用与预处理相同的检查点使用 [`AutoModelForObjectDetection`] 加载模型。
2. 在 [`TrainingArguments`] 中定义你的训练超参数。
3. 将训练参数与模型、数据集、图像处理器和数据收集器一起传递给 [`Trainer`]。
4. 调用 [`~Trainer.train`] 来微调你的模型。

在从与预处理相同的检查点加载模型时，请记得传递之前从数据集的元数据中创建的 `label2id` 和 `id2label` 映射。此外，我们指定了 `ignore_mismatched_sizes=True` 来用新的分类头替换现有的分类头。

```py
from transformers import AutoModelForObjectDetection


```py
>>> model = AutoModelForObjectDetection.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
...     ignore_mismatched_sizes=True,
... )
```