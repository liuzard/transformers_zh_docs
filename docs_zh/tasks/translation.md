<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ç¿»è¯‘

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

ç¿»è¯‘æ˜¯å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€çš„è¿‡ç¨‹ã€‚å®ƒæ˜¯å¯ä»¥å°†è¾“å…¥è½¬åŒ–ä¸ºè¾“å‡ºçš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªç”¨äºå®ç°ç¿»è¯‘æˆ–æ‘˜è¦ç­‰ä»»åŠ¡çš„å¼ºå¤§æ¡†æ¶ã€‚ç¿»è¯‘ç³»ç»Ÿé€šå¸¸ç”¨äºä¸åŒè¯­è¨€æ–‡æœ¬ä¹‹é—´çš„ç¿»è¯‘ï¼Œä½†ä¹Ÿå¯ç”¨äºè¯­éŸ³æˆ–æ–‡æœ¬è½¬è¯­éŸ³æˆ–è¯­éŸ³è½¬æ–‡æœ¬ç­‰æ··åˆåº”ç”¨ã€‚

æœ¬æŒ‡å—å°†å‘Šè¯‰ä½ å¦‚ä½•ï¼š

1. åœ¨è‹±æ³•å­é›†ä¸Šå¾®è°ƒ[T5](https://huggingface.co/t5-small)æ¨¡å‹ï¼Œä»¥å®ç°å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºæ³•æ–‡ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

<Tip>
æ­¤æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡æ”¯æŒä»¥ä¸‹æ¨¡å‹æ¶æ„:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)

<!--End of the generated tip-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…äº†æ‰€éœ€çš„åº“ï¼š

```bash
pip install transformers datasets evaluate sacreble
```
æˆ‘ä»¬é¼“åŠ±ä½ ç™»å½•åˆ°ä½ çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·ä½ å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥ä½ çš„tokenä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½OPUS Booksæ•°æ®é›†

é¦–å…ˆä»ğŸ¤—Datasetsåº“åŠ è½½[OPUS Books](https://huggingface.co/datasets/opus_books)æ•°æ®é›†çš„è‹±æ³•å­é›†ï¼š

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}
```

`translation`ï¼šæ–‡æœ¬çš„è‹±è¯­å’Œæ³•è¯­ç¿»è¯‘ã€‚

## é¢„å¤„ç†

<Youtube id="XAR8jnZZuUs"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ªT5åˆ†è¯å™¨æ¥å¤„ç†è‹±æ³•è¯­è¨€å¯¹ï¼š

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ä½ è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦ï¼š

1. ä½¿ç”¨æç¤ºå°†è¾“å…¥å‰ç¼€åŒ–ï¼Œä»¥ä¾¿T5çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡ã€‚ä¸€äº›èƒ½å¤Ÿæ‰§è¡Œå¤šä¸ªNLPä»»åŠ¡çš„æ¨¡å‹éœ€è¦ä¸ºç‰¹å®šä»»åŠ¡æä¾›æç¤ºã€‚
2. å°†è¾“å…¥ï¼ˆè‹±æ–‡ï¼‰å’Œç›®æ ‡ï¼ˆæ³•æ–‡ï¼‰åˆ†åˆ«è¿›è¡Œåˆ†è¯ï¼Œå› ä¸ºä¸èƒ½ä½¿ç”¨åœ¨è‹±æ–‡è¯æ±‡è¡¨ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„åˆ†è¯å™¨å¯¹æ³•æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚
3. å°†åºåˆ—æˆªæ–­ä¸º`max_length`å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

åº”ç”¨é¢„å¤„ç†å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ğŸ¤—Datasets [`~datasets.Dataset.map`] æ–¹æ³•ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½® `batched=True` æ¥åŠ é€Ÿ `map` å‡½æ•°ï¼Œä»è€Œä¸€æ¬¡å¤„ç†æ•°æ®é›†ä¸­çš„å¤šä¸ªå…ƒç´ ï¼š

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorForSeq2Seq`] åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ä¸Šï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

**1ã€pytorchä»£ç **

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

**2ã€tensorflowä»£ç **

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```


## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«åº¦é‡æŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ å¯ä»¥ä½¿ç”¨ ğŸ¤—[Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) åº¦é‡æŒ‡æ ‡ï¼ˆè¯·å‚é˜… ğŸ¤—Evaluate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour) äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†é¢„æµ‹ç»“æœå’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— SacreBLEU åˆ†æ•°ï¼š

```py
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.

ä½ çš„ `compute_metrics` å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å¥½äº†ï¼Œå½“ä½ è®¾ç½®è®­ç»ƒæ—¶ä¼šå›åˆ°å®ƒã€‚

## è®­ç»ƒ

1ã€pytorch ä»£ç 


>å¦‚æœä½ å¯¹ä½¿ç”¨ [`Trainer`] å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[æ­¤å¤„](../training#train-with-pytorch-trainer)ï¼


ç°åœ¨ä½ å¯ä»¥å¼€å§‹è®­ç»ƒä½ çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

æ­¤æ—¶åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`Seq2SeqTrainingArguments`] ä¸­å®šä¹‰ä½ çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯ `output_dir`ï¼Œç”¨äºæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True`ï¼Œä½ å¯ä»¥å°†è¯¥æ¨¡å‹æ¨é€åˆ° Hub ä¸Šï¼ˆä¸Šä¼ æ¨¡å‹éœ€è¦ç™»å½• Hugging Faceï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° SacreBLEU æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒçš„æ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Seq2SeqTrainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒä½ çš„æ¨¡å‹ã€‚

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
````

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†ä½ çš„æ¨¡å‹å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ï¼š

```python
>>> trainer.push_to_hub()
```

2ã€tensorflow ä»£ç 


>å¦‚æœä½ å¯¹ä½¿ç”¨ Keras è¿›è¡Œå¾®è°ƒæ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„](../training#train-a-tensorflow-model-with-keras)çš„åŸºæœ¬æ•™ç¨‹ï¼



è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹ï¼Œé¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```python
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ [`TFAutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```python
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ä½¿ç”¨ [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] å°†æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) æ¥é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½å…·æœ‰é»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éä½ æƒ³è¦æŒ‡å®šä¸€ä¸ªç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šï¼š

```python
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # ä¸éœ€è¦æŸå¤±å‚æ•°ï¼
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œè¿˜æœ‰æœ€åä¸¤ä»¶äº‹æƒ…è¦è®¾ç½®ï¼šä»é¢„æµ‹ä¸­è®¡ç®— SacreBLEU æŒ‡æ ‡ï¼Œå¹¶æä¾›ä¸€ç§å°†æ¨¡å‹æ¨é€åˆ° Hub çš„æ–¹å¼ã€‚è¿™ä¸¤ä¸ªéƒ½å¯ä»¥é€šè¿‡ä½¿ç”¨ [Keras å›è°ƒ](../main_classes/keras_callbacks) æ¥å®Œæˆã€‚

å°†ä½ çš„ `compute_metrics` å‡½æ•°ä¼ é€’ç»™ [`~transformers.KerasMetricCallback`]ï¼š

```python
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

åœ¨ [`~transformers.PushToHubCallback`] ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä½ç½®ï¼š

```python
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

ç„¶åå°†å›è°ƒå‡½æ•°æ†ç»‘åœ¨ä¸€èµ·ï¼š

```python
>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ä½ çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€è¿­ä»£æ¬¡æ•°ä»¥åŠå›è°ƒå‡½æ•°æ¥è°ƒç”¨ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```
ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½ çš„æ¨¡å‹å°±ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° Hub ä¸Šï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼



>è¦äº†è§£å¦‚ä½•å¯¹æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„å¾®è°ƒç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)
>æˆ–[TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)ã€‚



## æ¨ç†

å¤ªæ£’äº†ï¼Œç°åœ¨ä½ å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼

æƒ³å‡ºä¸€äº›ä½ æƒ³è¦ç¿»è¯‘æˆå…¶ä»–è¯­è¨€çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ ¹æ®ä½ æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ï¼Œä½ éœ€è¦ä¸ºè¾“å…¥åŠ ä¸Šå‰ç¼€ã€‚ä¾‹å¦‚ï¼Œè¦ä»è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ï¼Œä½ åº”è¯¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ä¸ºè¾“å…¥åŠ ä¸Šå‰ç¼€ï¼š

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

å°è¯•ä½¿ç”¨å·²ç»å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†å…¶ç”¨äº [`pipeline`] ä¸­ã€‚ä½¿ç”¨ä½ çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç¿»è¯‘çš„ `pipeline`ï¼Œç„¶åå°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> translator = pipeline("translation", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]
```

å¦‚æœä½ æ„¿æ„ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š

**1ã€pytorch ä»£ç **

å°†æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ä½¿ç”¨ [`~transformers.generation_utils.GenerationMixin.generate`] æ–¹æ³•ç”Ÿæˆç¿»è¯‘ç»“æœã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Text Generation](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

å°†ç”Ÿæˆçš„ token id è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignÃ©es partagent des ressources avec des bactÃ©ries fixant l'azote.'
```

**2ã€tensorflow ä»£ç **

å°†æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º TensorFlow å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

ä½¿ç”¨ [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] æ–¹æ³•ç”Ÿæˆç¿»è¯‘ç»“æœã€‚æœ‰å…³ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Text Generation](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

å°†ç”Ÿæˆçš„ token id è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'
```
