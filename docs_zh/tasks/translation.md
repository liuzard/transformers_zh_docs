<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# 翻译

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

翻译是将一种语言的文本序列转换为另一种语言的过程。它是可以将输入转化为输出的序列到序列问题之一，也是一个用于实现翻译或摘要等任务的强大框架。翻译系统通常用于不同语言文本之间的翻译，但也可用于语音或文本转语音或语音转文本等混合应用。

本指南将告诉你如何：

1. 在英法子集上微调[T5](https://huggingface.co/t5-small)模型，以实现将英文文本翻译为法文。
2. 使用微调后的模型进行预测。

<Tip>
此教程中所示的任务支持以下模型架构:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)

<!--End of the generated tip-->

</Tip>

开始之前，请确保已经安装了所需的库：

```bash
pip install transformers datasets evaluate sacreble
```
我们鼓励你登录到你的Hugging Face账户，这样你就可以上传和与社区分享你的模型。在提示时，输入你的token以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 加载OPUS Books数据集

首先从🤗Datasets库加载[OPUS Books](https://huggingface.co/datasets/opus_books)数据集的英法子集：

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

使用[`~datasets.Dataset.train_test_split`]方法将数据集分割为训练集和测试集：

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

然后查看一个示例：

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau élevé ne mesurait que quelques toises, et bientôt nous fûmes rentrés dans notre élément.'}}
```

`translation`：文本的英语和法语翻译。

## 预处理

<Youtube id="XAR8jnZZuUs"/>

下一步是加载一个T5分词器来处理英法语言对：

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

你要创建的预处理函数需要：

1. 使用提示将输入前缀化，以便T5知道这是一个翻译任务。一些能够执行多个NLP任务的模型需要为特定任务提供提示。
2. 将输入（英文）和目标（法文）分别进行分词，因为不能使用在英文词汇表上进行预训练的分词器对法文文本进行分词。
3. 将序列截断为`max_length`参数设置的最大长度。

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

应用预处理函数到整个数据集，可以使用🤗Datasets [`~datasets.Dataset.map`] 方法。你可以通过设置 `batched=True` 来加速 `map` 函数，从而一次处理数据集中的多个元素：

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

现在使用 [`DataCollatorForSeq2Seq`] 创建一个示例批次。在整理过程中，将句子动态填充到批次中的最长长度上，而不是将整个数据集填充到最大长度。

**1、pytorch代码**

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

**2、tensorflow代码**

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```


## 评估

在训练过程中包含度量指标通常有助于评估模型的性能。你可以使用 🤗[Evaluate](https://huggingface.co/docs/evaluate/index) 库快速加载评估方法。对于此任务，加载 [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) 度量指标（请参阅 🤗Evaluate [快速入门](https://huggingface.co/docs/evaluate/a_quick_tour) 了解有关如何加载和计算度量指标的更多信息）：

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

然后创建一个函数，将预测结果和标签传递给 [`~evaluate.EvaluationModule.compute`] 来计算 SacreBLEU 分数：

```py
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.

你的 `compute_metrics` 函数现在已经准备好了，当你设置训练时会回到它。

## 训练

1、pytorch 代码


>如果你对使用 [`Trainer`] 对模型进行微调不熟悉，请查看基本教程[此处](../training#train-with-pytorch-trainer)！


现在你可以开始训练你的模型了！使用 [`AutoModelForSeq2SeqLM`] 加载 T5：

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

此时只剩下三个步骤：

1. 在 [`Seq2SeqTrainingArguments`] 中定义你的训练超参数。唯一需要的参数是 `output_dir`，用于指定保存模型的位置。通过设置 `push_to_hub=True`，你可以将该模型推送到 Hub 上（上传模型需要登录 Hugging Face）。在每个 epoch 结束时，[`Trainer`] 将评估 SacreBLEU 指标并保存训练的检查点。
2. 将训练参数与模型、数据集、分词器、数据整理器和 `compute_metrics` 函数一起传递给 [`Seq2SeqTrainer`]。
3. 调用 [`~Trainer.train`] 来微调你的模型。

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
````

一旦训练完成，使用[`~transformers.Trainer.push_to_hub`]方法将你的模型共享到 Hub，以便每个人都可以使用你的模型：

```python
>>> trainer.push_to_hub()
```

2、tensorflow 代码


>如果你对使用 Keras 进行微调模型不熟悉，请查看[此处](../training#train-a-tensorflow-model-with-keras)的基本教程！



要在 TensorFlow 中微调模型，首先设置优化器函数、学习率计划和一些训练超参数：

```python
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

然后，你可以使用 [`TFAutoModelForSeq2SeqLM`] 加载 T5：

```python
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

使用 [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] 将数据集转换为 `tf.data.Dataset` 格式：

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

使用 [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) 来配置模型进行训练。请注意，Transformers 模型都具有默认的与任务相关的损失函数，因此除非你想要指定一个特定的损失函数，否则不需要指定：

```python
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # 不需要损失参数！
```

在开始训练之前，还有最后两件事情要设置：从预测中计算 SacreBLEU 指标，并提供一种将模型推送到 Hub 的方式。这两个都可以通过使用 [Keras 回调](../main_classes/keras_callbacks) 来完成。

将你的 `compute_metrics` 函数传递给 [`~transformers.KerasMetricCallback`]：

```python
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

在 [`~transformers.PushToHubCallback`] 中指定要推送模型和分词器的位置：

```python
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

然后将回调函数捆绑在一起：

```python
>>> callbacks = [metric_callback, push_to_hub_callback]
```

最后，你已经准备好开始训练模型了！使用你的训练和验证数据集、迭代次数以及回调函数来调用 [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) 来微调模型：

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```
一旦训练完成，你的模型就会自动上传到 Hub 上，这样每个人都可以使用它！



>要了解如何对模型进行更深入的微调示例，请查看相应的[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)
>或[TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)。



## 推理

太棒了，现在你已经对模型进行了微调，可以用它进行推理了！

想出一些你想要翻译成其他语言的文本。对于 T5，根据你正在处理的任务，你需要为输入加上前缀。例如，要从英语翻译成法语，你应该按照以下方式为输入加上前缀：

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

尝试使用已经微调的模型进行推理的最简单方法是将其用于 [`pipeline`] 中。使用你的模型实例化一个翻译的 `pipeline`，然后将文本传递给它：

```py
>>> from transformers import pipeline

>>> translator = pipeline("translation", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactéries azotantes.'}]
```

如果你愿意，你也可以手动复制 `pipeline` 的结果：

**1、pytorch 代码**

将文本进行标记化，并将 `input_ids` 返回为 PyTorch 张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

使用 [`~transformers.generation_utils.GenerationMixin.generate`] 方法生成翻译结果。有关不同的文本生成策略和控制生成的参数的更多详细信息，请查看 [Text Generation](../main_classes/text_generation) API。

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

将生成的 token id 解码回文本：

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignées partagent des ressources avec des bactéries fixant l'azote.'
```

**2、tensorflow 代码**

将文本进行标记化，并将 `input_ids` 返回为 TensorFlow 张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

使用 [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] 方法生成翻译结果。有关不同的文本生成策略和控制生成的参数的更多详细信息，请查看 [Text Generation](../main_classes/text_generation) API。

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

将生成的 token id 解码回文本：

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bactéries fixatrices d'azote.'
```
